{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "da1b53a1-73eb-4d89-9478-b712d7ff5ac9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Task**\n",
    "--------\n",
    "\n",
    "Create a Databricks python code that extracts air quality data from an API, groups it by month, and calculates monthly averages for all pollutant measurements.\n",
    "\n",
    "**Requirements**\n",
    "----------------\n",
    "\n",
    "### **1\\. Extract Data**\n",
    "\n",
    "*   Fetch air quality data from: https://air-quality-api.open-meteo.com/v1/air-quality?latitude=40.3548&longitude=18.1724&hourly=pm10,pm2\\_5,carbon\\_monoxide,carbon\\_dioxide,nitrogen\\_dioxide,sulphur\\_dioxide,ozone&start\\_date=2025-03-01&end\\_date=2025-08-31\n",
    "    \n",
    "*   Use Python requests to get the JSON response\n",
    "    \n",
    "\n",
    "### **2\\. Transform Data**\n",
    "\n",
    "*   Parse the JSON hourly data into a PySpark DataFrame\n",
    "    \n",
    "*   Convert time and pollutant lists into structured rows\n",
    "    \n",
    "*   Extract year and month from timestamp for grouping\n",
    "    \n",
    "*   Add an ingestion\\_date column\n",
    "    \n",
    "\n",
    "### **3\\. Monthly Aggregation**\n",
    "\n",
    "*   Group data by year and month\n",
    "    \n",
    "*   Calculate average values for each pollutant (pm10, pm2\\_5, carbon\\_monoxide, etc.)\n",
    "    \n",
    "*   Handle null values appropriately during aggregation\n",
    "    \n",
    "*   Create a summary DataFrame with monthly averages\n",
    "    \n",
    "\n",
    "### **4\\. Save Results**\n",
    "\n",
    "*   Save the monthly aggregated data to Delta table air\\_quality\\_monthly\\_avg (append mode)\n",
    "    \n",
    "*   Include progress updates and execution times\n",
    "    \n",
    "*   Display sample results and summary statistics\n",
    "    \n",
    "\n",
    "**Output**\n",
    "----------\n",
    "\n",
    "A complete Databricks python code that performs monthly aggregation of air quality data with proper error handling and result visualization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a8c3271f-8849-4eb8-b85c-c94a943f0343",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Databricks notebook - Python\n",
    "\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import to_timestamp, year, month, current_timestamp, col, avg\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, TimestampType, IntegerType\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(\"air_quality_ingest\")\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# ---- Configuration ----\n",
    "API_URL = (\"https://air-quality-api.open-meteo.com/v1/air-quality\"\n",
    "           \"?latitude=40.3548&longitude=18.1724\"\n",
    "           \"&hourly=pm10,pm2_5,carbon_monoxide,carbon_dioxide,nitrogen_dioxide,sulphur_dioxide,ozone\"\n",
    "           \"&start_date=2025-03-01&end_date=2025-08-31\")\n",
    "\n",
    "DELTA_TABLE_NAME = \"air_quality_monthly_avg\"\n",
    "\n",
    "# Pollutant keys expected in hourly block\n",
    "POLLUTANTS = [\n",
    "    \"pm10\",\n",
    "    \"pm2_5\",\n",
    "    \"carbon_monoxide\",\n",
    "    \"carbon_dioxide\",\n",
    "    \"nitrogen_dioxide\",\n",
    "    \"sulphur_dioxide\",\n",
    "    \"ozone\"\n",
    "]\n",
    "\n",
    "# ---- Utility timing helper ----\n",
    "def now_ts():\n",
    "    return datetime.utcnow().isoformat() + \"Z\"\n",
    "\n",
    "# ---- 1) Extract Data ----\n",
    "t0 = time.perf_counter()\n",
    "logger.info(\"Starting extraction from API at %s\", now_ts())\n",
    "\n",
    "try:\n",
    "    resp = requests.get(API_URL, timeout=30)\n",
    "    resp.raise_for_status()\n",
    "    data = resp.json()\n",
    "    logger.info(\"API request successful. HTTP status: %s\", resp.status_code)\n",
    "except requests.RequestException as e:\n",
    "    logger.exception(\"Request to air-quality API failed: %s\", e)\n",
    "    raise\n",
    "\n",
    "t1 = time.perf_counter()\n",
    "logger.info(\"Extraction completed in %.2f seconds\", t1 - t0)\n",
    "\n",
    "# ---- Validate and find hourly data ----\n",
    "if \"hourly\" not in data:\n",
    "    raise ValueError(\"API response does not contain 'hourly' field. Response keys: %s\" % list(data.keys()))\n",
    "\n",
    "hourly = data[\"hourly\"]\n",
    "\n",
    "# Check required keys present\n",
    "expected_keys = set(POLLUTANTS + [\"time\"])\n",
    "missing = [k for k in ([\"time\"] + POLLUTANTS) if k not in hourly]\n",
    "if missing:\n",
    "    raise ValueError(f\"Missing expected keys in hourly payload: {missing}\")\n",
    "\n",
    "times = hourly[\"time\"]\n",
    "n = len(times)\n",
    "logger.info(\"Number of hourly timestamps received: %d\", n)\n",
    "\n",
    "# Ensure pollutant arrays match length (if not, we'll handle gracefully)\n",
    "for p in POLLUTANTS:\n",
    "    if not isinstance(hourly.get(p), list):\n",
    "        raise ValueError(f\"Hourly pollutant '{p}' is not a list in response.\")\n",
    "    if len(hourly[p]) != n:\n",
    "        # Option: align to shortest length\n",
    "        logger.warning(\"Length mismatch for '%s' (%d) vs time (%d). Aligning to min length.\", p, len(hourly[p]), n)\n",
    "\n",
    "min_len = min([len(hourly[k]) for k in ([\"time\"] + POLLUTANTS)])\n",
    "if min_len != n:\n",
    "    logger.info(\"Truncating all arrays to min length %d to ensure row alignment.\", min_len)\n",
    "    times = times[:min_len]\n",
    "\n",
    "# ---- 2) Transform Data: build rows and create PySpark DataFrame ----\n",
    "t2 = time.perf_counter()\n",
    "logger.info(\"Starting transformation/parsing at %s\", now_ts())\n",
    "\n",
    "rows = []\n",
    "for i in range(min_len):\n",
    "    row = {\"time\": times[i]}\n",
    "    for p in POLLUTANTS:\n",
    "        # Some values may be null (None in JSON)\n",
    "        val = hourly[p][i]\n",
    "        # Keep None as-is; pandas will hold NaN for float values\n",
    "        row[p] = val\n",
    "    rows.append(row)\n",
    "\n",
    "# Create a pandas DataFrame first (small to medium-sized payload)\n",
    "pdf = pd.DataFrame(rows)\n",
    "logger.info(\"Created pandas DataFrame with shape %s\", pdf.shape)\n",
    "\n",
    "# Convert empty strings to NaN if any (defensive)\n",
    "pdf.replace(\"\", pd.NA, inplace=True)\n",
    "\n",
    "# Create Spark DataFrame: specify schema to get consistent dtypes\n",
    "schema = StructType([\n",
    "    StructField(\"time\", StringType(), True),\n",
    "    StructField(\"pm10\", DoubleType(), True),\n",
    "    StructField(\"pm2_5\", DoubleType(), True),\n",
    "    StructField(\"carbon_monoxide\", DoubleType(), True),\n",
    "    StructField(\"carbon_dioxide\", DoubleType(), True),\n",
    "    StructField(\"nitrogen_dioxide\", DoubleType(), True),\n",
    "    StructField(\"sulphur_dioxide\", DoubleType(), True),\n",
    "    StructField(\"ozone\", DoubleType(), True),\n",
    "])\n",
    "\n",
    "sdf = spark.createDataFrame(pdf, schema=schema)\n",
    "\n",
    "# Convert time string to timestamp type (assuming ISO format)\n",
    "sdf = sdf.withColumn(\"timestamp\", to_timestamp(col(\"time\")))\n",
    "# Extract year and month\n",
    "sdf = sdf.withColumn(\"year\", year(col(\"timestamp\"))).withColumn(\"month\", month(col(\"timestamp\")))\n",
    "# Add ingestion_date column\n",
    "sdf = sdf.withColumn(\"ingestion_date\", current_timestamp())\n",
    "\n",
    "# Reorder / select useful columns\n",
    "select_cols = [\"timestamp\", \"year\", \"month\", \"ingestion_date\"] + POLLUTANTS\n",
    "sdf = sdf.select(*select_cols)\n",
    "\n",
    "t3 = time.perf_counter()\n",
    "logger.info(\"Transformation completed in %.2f seconds\", t3 - t2)\n",
    "logger.info(\"Spark DataFrame schema:\")\n",
    "sdf.printSchema()\n",
    "logger.info(\"Total rows in Spark DataFrame: %d\", sdf.count())\n",
    "\n",
    "# ---- 3) Monthly Aggregation ----\n",
    "t4 = time.perf_counter()\n",
    "logger.info(\"Starting monthly aggregation at %s\", now_ts())\n",
    "\n",
    "# Build aggregation expression - average for each pollutant.\n",
    "# Spark's avg ignores nulls by default, so nulls are handled automatically.\n",
    "agg_exprs = [avg(col(p)).alias(f\"avg_{p}\") for p in POLLUTANTS]\n",
    "\n",
    "monthly = sdf.groupBy(\"year\", \"month\").agg(*agg_exprs).orderBy(\"year\", \"month\")\n",
    "\n",
    "t5 = time.perf_counter()\n",
    "logger.info(\"Aggregation completed in %.2f seconds\", t5 - t4)\n",
    "logger.info(\"Number of monthly groups: %d\", monthly.count())\n",
    "\n",
    "# Add metadata columns (ingestion_date = now) to the monthly summary\n",
    "monthly = monthly.withColumn(\"ingestion_date\", current_timestamp())\n",
    "\n",
    "# ---- 4) Save Results to Delta table ----\n",
    "t6 = time.perf_counter()\n",
    "logger.info(\"Saving monthly aggregated results to Delta table '%s' (append mode).\", DELTA_TABLE_NAME)\n",
    "\n",
    "# Optionally enable autoMerge (if schema evolution required) - uncomment if needed\n",
    "# spark.conf.set(\"spark.databricks.delta.schema.autoMerge.enabled\", \"true\")\n",
    "\n",
    "try:\n",
    "    monthly.write.format(\"delta\").mode(\"append\").saveAsTable(DELTA_TABLE_NAME)\n",
    "    logger.info(\"Write to Delta table completed.\")\n",
    "except Exception as e:\n",
    "    logger.exception(\"Failed to write to Delta table: %s\", e)\n",
    "    raise\n",
    "\n",
    "t7 = time.perf_counter()\n",
    "logger.info(\"Save completed in %.2f seconds\", t7 - t6)\n",
    "\n",
    "# ---- Display sample results and summary stats ----\n",
    "logger.info(\"Displaying sample monthly aggregated rows:\")\n",
    "display(monthly.limit(10))   # In Databricks, display() renders DataFrame nicely. If not available, use show().\n",
    "# Also show descriptive statistics for the numeric average columns\n",
    "logger.info(\"Summary statistics for monthly averages (describe):\")\n",
    "display(monthly.select([f\"avg_{p}\" for p in POLLUTANTS]).summary())\n",
    "\n",
    "t8 = time.perf_counter()\n",
    "total_elapsed = t8 - t0\n",
    "logger.info(\"Total pipeline execution time: %.2f seconds\", total_elapsed)\n",
    "\n",
    "# Print final counts and times\n",
    "print(\"==== PIPELINE COMPLETE ====\")\n",
    "print(f\"Records (hourly) processed: {sdf.count()}\")\n",
    "print(f\"Monthly groups produced: {monthly.count()}\")\n",
    "print(f\"Total time: {total_elapsed:.2f} seconds\")\n",
    "print(f\"Delta table appended: {DELTA_TABLE_NAME}\")\n",
    "print(\"Sample monthly results (first 10 rows):\")\n",
    "monthly.show(10, truncate=False)\n",
    "\n",
    "# If you would like to query the table afterwards, example:\n",
    "print(\"Example SELECT from Delta table:\")\n",
    "spark.sql(f\"SELECT * FROM {DELTA_TABLE_NAME} ORDER BY year, month LIMIT 10\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a8789561-b05f-4f36-8014-ce5889c3ce9e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "GPT 5 mini work at the first attempt"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "GPT 5 mini",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
