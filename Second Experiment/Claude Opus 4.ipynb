{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d41243e5-bbf0-4e38-a4fa-109bb9008577",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Task**\n",
    "--------\n",
    "\n",
    "Create a Databricks python code that extracts air quality data from an API, groups it by month, and calculates monthly averages for all pollutant measurements.\n",
    "\n",
    "**Requirements**\n",
    "----------------\n",
    "\n",
    "### **1\\. Extract Data**\n",
    "\n",
    "*   Fetch air quality data from: https://air-quality-api.open-meteo.com/v1/air-quality?latitude=40.3548&longitude=18.1724&hourly=pm10,pm2\\_5,carbon\\_monoxide,carbon\\_dioxide,nitrogen\\_dioxide,sulphur\\_dioxide,ozone&start\\_date=2025-03-01&end\\_date=2025-08-31\n",
    "    \n",
    "*   Use Python requests to get the JSON response\n",
    "    \n",
    "\n",
    "### **2\\. Transform Data**\n",
    "\n",
    "*   Parse the JSON hourly data into a PySpark DataFrame\n",
    "    \n",
    "*   Convert time and pollutant lists into structured rows\n",
    "    \n",
    "*   Extract year and month from timestamp for grouping\n",
    "    \n",
    "*   Add an ingestion\\_date column\n",
    "    \n",
    "\n",
    "### **3\\. Monthly Aggregation**\n",
    "\n",
    "*   Group data by year and month\n",
    "    \n",
    "*   Calculate average values for each pollutant (pm10, pm2\\_5, carbon\\_monoxide, etc.)\n",
    "    \n",
    "*   Handle null values appropriately during aggregation\n",
    "    \n",
    "*   Create a summary DataFrame with monthly averages\n",
    "    \n",
    "\n",
    "### **4\\. Save Results**\n",
    "\n",
    "*   Save the monthly aggregated data to Delta table air\\_quality\\_monthly\\_avg (append mode)\n",
    "    \n",
    "*   Include progress updates and execution times\n",
    "    \n",
    "*   Display sample results and summary statistics\n",
    "    \n",
    "\n",
    "**Output**\n",
    "----------\n",
    "\n",
    "A complete Databricks python code that performs monthly aggregation of air quality data with proper error handling and result visualization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1bf38f6b-1350-40e9-8c9e-c034b933feae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Databricks notebook source\n",
    "# MAGIC %md\n",
    "# MAGIC # Air Quality Data Monthly Aggregation Pipeline\n",
    "# MAGIC \n",
    "# MAGIC This notebook extracts air quality data from Open-Meteo API, transforms it into a structured format,\n",
    "# MAGIC calculates monthly averages for all pollutants, and saves the results to a Delta table.\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Import required libraries\n",
    "import requests\n",
    "import json\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    col, explode, arrays_zip, to_timestamp, \n",
    "    year, month, avg, count, when, isnan, \n",
    "    current_timestamp, lit, round as spark_round\n",
    ")\n",
    "from pyspark.sql.types import *\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"AirQualityMonthlyAggregation\").getOrCreate()\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## 1. Extract Data from API\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "def fetch_air_quality_data(api_url):\n",
    "    \"\"\"\n",
    "    Fetch air quality data from the Open-Meteo API\n",
    "    \n",
    "    Args:\n",
    "        api_url (str): The API endpoint URL\n",
    "        \n",
    "    Returns:\n",
    "        dict: JSON response from the API\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    print(f\"Fetching data from API at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(api_url, timeout=30)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        data = response.json()\n",
    "        elapsed_time = time.time() - start_time\n",
    "        print(f\"âœ“ Data fetched successfully in {elapsed_time:.2f} seconds\")\n",
    "        print(f\"  - Location: Latitude {data['latitude']}, Longitude {data['longitude']}\")\n",
    "        print(f\"  - Time range: {len(data['hourly']['time'])} hourly records\")\n",
    "        \n",
    "        return data\n",
    "    \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"âœ— Error fetching data: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# API URL with parameters\n",
    "api_url = \"https://air-quality-api.open-meteo.com/v1/air-quality?latitude=40.3548&longitude=18.1724&hourly=pm10,pm2_5,carbon_monoxide,carbon_dioxide,nitrogen_dioxide,sulphur_dioxide,ozone&start_date=2025-03-01&end_date=2025-08-31\"\n",
    "\n",
    "# Fetch the data\n",
    "air_quality_data = fetch_air_quality_data(api_url)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## 2. Transform Data into PySpark DataFrame\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "def create_air_quality_dataframe(data):\n",
    "    \"\"\"\n",
    "    Transform JSON data into a structured PySpark DataFrame\n",
    "    \n",
    "    Args:\n",
    "        data (dict): JSON response from API\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame: Structured PySpark DataFrame with air quality data\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    print(f\"\\nTransforming data to DataFrame at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    \n",
    "    # Extract hourly data\n",
    "    hourly_data = data['hourly']\n",
    "    \n",
    "    # Create lists for DataFrame creation\n",
    "    rows = []\n",
    "    for i in range(len(hourly_data['time'])):\n",
    "        row = {\n",
    "            'timestamp': hourly_data['time'][i],\n",
    "            'pm10': hourly_data['pm10'][i],\n",
    "            'pm2_5': hourly_data['pm2_5'][i],\n",
    "            'carbon_monoxide': hourly_data['carbon_monoxide'][i],\n",
    "            'carbon_dioxide': hourly_data['carbon_dioxide'][i],\n",
    "            'nitrogen_dioxide': hourly_data['nitrogen_dioxide'][i],\n",
    "            'sulphur_dioxide': hourly_data['sulphur_dioxide'][i],\n",
    "            'ozone': hourly_data['ozone'][i]\n",
    "        }\n",
    "        rows.append(row)\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = spark.createDataFrame(rows)\n",
    "    \n",
    "    # Convert timestamp string to timestamp type and add date components\n",
    "    df = df.withColumn(\"timestamp\", to_timestamp(col(\"timestamp\"), \"yyyy-MM-dd'T'HH:mm\")) \\\n",
    "           .withColumn(\"year\", year(col(\"timestamp\"))) \\\n",
    "           .withColumn(\"month\", month(col(\"timestamp\"))) \\\n",
    "           .withColumn(\"ingestion_date\", current_timestamp())\n",
    "    \n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f\"âœ“ DataFrame created successfully in {elapsed_time:.2f} seconds\")\n",
    "    print(f\"  - Total records: {df.count()}\")\n",
    "    print(f\"  - Columns: {', '.join(df.columns)}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Create the DataFrame\n",
    "df_air_quality = create_air_quality_dataframe(air_quality_data)\n",
    "\n",
    "# Display sample data\n",
    "print(\"\\nSample data (first 10 rows):\")\n",
    "df_air_quality.show(10, truncate=False)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## 3. Data Quality Check and Null Value Analysis\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "def analyze_data_quality(df):\n",
    "    \"\"\"\n",
    "    Analyze data quality and null values in the DataFrame\n",
    "    \n",
    "    Args:\n",
    "        df (DataFrame): Input DataFrame\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame: Summary statistics\n",
    "    \"\"\"\n",
    "    print(f\"\\nAnalyzing data quality at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    \n",
    "    # Get pollutant columns\n",
    "    pollutant_cols = ['pm10', 'pm2_5', 'carbon_monoxide', 'carbon_dioxide', \n",
    "                      'nitrogen_dioxide', 'sulphur_dioxide', 'ozone']\n",
    "    \n",
    "    # Calculate null counts for each pollutant\n",
    "    null_counts = []\n",
    "    total_count = df.count()\n",
    "    \n",
    "    for col_name in pollutant_cols:\n",
    "        null_count = df.filter(col(col_name).isNull() | isnan(col(col_name))).count()\n",
    "        null_percentage = (null_count / total_count) * 100\n",
    "        null_counts.append({\n",
    "            'pollutant': col_name,\n",
    "            'null_count': null_count,\n",
    "            'null_percentage': round(null_percentage, 2),\n",
    "            'valid_count': total_count - null_count\n",
    "        })\n",
    "    \n",
    "    # Create summary DataFrame\n",
    "    null_summary_df = spark.createDataFrame(null_counts)\n",
    "    \n",
    "    print(\"\\nData Quality Summary:\")\n",
    "    null_summary_df.show(truncate=False)\n",
    "    \n",
    "    return null_summary_df\n",
    "\n",
    "# Analyze data quality\n",
    "data_quality_summary = analyze_data_quality(df_air_quality)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## 4. Monthly Aggregation\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "def calculate_monthly_averages(df):\n",
    "    \"\"\"\n",
    "    Calculate monthly average values for all pollutants\n",
    "    \n",
    "    Args:\n",
    "        df (DataFrame): Input DataFrame with hourly data\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame: Monthly aggregated data\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    print(f\"\\nCalculating monthly averages at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    \n",
    "    # Define pollutant columns\n",
    "    pollutant_cols = ['pm10', 'pm2_5', 'carbon_monoxide', 'carbon_dioxide', \n",
    "                      'nitrogen_dioxide', 'sulphur_dioxide', 'ozone']\n",
    "    \n",
    "    # Create aggregation expressions\n",
    "    agg_exprs = []\n",
    "    for col_name in pollutant_cols:\n",
    "        # Calculate average ignoring null values\n",
    "        agg_exprs.append(\n",
    "            spark_round(avg(when(col(col_name).isNotNull() & ~isnan(col(col_name)), col(col_name))), 2)\n",
    "            .alias(f\"avg_{col_name}\")\n",
    "        )\n",
    "        # Count non-null values\n",
    "        agg_exprs.append(\n",
    "            count(when(col(col_name).isNotNull() & ~isnan(col(col_name)), col(col_name)))\n",
    "            .alias(f\"count_{col_name}\")\n",
    "        )\n",
    "    \n",
    "    # Add total record count\n",
    "    agg_exprs.append(count(\"*\").alias(\"total_records\"))\n",
    "    \n",
    "    # Perform monthly aggregation\n",
    "    monthly_avg_df = df.groupBy(\"year\", \"month\") \\\n",
    "                       .agg(*agg_exprs) \\\n",
    "                       .orderBy(\"year\", \"month\")\n",
    "    \n",
    "    # Add metadata columns\n",
    "    monthly_avg_df = monthly_avg_df.withColumn(\"aggregation_date\", current_timestamp()) \\\n",
    "                                   .withColumn(\"latitude\", lit(40.3548)) \\\n",
    "                                   .withColumn(\"longitude\", lit(18.1724))\n",
    "    \n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f\"âœ“ Monthly aggregation completed in {elapsed_time:.2f} seconds\")\n",
    "    print(f\"  - Months aggregated: {monthly_avg_df.count()}\")\n",
    "    \n",
    "    return monthly_avg_df\n",
    "\n",
    "# Calculate monthly averages\n",
    "df_monthly_avg = calculate_monthly_averages(df_air_quality)\n",
    "\n",
    "# Display results\n",
    "print(\"\\nMonthly Average Air Quality Data:\")\n",
    "df_monthly_avg.select(\"year\", \"month\", \"avg_pm10\", \"avg_pm2_5\", \"avg_carbon_monoxide\", \n",
    "                      \"avg_nitrogen_dioxide\", \"avg_ozone\", \"total_records\").show(truncate=False)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## 5. Create Summary Statistics\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "def create_summary_statistics(df):\n",
    "    \"\"\"\n",
    "    Create summary statistics for the monthly aggregated data\n",
    "    \n",
    "    Args:\n",
    "        df (DataFrame): Monthly aggregated DataFrame\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame: Summary statistics\n",
    "    \"\"\"\n",
    "    print(f\"\\nGenerating summary statistics at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    \n",
    "    # Calculate overall statistics for each pollutant\n",
    "    pollutant_cols = ['avg_pm10', 'avg_pm2_5', 'avg_carbon_monoxide', 'avg_carbon_dioxide', \n",
    "                      'avg_nitrogen_dioxide', 'avg_sulphur_dioxide', 'avg_ozone']\n",
    "    \n",
    "    summary_stats = []\n",
    "    for col_name in pollutant_cols:\n",
    "        stats = df.select(col_name).summary(\"min\", \"max\", \"mean\", \"stddev\").collect()\n",
    "        \n",
    "        pollutant_name = col_name.replace(\"avg_\", \"\")\n",
    "        summary_stats.append({\n",
    "            'pollutant': pollutant_name,\n",
    "            'min_monthly_avg': float(stats[0][1]) if stats[0][1] else None,\n",
    "            'max_monthly_avg': float(stats[1][1]) if stats[1][1] else None,\n",
    "            'overall_mean': round(float(stats[2][1]), 2) if stats[2][1] else None,\n",
    "            'std_deviation': round(float(stats[3][1]), 2) if stats[3][1] else None\n",
    "        })\n",
    "    \n",
    "    summary_df = spark.createDataFrame(summary_stats)\n",
    "    \n",
    "    print(\"\\nOverall Summary Statistics:\")\n",
    "    summary_df.show(truncate=False)\n",
    "    \n",
    "    return summary_df\n",
    "\n",
    "# Generate summary statistics\n",
    "summary_stats_df = create_summary_statistics(df_monthly_avg)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## 6. Save Results to Delta Table\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "def save_to_delta_table(df, table_name, mode=\"append\"):\n",
    "    \"\"\"\n",
    "    Save DataFrame to Delta table\n",
    "    \n",
    "    Args:\n",
    "        df (DataFrame): DataFrame to save\n",
    "        table_name (str): Name of the Delta table\n",
    "        mode (str): Save mode (append, overwrite, etc.)\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    print(f\"\\nSaving to Delta table '{table_name}' at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    \n",
    "    try:\n",
    "        # Save to Delta table\n",
    "        df.write \\\n",
    "          .mode(mode) \\\n",
    "          .option(\"mergeSchema\", \"true\") \\\n",
    "          .saveAsTable(table_name)\n",
    "        \n",
    "        elapsed_time = time.time() - start_time\n",
    "        print(f\"âœ“ Data saved successfully to '{table_name}' in {elapsed_time:.2f} seconds\")\n",
    "        \n",
    "        # Verify the save\n",
    "        record_count = spark.table(table_name).count()\n",
    "        print(f\"  - Total records in table: {record_count}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âœ— Error saving to Delta table: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# Save monthly aggregated data to Delta table\n",
    "save_to_delta_table(df_monthly_avg, \"air_quality_monthly_avg\", mode=\"append\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## 7. Visualization and Final Summary\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Create a visualization-friendly DataFrame\n",
    "viz_df = df_monthly_avg.select(\n",
    "    \"year\", \n",
    "    \"month\",\n",
    "    \"avg_pm10\",\n",
    "    \"avg_pm2_5\",\n",
    "    \"avg_ozone\",\n",
    "    \"avg_nitrogen_dioxide\"\n",
    ").orderBy(\"year\", \"month\")\n",
    "\n",
    "print(\"\\nMonthly Trends Summary:\")\n",
    "viz_df.show(truncate=False)\n",
    "\n",
    "# Display execution summary\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EXECUTION SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"âœ“ Data extraction: Successfully fetched {df_air_quality.count()} hourly records\")\n",
    "print(f\"âœ“ Data transformation: Created structured DataFrame with {len(df_air_quality.columns)} columns\")\n",
    "print(f\"âœ“ Monthly aggregation: Calculated averages for {df_monthly_avg.count()} months\")\n",
    "print(f\"âœ“ Data persistence: Saved results to 'air_quality_monthly_avg' Delta table\")\n",
    "print(f\"âœ“ Execution completed at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## 8. Query Verification\n",
    "# MAGIC \n",
    "# MAGIC Verify the saved data by querying the Delta table\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Query the Delta table to verify the save\n",
    "print(\"Verifying saved data in Delta table:\")\n",
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        year,\n",
    "        month,\n",
    "        ROUND(avg_pm10, 2) as pm10,\n",
    "        ROUND(avg_pm2_5, 2) as pm2_5,\n",
    "        ROUND(avg_ozone, 2) as ozone,\n",
    "        ROUND(avg_nitrogen_dioxide, 2) as no2,\n",
    "        total_records,\n",
    "        aggregation_date\n",
    "    FROM air_quality_monthly_avg\n",
    "    ORDER BY year DESC, month DESC\n",
    "    LIMIT 10\n",
    "\"\"\").show(truncate=False)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Pipeline Complete! ðŸŽ‰\n",
    "# MAGIC \n",
    "# MAGIC The air quality data has been successfully:\n",
    "# MAGIC 1. Extracted from the Open-Meteo API\n",
    "# MAGIC 2. Transformed into a structured format\n",
    "# MAGIC 3. Aggregated by month with proper null handling\n",
    "# MAGIC 4. Saved to the `air_quality_monthly_avg` Delta table\n",
    "# MAGIC \n",
    "# MAGIC The data is now ready for further analysis, reporting, or visualization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0e66eef3-27bf-4394-a749-4b794db452bb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Claude Opus 4 work at the first attempt"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Claude Opus 4",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
