{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0f48984e-5453-4462-9656-b7e2b0b33436",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Task**\n",
    "--------\n",
    "\n",
    "Create a Databricks python code that extracts air quality data from an API, groups it by month, and calculates monthly averages for all pollutant measurements.\n",
    "\n",
    "**Requirements**\n",
    "----------------\n",
    "\n",
    "### **1\\. Extract Data**\n",
    "\n",
    "*   Fetch air quality data from: https://air-quality-api.open-meteo.com/v1/air-quality?latitude=40.3548&longitude=18.1724&hourly=pm10,pm2\\_5,carbon\\_monoxide,carbon\\_dioxide,nitrogen\\_dioxide,sulphur\\_dioxide,ozone&start\\_date=2025-03-01&end\\_date=2025-08-31\n",
    "    \n",
    "*   Use Python requests to get the JSON response\n",
    "    \n",
    "\n",
    "### **2\\. Transform Data**\n",
    "\n",
    "*   Parse the JSON hourly data into a PySpark DataFrame\n",
    "    \n",
    "*   Convert time and pollutant lists into structured rows\n",
    "    \n",
    "*   Extract year and month from timestamp for grouping\n",
    "    \n",
    "*   Add an ingestion\\_date column\n",
    "    \n",
    "\n",
    "### **3\\. Monthly Aggregation**\n",
    "\n",
    "*   Group data by year and month\n",
    "    \n",
    "*   Calculate average values for each pollutant (pm10, pm2\\_5, carbon\\_monoxide, etc.)\n",
    "    \n",
    "*   Handle null values appropriately during aggregation\n",
    "    \n",
    "*   Create a summary DataFrame with monthly averages\n",
    "    \n",
    "\n",
    "### **4\\. Save Results**\n",
    "\n",
    "*   Save the monthly aggregated data to Delta table air\\_quality\\_monthly\\_avg (append mode)\n",
    "    \n",
    "*   Include progress updates and execution times\n",
    "    \n",
    "*   Display sample results and summary statistics\n",
    "    \n",
    "\n",
    "**Output**\n",
    "----------\n",
    "\n",
    "A complete Databricks python code that performs monthly aggregation of air quality data with proper error handling and result visualization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f7572c3e-45aa-4ac0-bb5c-180841924d55",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Databricks Notebook: Air Quality Data Processing Pipeline\n",
    "\n",
    "import requests\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, avg, year, month, to_timestamp, current_timestamp, lit\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, TimestampType\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# Initialize Spark Session (Databricks handles this automatically, but good for clarity)\n",
    "spark = SparkSession.builder.appName(\"AirQualityMonthlyAggregation\").getOrCreate()\n",
    "\n",
    "# ----------------------------\n",
    "# 1. Extract Data from API\n",
    "# ----------------------------\n",
    "\n",
    "print(\"üöÄ Starting air quality data extraction...\")\n",
    "\n",
    "# Define API endpoint\n",
    "api_url = (\n",
    "    \"https://air-quality-api.open-meteo.com/v1/air-quality\"\n",
    "    \"?latitude=40.3548\"\n",
    "    \"&longitude=18.1724\"\n",
    "    \"&hourly=pm10,pm2_5,carbon_monoxide,carbon_dioxide,nitrogen_dioxide,sulphur_dioxide,ozone\"\n",
    "    \"&start_date=2025-03-01\"\n",
    "    \"&end_date=2025-08-31\"\n",
    ")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "try:\n",
    "    response = requests.get(api_url)\n",
    "    response.raise_for_status()  # Raise exception for HTTP errors\n",
    "    data = response.json()\n",
    "    print(f\"‚úÖ Successfully fetched data from {api_url}\")\n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(f\"‚ùå Error fetching data from API: {e}\")\n",
    "    raise\n",
    "\n",
    "# Extract hourly data\n",
    "hourly_data = data.get(\"hourly\", {})\n",
    "timestamps = hourly_data.get(\"time\", [])\n",
    "pollutants = {\n",
    "    key: hourly_data.get(key, [])\n",
    "    for key in [\n",
    "        \"pm10\", \"pm2_5\", \"carbon_monoxide\", \n",
    "        \"carbon_dioxide\", \"nitrogen_dioxide\", \n",
    "        \"sulphur_dioxide\", \"ozone\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(f\"üìä Retrieved {len(timestamps)} hourly records.\")\n",
    "\n",
    "# ----------------------------\n",
    "# 2. Transform Data into PySpark DataFrame\n",
    "# ----------------------------\n",
    "\n",
    "print(\"üîÑ Transforming data into structured format...\")\n",
    "\n",
    "# Prepare list of rows\n",
    "rows = []\n",
    "ingestion_datetime = datetime.utcnow()  # Use UTC to avoid timezone issues\n",
    "\n",
    "timestamps = hourly_data.get(\"time\", [])\n",
    "pollutants = {\n",
    "    key: hourly_data.get(key, [])\n",
    "    for key in [\n",
    "        \"pm10\", \"pm2_5\", \"carbon_monoxide\", \n",
    "        \"carbon_dioxide\", \"nitrogen_dioxide\", \n",
    "        \"sulphur_dioxide\", \"ozone\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "for i in range(len(timestamps)):\n",
    "    row = {\n",
    "        \"timestamp\": timestamps[i],\n",
    "        \"pm10\": pollutants[\"pm10\"][i] if i < len(pollutants[\"pm10\"]) else None,\n",
    "        \"pm2_5\": pollutants[\"pm2_5\"][i] if i < len(pollutants[\"pm2_5\"]) else None,\n",
    "        \"carbon_monoxide\": pollutants[\"carbon_monoxide\"][i] if i < len(pollutants[\"carbon_monoxide\"]) else None,\n",
    "        \"carbon_dioxide\": pollutants[\"carbon_dioxide\"][i] if i < len(pollutants[\"carbon_dioxide\"]) else None,\n",
    "        \"nitrogen_dioxide\": pollutants[\"nitrogen_dioxide\"][i] if i < len(pollutants[\"nitrogen_dioxide\"]) else None,\n",
    "        \"sulphur_dioxide\": pollutants[\"sulphur_dioxide\"][i] if i < len(pollutants[\"sulphur_dioxide\"]) else None,\n",
    "        \"ozone\": pollutants[\"ozone\"][i] if i < len(pollutants[\"ozone\"]) else None,\n",
    "        \"ingestion_date\": ingestion_datetime  # ‚úÖ Fixed\n",
    "    }\n",
    "    rows.append(row)\n",
    "\n",
    "# Define schema\n",
    "schema = StructType([\n",
    "    StructField(\"timestamp\", StringType(), True),\n",
    "    StructField(\"pm10\", DoubleType(), True),\n",
    "    StructField(\"pm2_5\", DoubleType(), True),\n",
    "    StructField(\"carbon_monoxide\", DoubleType(), True),\n",
    "    StructField(\"carbon_dioxide\", DoubleType(), True),\n",
    "    StructField(\"nitrogen_dioxide\", DoubleType(), True),\n",
    "    StructField(\"sulphur_dioxide\", DoubleType(), True),\n",
    "    StructField(\"ozone\", DoubleType(), True),\n",
    "    StructField(\"ingestion_date\", TimestampType(), True)\n",
    "])\n",
    "\n",
    "# Create DataFrame\n",
    "df_raw = spark.createDataFrame(rows, schema=schema)\n",
    "\n",
    "# Convert timestamp to proper timestamp type and extract year/month\n",
    "df_transformed = df_raw \\\n",
    "    .withColumn(\"timestamp\", to_timestamp(col(\"timestamp\"))) \\\n",
    "    .withColumn(\"year\", year(col(\"timestamp\"))) \\\n",
    "    .withColumn(\"month\", month(col(\"timestamp\")))\n",
    "\n",
    "print(\"‚úÖ Data transformation completed.\")\n",
    "\n",
    "# Show sample data\n",
    "print(\"üîç Sample of transformed data:\")\n",
    "df_transformed.show(10)\n",
    "\n",
    "# ----------------------------\n",
    "# 3. Monthly Aggregation\n",
    "# ----------------------------\n",
    "\n",
    "print(\"üìà Performing monthly aggregation...\")\n",
    "\n",
    "# Group by year and month, calculate average for each pollutant\n",
    "# Null values are automatically ignored in avg() in Spark\n",
    "df_monthly_avg = df_transformed.groupBy(\"year\", \"month\") \\\n",
    "    .agg(\n",
    "        avg(\"pm10\").alias(\"avg_pm10\"),\n",
    "        avg(\"pm2_5\").alias(\"avg_pm2_5\"),\n",
    "        avg(\"carbon_monoxide\").alias(\"avg_carbon_monoxide\"),\n",
    "        avg(\"carbon_dioxide\").alias(\"avg_carbon_dioxide\"),\n",
    "        avg(\"nitrogen_dioxide\").alias(\"avg_nitrogen_dioxide\"),\n",
    "        avg(\"sulphur_dioxide\").alias(\"avg_sulphur_dioxide\"),\n",
    "        avg(\"ozone\").alias(\"avg_ozone\")\n",
    "    ) \\\n",
    "    .withColumn(\"ingestion_date\", current_timestamp()) \\\n",
    "    .orderBy(\"year\", \"month\")\n",
    "\n",
    "print(\"‚úÖ Monthly aggregation completed.\")\n",
    "\n",
    "# Show results\n",
    "print(\"üìä Monthly Average Air Quality Summary:\")\n",
    "df_monthly_avg.show(truncate=False)\n",
    "\n",
    "# Display summary statistics for numeric columns\n",
    "print(\"üìâ Summary Statistics for Pollutants:\")\n",
    "df_monthly_avg.select(\n",
    "    \"avg_pm10\", \"avg_pm2_5\", \"avg_carbon_monoxide\",\n",
    "    \"avg_carbon_dioxide\", \"avg_nitrogen_dioxide\",\n",
    "    \"avg_sulphur_dioxide\", \"avg_ozone\"\n",
    ").describe().show()\n",
    "\n",
    "# ----------------------------\n",
    "# 4. Save Results to Delta Table\n",
    "# ----------------------------\n",
    "\n",
    "table_name = \"air_quality_monthly_avg\"\n",
    "\n",
    "print(f\"üíæ Saving results to Delta table: {table_name} (append mode)...\")\n",
    "\n",
    "try:\n",
    "    df_monthly_avg.write.format(\"delta\") \\\n",
    "        .mode(\"append\") \\\n",
    "        .saveAsTable(table_name)\n",
    "    print(f\"‚úÖ Data successfully appended to Delta table '{table_name}'.\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error saving to Delta table: {e}\")\n",
    "    # Suggest creating the table first if it doesn't exist\n",
    "    print(\"üí° Hint: You may need to create the table first using overwrite mode initially.\")\n",
    "    raise\n",
    "\n",
    "# ----------------------------\n",
    "# Final Execution Summary\n",
    "# ----------------------------\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "\n",
    "print(\"‚úÖ‚úÖ‚úÖ Pipeline Completed Successfully ‚úÖ‚úÖ‚úÖ\")\n",
    "print(f\"üì¶ Total Records Processed: {df_transformed.count()}\")\n",
    "print(f\"üìÖ Distinct Months Processed: {df_monthly_avg.count()}\")\n",
    "print(f\"‚è±  Execution Time: {execution_time:.2f} seconds\")\n",
    "print(f\"üìå Output Table: {table_name}\")\n",
    "\n",
    "# Optional: Display data using display() in Databricks (for visualization)\n",
    "try:\n",
    "    display(df_monthly_avg)  # Databricks native visualization\n",
    "except:\n",
    "    print(\"‚ÑπÔ∏è Note: 'display()' is only available in Databricks Runtime.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f8a71dc6-7758-4df6-b53e-6fa1a28d1b5d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "QWEN need 1 fix to work"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Qwen3-235B-A22B-2507",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
