{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e5bf70fa-7d66-4fb3-a928-0b4c2d8382f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Task**\n",
    "--------\n",
    "\n",
    "Create a Databricks python code that extracts air quality data from an API, groups it by month, and calculates monthly averages for all pollutant measurements.\n",
    "\n",
    "**Requirements**\n",
    "----------------\n",
    "\n",
    "### **1\\. Extract Data**\n",
    "\n",
    "*   Fetch air quality data from: https://air-quality-api.open-meteo.com/v1/air-quality?latitude=40.3548&longitude=18.1724&hourly=pm10,pm2\\_5,carbon\\_monoxide,carbon\\_dioxide,nitrogen\\_dioxide,sulphur\\_dioxide,ozone&start\\_date=2025-03-01&end\\_date=2025-08-31\n",
    "    \n",
    "*   Use Python requests to get the JSON response\n",
    "    \n",
    "\n",
    "### **2\\. Transform Data**\n",
    "\n",
    "*   Parse the JSON hourly data into a PySpark DataFrame\n",
    "    \n",
    "*   Convert time and pollutant lists into structured rows\n",
    "    \n",
    "*   Extract year and month from timestamp for grouping\n",
    "    \n",
    "*   Add an ingestion\\_date column\n",
    "    \n",
    "\n",
    "### **3\\. Monthly Aggregation**\n",
    "\n",
    "*   Group data by year and month\n",
    "    \n",
    "*   Calculate average values for each pollutant (pm10, pm2\\_5, carbon\\_monoxide, etc.)\n",
    "    \n",
    "*   Handle null values appropriately during aggregation\n",
    "    \n",
    "*   Create a summary DataFrame with monthly averages\n",
    "    \n",
    "\n",
    "### **4\\. Save Results**\n",
    "\n",
    "*   Save the monthly aggregated data to Delta table air\\_quality\\_monthly\\_avg (append mode)\n",
    "    \n",
    "*   Include progress updates and execution times\n",
    "    \n",
    "*   Display sample results and summary statistics\n",
    "    \n",
    "\n",
    "**Output**\n",
    "----------\n",
    "\n",
    "A complete Databricks python code that performs monthly aggregation of air quality data with proper error handling and result visualization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "626c3d40-17f7-4083-a4aa-83a394fb1932",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Databricks Python code - corrected monthly aggregation (no Window import needed)\n",
    "\n",
    "import time\n",
    "import requests\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# ----------------------------\n",
    "# 1) Extract Data\n",
    "# ----------------------------\n",
    "endpoint = (\n",
    "    \"https://air-quality-api.open-meteo.com/v1/air-quality\"\n",
    "    \"?latitude=40.3548&longitude=18.1724\"\n",
    "    \"&hourly=pm10,pm2_5,carbon_monoxide,carbon_dioxide,nitrogen_dioxide,sulphur_dioxide,ozone\"\n",
    "    \"&start_date=2025-03-01&end_date=2025-08-31\"\n",
    ")\n",
    "\n",
    "pollutants = [\n",
    "    \"pm10\",\n",
    "    \"pm2_5\",\n",
    "    \"carbon_monoxide\",\n",
    "    \"carbon_dioxide\",\n",
    "    \"nitrogen_dioxide\",\n",
    "    \"sulphur_dioxide\",\n",
    "    \"ozone\",\n",
    "]\n",
    "\n",
    "print(\"Starting data extraction from API...\")\n",
    "start_time = time.time()\n",
    "\n",
    "try:\n",
    "    resp = requests.get(endpoint, timeout=120)\n",
    "    resp.raise_for_status()\n",
    "    data = resp.json()\n",
    "    print(\"API request successful.\")\n",
    "except Exception as e:\n",
    "    print(f\"ERROR during API request: {e}\")\n",
    "    raise\n",
    "\n",
    "fetch_time = time.time() - start_time\n",
    "print(f\"Data retrieval completed in {fetch_time:.2f} seconds.\")\n",
    "\n",
    "# ----------------------------\n",
    "# 2) Transform Data\n",
    "# ----------------------------\n",
    "print(\"Transforming JSON into Spark DataFrame...\")\n",
    "trans_start = time.time()\n",
    "\n",
    "hourly = data.get(\"hourly\", {})\n",
    "times = hourly.get(\"time\", [])\n",
    "\n",
    "# Build a list of dict records: one row per hour\n",
    "records = []\n",
    "for idx, ts in enumerate(times):\n",
    "    row = {\"timestamp\": ts}\n",
    "    for p in pollutants:\n",
    "        arr = hourly.get(p, [])\n",
    "        row[p] = arr[idx] if idx < len(arr) else None\n",
    "    records.append(row)\n",
    "\n",
    "# Create Spark DataFrame\n",
    "df = spark.createDataFrame(records)\n",
    "\n",
    "# Convert timestamp string to actual Timestamp type\n",
    "df = df.withColumn(\"timestamp\", F.to_timestamp(F.col(\"timestamp\"), \"yyyy-MM-dd'T'HH:mm\"))\n",
    "\n",
    "# Extract year and month for grouping\n",
    "df = (\n",
    "    df\n",
    "    .withColumn(\"year\", F.year(F.col(\"timestamp\")))\n",
    "    .withColumn(\"month\", F.month(F.col(\"timestamp\")))\n",
    "    .withColumn(\"ingestion_date\", F.current_date())\n",
    ")\n",
    "\n",
    "row_count = df.count()\n",
    "print(f\"Transformed DataFrame has {row_count} rows and {len(df.columns)} columns.\")\n",
    "trans_time = time.time() - trans_start\n",
    "print(f\"Transformation completed in {trans_time:.2f} seconds.\")\n",
    "\n",
    "# ----------------------------\n",
    "# 3) Monthly Aggregation\n",
    "# ----------------------------\n",
    "print(\"Computing monthly averages for all pollutants (aligned to existing Delta table schema)...\")\n",
    "agg_start = time.time()\n",
    "\n",
    "# Averages for each pollutant, named to match the target 'avg_<pollutant>' columns\n",
    "agg_exprs = [\n",
    "    F.avg(\"pm10\").alias(\"avg_pm10\"),\n",
    "    F.avg(\"pm2_5\").alias(\"avg_pm2_5\"),\n",
    "    F.avg(\"carbon_monoxide\").alias(\"avg_carbon_monoxide\"),\n",
    "    F.avg(\"carbon_dioxide\").alias(\"avg_carbon_dioxide\"),\n",
    "    F.avg(\"nitrogen_dioxide\").alias(\"avg_nitrogen_dioxide\"),\n",
    "    F.avg(\"sulphur_dioxide\").alias(\"avg_sulphur_dioxide\"),\n",
    "    F.avg(\"ozone\").alias(\"avg_ozone\"),\n",
    "]\n",
    "\n",
    "monthly = (\n",
    "    df\n",
    "    .groupBy(\"year\", \"month\")\n",
    "    .agg(*agg_exprs, F.count(\"*\").alias(\"total_records\"))\n",
    "    .withColumn(\"processing_date\", F.current_timestamp())\n",
    "    .orderBy(\"year\", \"month\")\n",
    ")\n",
    "\n",
    "agg_rows = monthly.count()\n",
    "print(f\"Monthly aggregated rows: {agg_rows}\")\n",
    "agg_time = time.time() - agg_start\n",
    "print(f\"Aggregation completed in {agg_time:.2f} seconds.\")\n",
    "\n",
    "# ----------------------------\n",
    "# 4) Save Results\n",
    "# ----------------------------\n",
    "print(\"Saving results to Delta table air_quality_monthly_avg (append with schema merge)...\")\n",
    "write_start = time.time()\n",
    "\n",
    "try:\n",
    "    monthly.write.format(\"delta\").mode(\"append\") \\\n",
    "        .option(\"mergeSchema\", \"true\") \\\n",
    "        .saveAsTable(\"air_quality_monthly_avg\")\n",
    "    print(\"Data successfully written to Delta table air_quality_monthly_avg.\")\n",
    "except Exception as e:\n",
    "    print(f\"ERROR while writing to Delta (mergeSchema): {e}\")\n",
    "    print(\"Attempting fallback: write to a separate table to avoid blocking the pipeline.\")\n",
    "    try:\n",
    "        monthly.write.format(\"delta\").mode(\"append\").saveAsTable(\"air_quality_monthly_avg_v2\")\n",
    "        print(\"Data written to Delta table air_quality_monthly_avg_v2 as a fallback.\")\n",
    "    except Exception as e2:\n",
    "        print(f\"Fallback write failed as well: {e2}\")\n",
    "        raise\n",
    "\n",
    "write_time = time.time() - write_start\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "print(f\"Total pipeline time: {total_time:.2f} seconds.\")\n",
    "\n",
    "# ----------------------------\n",
    "# 5) Results Visualization\n",
    "# ----------------------------\n",
    "print(\"\\nSample of monthly averages:\")\n",
    "monthly.show(5, truncate=False)\n",
    "\n",
    "print(\"\\nSummary statistics for monthly averages:\")\n",
    "monthly.describe().show(truncate=False)\n",
    "\n",
    "# End of script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a3919986-d289-48c4-9b7f-87249037c42e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "GPT 5 need 2 fix to work"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "GPT 5",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
