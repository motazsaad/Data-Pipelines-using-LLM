{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "42357349-b72a-402f-a51a-9ac67ca1cb24",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Task**\n",
    "--------\n",
    "\n",
    "Create a Databricks python code that extracts air quality data from an API, groups it by month, and calculates monthly averages for all pollutant measurements.\n",
    "\n",
    "**Requirements**\n",
    "----------------\n",
    "\n",
    "### **1\\. Extract Data**\n",
    "\n",
    "*   Fetch air quality data from: https://air-quality-api.open-meteo.com/v1/air-quality?latitude=40.3548&longitude=18.1724&hourly=pm10,pm2\\_5,carbon\\_monoxide,carbon\\_dioxide,nitrogen\\_dioxide,sulphur\\_dioxide,ozone&start\\_date=2025-03-01&end\\_date=2025-08-31\n",
    "    \n",
    "*   Use Python requests to get the JSON response\n",
    "    \n",
    "\n",
    "### **2\\. Transform Data**\n",
    "\n",
    "*   Parse the JSON hourly data into a PySpark DataFrame\n",
    "    \n",
    "*   Convert time and pollutant lists into structured rows\n",
    "    \n",
    "*   Extract year and month from timestamp for grouping\n",
    "    \n",
    "*   Add an ingestion\\_date column\n",
    "    \n",
    "\n",
    "### **3\\. Monthly Aggregation**\n",
    "\n",
    "*   Group data by year and month\n",
    "    \n",
    "*   Calculate average values for each pollutant (pm10, pm2\\_5, carbon\\_monoxide, etc.)\n",
    "    \n",
    "*   Handle null values appropriately during aggregation\n",
    "    \n",
    "*   Create a summary DataFrame with monthly averages\n",
    "    \n",
    "\n",
    "### **4\\. Save Results**\n",
    "\n",
    "*   Save the monthly aggregated data to Delta table air\\_quality\\_monthly\\_avg (append mode)\n",
    "    \n",
    "*   Include progress updates and execution times\n",
    "    \n",
    "*   Display sample results and summary statistics\n",
    "    \n",
    "\n",
    "**Output**\n",
    "----------\n",
    "\n",
    "A complete Databricks python code that performs monthly aggregation of air quality data with proper error handling and result visualization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9514c07d-102e-4ed2-8f00-2a604bc1e4f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Databricks Python notebook cell\n",
    "# Fetch air quality data, compute monthly averages, align schema to existing Delta table, and append.\n",
    "#\n",
    "# If 'requests' is not available, run: %pip install requests\n",
    "\n",
    "import time\n",
    "from datetime import datetime\n",
    "import requests\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import DoubleType, TimestampType\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "API_URL = (\n",
    "    \"https://air-quality-api.open-meteo.com/v1/air-quality\"\n",
    "    \"?latitude=40.3548&longitude=18.1724\"\n",
    "    \"&hourly=pm10,pm2_5,carbon_monoxide,carbon_dioxide,nitrogen_dioxide,sulphur_dioxide,ozone\"\n",
    "    \"&start_date=2025-03-01&end_date=2025-08-31\"\n",
    ")\n",
    "\n",
    "TARGET_TABLE = \"air_quality_monthly_avg\"\n",
    "\n",
    "def safe_get(lst, idx):\n",
    "    try:\n",
    "        return lst[idx]\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def fetch_api(url, timeout=30):\n",
    "    print(f\"[{datetime.now()}] Fetching data from API...\")\n",
    "    t0 = time.time()\n",
    "    try:\n",
    "        resp = requests.get(url, timeout=timeout)\n",
    "        resp.raise_for_status()\n",
    "        data = resp.json()\n",
    "        elapsed = time.time() - t0\n",
    "        print(f\"[{datetime.now()}] Fetched API in {elapsed:.2f}s. Top-level keys: {list(data.keys())}\")\n",
    "        return data, elapsed\n",
    "    except Exception as e:\n",
    "        elapsed = time.time() - t0\n",
    "        print(f\"[{datetime.now()}] ERROR fetching API ({elapsed:.2f}s): {e}\")\n",
    "        raise\n",
    "\n",
    "def transform_to_spark(data):\n",
    "    print(f\"[{datetime.now()}] Transforming JSON hourly arrays into Spark DataFrame...\")\n",
    "    t0 = time.time()\n",
    "    hourly = data.get(\"hourly\", {})\n",
    "    times = hourly.get(\"time\", [])\n",
    "    pollutant_keys = [\n",
    "        \"pm10\",\n",
    "        \"pm2_5\",\n",
    "        \"carbon_monoxide\",\n",
    "        \"carbon_dioxide\",\n",
    "        \"nitrogen_dioxide\",\n",
    "        \"sulphur_dioxide\",\n",
    "        \"ozone\",\n",
    "    ]\n",
    "    pollutant_lists = {k: hourly.get(k, []) for k in pollutant_keys}\n",
    "\n",
    "    if len(times) == 0:\n",
    "        raise ValueError(\"No 'hourly.time' entries found in API response.\")\n",
    "\n",
    "    n = len(times)\n",
    "    for k, lst in pollutant_lists.items():\n",
    "        if len(lst) != n:\n",
    "            print(f\"Warning: length mismatch for {k} (len={len(lst)}) vs time (len={n}). Will safe-index.\")\n",
    "\n",
    "    rows = []\n",
    "    for i in range(n):\n",
    "        row = {\"time\": times[i]}\n",
    "        for k in pollutant_keys:\n",
    "            row[k] = safe_get(pollutant_lists.get(k, []), i)\n",
    "        rows.append(row)\n",
    "\n",
    "    df = spark.createDataFrame(rows)\n",
    "    df = df.withColumn(\"time\", F.to_timestamp(\"time\"))\n",
    "    for k in pollutant_keys:\n",
    "        df = df.withColumn(k, F.col(k).cast(DoubleType()))\n",
    "    df = df.withColumn(\"ingestion_date\", F.current_timestamp())\n",
    "    elapsed = time.time() - t0\n",
    "    count = df.count()\n",
    "    print(f\"[{datetime.now()}] Transformation done in {elapsed:.2f}s. Hourly rows: {count}\")\n",
    "    return df, elapsed\n",
    "\n",
    "# Helpers to align column naming between DataFrame and target table\n",
    "def to_avg_prefix(col_name):\n",
    "    # Convert pollutant_avg -> avg_pollutant\n",
    "    if col_name.startswith(\"avg_\"):\n",
    "        return col_name\n",
    "    if col_name.endswith(\"_avg\"):\n",
    "        core = col_name[:-4]\n",
    "        return f\"avg_{core}\"\n",
    "    return col_name\n",
    "\n",
    "def to_suffix_avg(col_name):\n",
    "    # Convert avg_pollutant -> pollutant_avg\n",
    "    if col_name.endswith(\"_avg\"):\n",
    "        return col_name\n",
    "    if col_name.startswith(\"avg_\"):\n",
    "        core = col_name[4:]\n",
    "        return f\"{core}_avg\"\n",
    "    return col_name\n",
    "\n",
    "def align_df_to_table(df, table_name):\n",
    "    \"\"\"\n",
    "    If table exists: rename df columns to match table column names, add missing table columns as nulls (with proper types),\n",
    "    and ensure ingestion_date exists when table expects it.\n",
    "    If table does not exist: rename pollutant_avg cols to avg_<pollutant> convention and add ingestion_date.\n",
    "    Returns the adjusted DataFrame ready to be written to the table.\n",
    "    \"\"\"\n",
    "    df_current = df\n",
    "    if spark.catalog.tableExists(table_name):\n",
    "        target_df = spark.table(table_name)\n",
    "        target_schema = target_df.schema  # pyspark.sql.types.StructType\n",
    "        target_cols = [f.name for f in target_schema]\n",
    "\n",
    "        print(f\"[{datetime.now()}] Target table '{table_name}' exists. Schema columns: {target_cols}\")\n",
    "\n",
    "        # Build mapping from current df column -> target column name\n",
    "        rename_map = {}\n",
    "        df_cols = df_current.columns\n",
    "\n",
    "        # Attempt direct matches first (case sensitive)\n",
    "        for c in df_cols:\n",
    "            if c in target_cols:\n",
    "                rename_map[c] = c\n",
    "\n",
    "        # For unmatched df columns, try swapping suffix/prefix avg naming\n",
    "        for c in df_cols:\n",
    "            if c in rename_map:\n",
    "                continue\n",
    "            alt = to_avg_prefix(c)  # e.g. pm10_avg -> avg_pm10 or pm10_avg->pm10_avg stays\n",
    "            if alt in target_cols:\n",
    "                rename_map[c] = alt\n",
    "                continue\n",
    "            # also check suffix form\n",
    "            alt2 = to_suffix_avg(c)\n",
    "            if alt2 in target_cols:\n",
    "                rename_map[c] = alt2\n",
    "                continue\n",
    "\n",
    "        # Apply renames\n",
    "        for src, dst in rename_map.items():\n",
    "            if src != dst:\n",
    "                df_current = df_current.withColumnRenamed(src, dst)\n",
    "                print(f\"Renamed column: {src} -> {dst}\")\n",
    "\n",
    "        # Add any target columns that are missing in df as null with proper type\n",
    "        df_cols_after = df_current.columns\n",
    "        for field in target_schema:\n",
    "            if field.name not in df_cols_after:\n",
    "                print(f\"Adding missing target column '{field.name}' as NULL (type: {field.dataType.simpleString()})\")\n",
    "                df_current = df_current.withColumn(field.name, F.lit(None).cast(field.dataType))\n",
    "\n",
    "        # Reorder columns to match target's column order (helps avoid implicit metadata mismatch)\n",
    "        df_current = df_current.select(*target_cols)\n",
    "        return df_current\n",
    "    else:\n",
    "        # Table does not exist: create using avg_ prefix naming convention for pollutant averages\n",
    "        print(f\"[{datetime.now()}] Target table '{table_name}' does not exist. Adopting avg_ prefix naming for averages.\")\n",
    "        df_current = df_current.fillna({})  # no-op but explicit\n",
    "        # Rename pollutant_avg -> avg_pollutant\n",
    "        for c in df_current.columns:\n",
    "            if c.endswith(\"_avg\"):\n",
    "                new = to_avg_prefix(c)\n",
    "                if new != c:\n",
    "                    df_current = df_current.withColumnRenamed(c, new)\n",
    "                    print(f\"Renamed column for new table: {c} -> {new}\")\n",
    "        # Ensure ingestion_date exists (we added ingestion_date in transform, but guarantee it)\n",
    "        if \"ingestion_date\" not in df_current.columns:\n",
    "            df_current = df_current.withColumn(\"ingestion_date\", F.current_timestamp())\n",
    "        # Ensure year and month exist: our aggregated DF will supply them before calling this function\n",
    "        return df_current\n",
    "\n",
    "def aggregate_monthly(df_hourly):\n",
    "    print(f\"[{datetime.now()}] Aggregating hourly data into monthly averages...\")\n",
    "    t0 = time.time()\n",
    "    df2 = df_hourly.withColumn(\"year\", F.year(\"time\")).withColumn(\"month\", F.month(\"time\"))\n",
    "    pollutant_cols = [\n",
    "        \"pm10\",\n",
    "        \"pm2_5\",\n",
    "        \"carbon_monoxide\",\n",
    "        \"carbon_dioxide\",\n",
    "        \"nitrogen_dioxide\",\n",
    "        \"sulphur_dioxide\",\n",
    "        \"ozone\",\n",
    "    ]\n",
    "    agg_exprs = [F.avg(F.col(c)).alias(f\"{c}_avg\") for c in pollutant_cols]\n",
    "    summary = df2.groupBy(\"year\", \"month\").agg(*agg_exprs).orderBy(\"year\", \"month\")\n",
    "    elapsed = time.time() - t0\n",
    "    count = summary.count()\n",
    "    print(f\"[{datetime.now()}] Aggregation done in {elapsed:.2f}s. {count} monthly rows.\")\n",
    "    return summary, elapsed\n",
    "\n",
    "def prepare_for_write(df_monthly):\n",
    "    # Round numeric averages for readability\n",
    "    select_exprs = [F.col(\"year\"), F.col(\"month\")]\n",
    "    for c in df_monthly.columns:\n",
    "        if c.endswith(\"_avg\"):\n",
    "            select_exprs.append(F.round(F.col(c), 4).alias(c))\n",
    "    # Keep any other columns (none expected)\n",
    "    prepared = df_monthly.select(*select_exprs)\n",
    "    return prepared\n",
    "\n",
    "def write_aligned_to_delta(df_aligned, table_name=TARGET_TABLE):\n",
    "    print(f\"[{datetime.now()}] Writing DataFrame to Delta table '{table_name}' (append mode)...\")\n",
    "    t0 = time.time()\n",
    "    try:\n",
    "        if not spark.catalog.tableExists(table_name):\n",
    "            # First-run: create table using overwrite (so we control schema naming)\n",
    "            print(f\"[{datetime.now()}] Table '{table_name}' not found. Creating table.\")\n",
    "            df_aligned.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(table_name)\n",
    "            action = \"created\"\n",
    "        else:\n",
    "            # Table exists: Append. We already aligned schema and column order.\n",
    "            df_aligned.write.format(\"delta\").mode(\"append\").saveAsTable(table_name)\n",
    "            action = \"appended\"\n",
    "        elapsed = time.time() - t0\n",
    "        print(f\"[{datetime.now()}] Write {action} in {elapsed:.2f}s.\")\n",
    "        return elapsed\n",
    "    except Exception as e:\n",
    "        elapsed = time.time() - t0\n",
    "        print(f\"[{datetime.now()}] ERROR during write ({elapsed:.2f}s): {e}\")\n",
    "        raise\n",
    "\n",
    "def show_results(df_summary, table_name=TARGET_TABLE):\n",
    "    print(f\"[{datetime.now()}] Showing aggregated results and stats...\")\n",
    "    try:\n",
    "        df_summary.show(truncate=False)\n",
    "        avg_cols = [c for c in df_summary.columns if c.endswith(\"_avg\") or c.startswith(\"avg_\")]\n",
    "        if avg_cols:\n",
    "            df_summary.select(*avg_cols).describe().show(truncate=False)\n",
    "        if spark.catalog.tableExists(table_name):\n",
    "            print(f\"Latest rows from '{table_name}':\")\n",
    "            spark.table(table_name).orderBy(F.col(\"year\").desc(), F.col(\"month\").desc()).show(10, truncate=False)\n",
    "    except Exception as e:\n",
    "        print(f\"[{datetime.now()}] ERROR showing results: {e}\")\n",
    "\n",
    "def main():\n",
    "    overall_start = time.time()\n",
    "    try:\n",
    "        # 1) Fetch\n",
    "        data, fetch_time = fetch_api(API_URL)\n",
    "\n",
    "        # 2) Transform hourly JSON to Spark DataFrame\n",
    "        df_hourly, transform_time = transform_to_spark(data)\n",
    "\n",
    "        # 3) Monthly aggregation (produces columns like pm10_avg, pm2_5_avg, etc.)\n",
    "        df_monthly, aggregate_time = aggregate_monthly(df_hourly)\n",
    "\n",
    "        # Prepare (rounding)\n",
    "        df_monthly_prepared = prepare_for_write(df_monthly)\n",
    "\n",
    "        # 4) Align to target table schema (rename columns to avg_<pollutant> if needed,\n",
    "        # add ingestion_date if table expects it, and add missing target columns as nulls).\n",
    "        df_aligned = align_df_to_table(df_monthly_prepared, TARGET_TABLE)\n",
    "\n",
    "        # 5) Write to Delta (append if exists)\n",
    "        write_time = write_aligned_to_delta(df_aligned, TARGET_TABLE)\n",
    "\n",
    "        # 6) Show results & stats\n",
    "        show_results(df_aligned, TARGET_TABLE)\n",
    "\n",
    "        total_elapsed = time.time() - overall_start\n",
    "        print(\"\\nExecution times (seconds):\")\n",
    "        print(f\"  Fetch:      {fetch_time:.2f}s\")\n",
    "        print(f\"  Transform:  {transform_time:.2f}s\")\n",
    "        print(f\"  Aggregate:  {aggregate_time:.2f}s\")\n",
    "        print(f\"  Write:      {write_time:.2f}s\")\n",
    "        print(f\"  Total:      {total_elapsed:.2f}s\")\n",
    "        print(f\"[{datetime.now()}] Process completed successfully.\")\n",
    "    except Exception as exc:\n",
    "        print(f\"[{datetime.now()}] Pipeline failed: {exc}\")\n",
    "        raise\n",
    "\n",
    "# Run pipeline\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "61c35ce2-6292-49f7-b280-dff11946d29e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "GPT 5 nano need 1 fix to work"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "GPT 5 nano",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
