{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e8a1c498-6f5a-446d-90be-d65131fe48c5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Task**\n",
    "Create a Databricks Python code that extracts, processes, and loads data from two separate Open-Meteo APIs. The final output must be a clean, merged dataset saved to a Delta table using serverless compute.\n",
    "\n",
    "---\n",
    "\n",
    "**Requirements**\n",
    "\n",
    "### 1. Data Extraction\n",
    "* Fetch hourly air quality data from: `https://air-quality-api.open-meteo.com/v1/air-quality?latitude=40.3548&longitude=18.1724&hourly=pm10,pm2_5,carbon_monoxide,carbon_dioxide,nitrogen_dioxide,sulphur_dioxide,ozone&past_days=31&forecast_days=1`\n",
    "* Fetch hourly weather data from: `https://api.open-meteo.com/v1/forecast?latitude=40.3548&longitude=18.1724&hourly=temperature_2m,relative_humidity_2m,dew_point_2m,apparent_temperature,precipitation_probability,rain,wind_speed_10m&past_days=31&forecast_days=1`\n",
    "* Use the `requests` library to get the JSON responses.\n",
    "\n",
    "### 2. Data Processing and Merging\n",
    "* Parse the hourly JSON data from both APIs into two separate PySpark DataFrames.\n",
    "* The data within the `hourly` object (e.g., `time` and pollutant lists) must be flattened into structured rows.\n",
    "* Add an `ingestion_date` column to each DataFrame.\n",
    "* Merge the two DataFrames based on the `time` column to create a single, unified DataFrame. Use an `inner` join.\n",
    "\n",
    "### 3. Data Quality Checks\n",
    "* Implement checks to ensure data integrity and quality on the merged DataFrame.\n",
    "* **Null Check**: Identify and report the count of missing values in all pollutant and weather columns.\n",
    "* **Duplicate Check**: Identify duplicate entries based on the `time` column. Keep only the first occurrence for each timestamp.\n",
    "* Create a summary report using `print` statements to display the findings of the quality checks (e.g., total nulls, duplicate counts).\n",
    "\n",
    "### 4. Layered Architecture (Bronze & Silver)\n",
    "* **Bronze Layer**: Save the raw, unmerged DataFrames (air quality and weather) to separate Delta tables named `air_quality_bronze` and `weather_bronze`. Partition these tables by `ingestion_date` and use `append` mode.\n",
    "* **Silver Layer**: Save the final, clean, and merged DataFrame (after passing all quality checks) to a Delta table named `air_quality_and_weather_silver`. Use `append` mode for this save as well.\n",
    "\n",
    "### 5. Code Structure\n",
    "* The code should be a single, complete Databricks Python notebook script.\n",
    "* Include all necessary imports (`requests`, `pyspark.sql.functions`, etc.).\n",
    "* Add clear comments to explain each step of the ETL process.\n",
    "* The script should be runnable on Databricks using **serverless compute**.\n",
    "\n",
    "**Final Output**\n",
    "A comprehensive Python script that performs a full ETL pipeline with data merging, quality monitoring, and a Bronze/Silver layer architecture.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4485d8cd-9485-4f49-8740-66f05e7f0d13",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Databricks notebook source\n",
    "# MAGIC %md\n",
    "# MAGIC # Air Quality and Weather Data ETL Pipeline\n",
    "# MAGIC This notebook extracts data from Open-Meteo APIs, processes it, and loads it into Delta tables using a Bronze/Silver architecture\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Import required libraries\n",
    "import requests\n",
    "import json\n",
    "from datetime import datetime\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    col, lit, explode, arrays_zip, to_timestamp, \n",
    "    count, when, isnan, isnull, row_number, current_timestamp\n",
    ")\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## 1. Data Extraction\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Define API endpoints\n",
    "AIR_QUALITY_URL = \"https://air-quality-api.open-meteo.com/v1/air-quality?latitude=40.3548&longitude=18.1724&hourly=pm10,pm2_5,carbon_monoxide,carbon_dioxide,nitrogen_dioxide,sulphur_dioxide,ozone&past_days=31&forecast_days=1\"\n",
    "WEATHER_URL = \"https://api.open-meteo.com/v1/forecast?latitude=40.3548&longitude=18.1724&hourly=temperature_2m,relative_humidity_2m,dew_point_2m,apparent_temperature,precipitation_probability,rain,wind_speed_10m&past_days=31&forecast_days=1\"\n",
    "\n",
    "# Function to fetch data from API\n",
    "def fetch_api_data(url):\n",
    "    \"\"\"\n",
    "    Fetch data from the given API URL\n",
    "    Returns: JSON response\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        return response.json()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching data from {url}: {e}\")\n",
    "        raise\n",
    "\n",
    "# Fetch data from both APIs\n",
    "print(\"Fetching air quality data...\")\n",
    "air_quality_data = fetch_api_data(AIR_QUALITY_URL)\n",
    "print(\"Air quality data fetched successfully\")\n",
    "\n",
    "print(\"Fetching weather data...\")\n",
    "weather_data = fetch_api_data(WEATHER_URL)\n",
    "print(\"Weather data fetched successfully\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## 2. Data Processing and Transformation\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Function to flatten hourly data into structured rows\n",
    "def flatten_hourly_data(data, data_type):\n",
    "    \"\"\"\n",
    "    Flatten the nested hourly data structure into a list of dictionaries\n",
    "    Each dictionary represents one hour of data\n",
    "    \"\"\"\n",
    "    hourly_data = data.get('hourly', {})\n",
    "    time_array = hourly_data.get('time', [])\n",
    "    \n",
    "    # Get all metric keys except 'time'\n",
    "    metric_keys = [key for key in hourly_data.keys() if key != 'time']\n",
    "    \n",
    "    # Create list of dictionaries, one for each time point\n",
    "    flattened_data = []\n",
    "    for i, time_value in enumerate(time_array):\n",
    "        row = {'time': time_value}\n",
    "        \n",
    "        # Add each metric value for this time point\n",
    "        for metric in metric_keys:\n",
    "            metric_values = hourly_data.get(metric, [])\n",
    "            if i < len(metric_values):\n",
    "                row[metric] = metric_values[i]\n",
    "            else:\n",
    "                row[metric] = None\n",
    "                \n",
    "        flattened_data.append(row)\n",
    "    \n",
    "    return flattened_data\n",
    "\n",
    "# Flatten air quality data\n",
    "air_quality_flattened = flatten_hourly_data(air_quality_data, 'air_quality')\n",
    "print(f\"Flattened {len(air_quality_flattened)} air quality records\")\n",
    "\n",
    "# Flatten weather data\n",
    "weather_flattened = flatten_hourly_data(weather_data, 'weather')\n",
    "print(f\"Flattened {len(weather_flattened)} weather records\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Create PySpark DataFrames from flattened data\n",
    "# Define schema for air quality data\n",
    "air_quality_schema = StructType([\n",
    "    StructField(\"time\", StringType(), True),\n",
    "    StructField(\"pm10\", DoubleType(), True),\n",
    "    StructField(\"pm2_5\", DoubleType(), True),\n",
    "    StructField(\"carbon_monoxide\", DoubleType(), True),\n",
    "    StructField(\"carbon_dioxide\", DoubleType(), True),\n",
    "    StructField(\"nitrogen_dioxide\", DoubleType(), True),\n",
    "    StructField(\"sulphur_dioxide\", DoubleType(), True),\n",
    "    StructField(\"ozone\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "# Define schema for weather data\n",
    "weather_schema = StructType([\n",
    "    StructField(\"time\", StringType(), True),\n",
    "    StructField(\"temperature_2m\", DoubleType(), True),\n",
    "    StructField(\"relative_humidity_2m\", DoubleType(), True),\n",
    "    StructField(\"dew_point_2m\", DoubleType(), True),\n",
    "    StructField(\"apparent_temperature\", DoubleType(), True),\n",
    "    StructField(\"precipitation_probability\", DoubleType(), True),\n",
    "    StructField(\"rain\", DoubleType(), True),\n",
    "    StructField(\"wind_speed_10m\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "# Create DataFrames\n",
    "air_quality_df = spark.createDataFrame(air_quality_flattened, schema=air_quality_schema)\n",
    "weather_df = spark.createDataFrame(weather_flattened, schema=weather_schema)\n",
    "\n",
    "# Convert time string to timestamp\n",
    "air_quality_df = air_quality_df.withColumn(\"time\", to_timestamp(col(\"time\"), \"yyyy-MM-dd'T'HH:mm\"))\n",
    "weather_df = weather_df.withColumn(\"time\", to_timestamp(col(\"time\"), \"yyyy-MM-dd'T'HH:mm\"))\n",
    "\n",
    "# Add ingestion_date column\n",
    "current_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "air_quality_df = air_quality_df.withColumn(\"ingestion_date\", lit(current_date))\n",
    "weather_df = weather_df.withColumn(\"ingestion_date\", lit(current_date))\n",
    "\n",
    "# Add ingestion_timestamp for tracking\n",
    "air_quality_df = air_quality_df.withColumn(\"ingestion_timestamp\", current_timestamp())\n",
    "weather_df = weather_df.withColumn(\"ingestion_timestamp\", current_timestamp())\n",
    "\n",
    "print(\"DataFrames created successfully\")\n",
    "print(f\"Air quality DataFrame: {air_quality_df.count()} rows\")\n",
    "print(f\"Weather DataFrame: {weather_df.count()} rows\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## 3. Data Merging\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Merge the two DataFrames on the 'time' column using inner join\n",
    "merged_df = air_quality_df.join(\n",
    "    weather_df,\n",
    "    on=\"time\",\n",
    "    how=\"inner\"\n",
    ").select(\n",
    "    col(\"time\"),\n",
    "    # Air quality columns\n",
    "    col(\"pm10\"),\n",
    "    col(\"pm2_5\"),\n",
    "    col(\"carbon_monoxide\"),\n",
    "    col(\"carbon_dioxide\"),\n",
    "    col(\"nitrogen_dioxide\"),\n",
    "    col(\"sulphur_dioxide\"),\n",
    "    col(\"ozone\"),\n",
    "    # Weather columns\n",
    "    col(\"temperature_2m\"),\n",
    "    col(\"relative_humidity_2m\"),\n",
    "    col(\"dew_point_2m\"),\n",
    "    col(\"apparent_temperature\"),\n",
    "    col(\"precipitation_probability\"),\n",
    "    col(\"rain\"),\n",
    "    col(\"wind_speed_10m\"),\n",
    "    # Use the ingestion_date from air_quality_df (they should be the same)\n",
    "    air_quality_df[\"ingestion_date\"].alias(\"ingestion_date\"),\n",
    "    air_quality_df[\"ingestion_timestamp\"].alias(\"ingestion_timestamp\")\n",
    ")\n",
    "\n",
    "print(f\"Merged DataFrame created with {merged_df.count()} rows\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## 4. Data Quality Checks\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Define columns to check for nulls\n",
    "pollutant_columns = [\"pm10\", \"pm2_5\", \"carbon_monoxide\", \"carbon_dioxide\", \n",
    "                    \"nitrogen_dioxide\", \"sulphur_dioxide\", \"ozone\"]\n",
    "weather_columns = [\"temperature_2m\", \"relative_humidity_2m\", \"dew_point_2m\", \n",
    "                  \"apparent_temperature\", \"precipitation_probability\", \"rain\", \"wind_speed_10m\"]\n",
    "all_metric_columns = pollutant_columns + weather_columns\n",
    "\n",
    "# Null Check - Count missing values for each column\n",
    "print(\"=\" * 80)\n",
    "print(\"DATA QUALITY CHECK REPORT\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\n1. NULL VALUE CHECK:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "null_counts = {}\n",
    "for column in all_metric_columns:\n",
    "    null_count = merged_df.filter(col(column).isNull() | isnan(col(column))).count()\n",
    "    null_counts[column] = null_count\n",
    "    if null_count > 0:\n",
    "        print(f\"   {column}: {null_count} null values\")\n",
    "\n",
    "total_nulls = sum(null_counts.values())\n",
    "print(f\"\\n   Total null values across all columns: {total_nulls}\")\n",
    "\n",
    "# Duplicate Check - Check for duplicate timestamps\n",
    "print(\"\\n2. DUPLICATE CHECK:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Count total rows before deduplication\n",
    "total_rows_before = merged_df.count()\n",
    "\n",
    "# Count duplicates based on time column\n",
    "duplicate_count = merged_df.groupBy(\"time\").count().filter(col(\"count\") > 1).count()\n",
    "print(f\"   Number of duplicate timestamps found: {duplicate_count}\")\n",
    "\n",
    "# If duplicates exist, show how many rows are affected\n",
    "if duplicate_count > 0:\n",
    "    duplicate_rows = merged_df.groupBy(\"time\").count().filter(col(\"count\") > 1).select(\"count\").agg({\"count\": \"sum\"}).collect()[0][0]\n",
    "    print(f\"   Total rows affected by duplicates: {duplicate_rows}\")\n",
    "\n",
    "# Remove duplicates - keep only the first occurrence for each timestamp\n",
    "window_spec = Window.partitionBy(\"time\").orderBy(\"ingestion_timestamp\")\n",
    "deduplicated_df = merged_df.withColumn(\"row_num\", row_number().over(window_spec)) \\\n",
    "                          .filter(col(\"row_num\") == 1) \\\n",
    "                          .drop(\"row_num\")\n",
    "\n",
    "total_rows_after = deduplicated_df.count()\n",
    "rows_removed = total_rows_before - total_rows_after\n",
    "\n",
    "print(f\"\\n   Rows before deduplication: {total_rows_before}\")\n",
    "print(f\"   Rows after deduplication: {total_rows_after}\")\n",
    "print(f\"   Duplicate rows removed: {rows_removed}\")\n",
    "\n",
    "# Data completeness check\n",
    "print(\"\\n3. DATA COMPLETENESS:\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"   Total records in merged dataset: {total_rows_after}\")\n",
    "print(f\"   Date range: {deduplicated_df.agg({'time': 'min'}).collect()[0][0]} to {deduplicated_df.agg({'time': 'max'}).collect()[0][0]}\")\n",
    "\n",
    "# Final clean DataFrame\n",
    "clean_merged_df = deduplicated_df\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Data quality checks completed successfully!\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## 5. Bronze Layer - Save Raw Data\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Define Bronze layer table names\n",
    "AIR_QUALITY_BRONZE_TABLE = \"air_quality_bronze\"\n",
    "WEATHER_BRONZE_TABLE = \"weather_bronze\"\n",
    "\n",
    "# Save air quality data to Bronze layer\n",
    "print(\"Saving air quality data to Bronze layer...\")\n",
    "air_quality_df.write \\\n",
    "    .mode(\"append\") \\\n",
    "    .partitionBy(\"ingestion_date\") \\\n",
    "    .format(\"delta\") \\\n",
    "    .saveAsTable(AIR_QUALITY_BRONZE_TABLE)\n",
    "print(f\"Air quality data saved to table: {AIR_QUALITY_BRONZE_TABLE}\")\n",
    "\n",
    "# Save weather data to Bronze layer\n",
    "print(\"Saving weather data to Bronze layer...\")\n",
    "weather_df.write \\\n",
    "    .mode(\"append\") \\\n",
    "    .partitionBy(\"ingestion_date\") \\\n",
    "    .format(\"delta\") \\\n",
    "    .saveAsTable(WEATHER_BRONZE_TABLE)\n",
    "print(f\"Weather data saved to table: {WEATHER_BRONZE_TABLE}\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## 6. Silver Layer - Save Clean Merged Data\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Define Silver layer table name\n",
    "SILVER_TABLE = \"air_quality_and_weather_silver\"\n",
    "\n",
    "# Save clean merged data to Silver layer\n",
    "print(\"Saving clean merged data to Silver layer...\")\n",
    "clean_merged_df.write \\\n",
    "    .mode(\"append\") \\\n",
    "    .format(\"delta\") \\\n",
    "    .saveAsTable(SILVER_TABLE)\n",
    "print(f\"Clean merged data saved to table: {SILVER_TABLE}\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## 7. Verification and Summary\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Verify the data was saved correctly\n",
    "print(\"VERIFICATION SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Check Bronze tables\n",
    "air_quality_bronze_count = spark.table(AIR_QUALITY_BRONZE_TABLE).count()\n",
    "weather_bronze_count = spark.table(WEATHER_BRONZE_TABLE).count()\n",
    "silver_count = spark.table(SILVER_TABLE).count()\n",
    "\n",
    "print(f\"\\nBronze Layer:\")\n",
    "print(f\"  - {AIR_QUALITY_BRONZE_TABLE}: {air_quality_bronze_count} records\")\n",
    "print(f\"  - {WEATHER_BRONZE_TABLE}: {weather_bronze_count} records\")\n",
    "\n",
    "print(f\"\\nSilver Layer:\")\n",
    "print(f\"  - {SILVER_TABLE}: {silver_count} records\")\n",
    "\n",
    "# Show sample of Silver table\n",
    "print(\"\\nSample data from Silver table:\")\n",
    "spark.table(SILVER_TABLE).select(\n",
    "    \"time\", \"pm10\", \"pm2_5\", \"temperature_2m\", \"relative_humidity_2m\", \"wind_speed_10m\"\n",
    ").orderBy(\"time\", ascending=False).show(5, truncate=False)\n",
    "\n",
    "print(\"\\nETL Pipeline completed successfully!\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Pipeline Summary\n",
    "# MAGIC \n",
    "# MAGIC This ETL pipeline successfully:\n",
    "# MAGIC 1. **Extracted** data from two Open-Meteo APIs (air quality and weather)\n",
    "# MAGIC 2. **Transformed** the nested JSON structure into flattened PySpark DataFrames\n",
    "# MAGIC 3. **Merged** the datasets on the time column using an inner join\n",
    "# MAGIC 4. **Performed quality checks**:\n",
    "# MAGIC    - Identified and reported null values\n",
    "# MAGIC    - Detected and removed duplicate timestamps\n",
    "# MAGIC 5. **Implemented Bronze/Silver architecture**:\n",
    "# MAGIC    - Bronze: Raw data stored in separate tables, partitioned by ingestion_date\n",
    "# MAGIC    - Silver: Clean, merged data stored in a single table\n",
    "# MAGIC \n",
    "# MAGIC The pipeline is designed to run on Databricks serverless compute and can be scheduled for regular updates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7fd6ee4a-4d3e-4cce-b28b-0ca2053b77c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Claude Opus 4 work at the first attempt"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Claude Opus 4",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
