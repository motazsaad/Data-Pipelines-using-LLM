{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "71fba0a5-7483-40ca-a28a-469678cd254f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Task**\n",
    "Create a Databricks Python code that extracts, processes, and loads data from two separate Open-Meteo APIs. The final output must be a clean, merged dataset saved to a Delta table using serverless compute.\n",
    "\n",
    "---\n",
    "\n",
    "**Requirements**\n",
    "\n",
    "### 1. Data Extraction\n",
    "* Fetch hourly air quality data from: `https://air-quality-api.open-meteo.com/v1/air-quality?latitude=40.3548&longitude=18.1724&hourly=pm10,pm2_5,carbon_monoxide,carbon_dioxide,nitrogen_dioxide,sulphur_dioxide,ozone&past_days=31&forecast_days=1`\n",
    "* Fetch hourly weather data from: `https://api.open-meteo.com/v1/forecast?latitude=40.3548&longitude=18.1724&hourly=temperature_2m,relative_humidity_2m,dew_point_2m,apparent_temperature,precipitation_probability,rain,wind_speed_10m&past_days=31&forecast_days=1`\n",
    "* Use the `requests` library to get the JSON responses.\n",
    "\n",
    "### 2. Data Processing and Merging\n",
    "* Parse the hourly JSON data from both APIs into two separate PySpark DataFrames.\n",
    "* The data within the `hourly` object (e.g., `time` and pollutant lists) must be flattened into structured rows.\n",
    "* Add an `ingestion_date` column to each DataFrame.\n",
    "* Merge the two DataFrames based on the `time` column to create a single, unified DataFrame. Use an `inner` join.\n",
    "\n",
    "### 3. Data Quality Checks\n",
    "* Implement checks to ensure data integrity and quality on the merged DataFrame.\n",
    "* **Null Check**: Identify and report the count of missing values in all pollutant and weather columns.\n",
    "* **Duplicate Check**: Identify duplicate entries based on the `time` column. Keep only the first occurrence for each timestamp.\n",
    "* Create a summary report using `print` statements to display the findings of the quality checks (e.g., total nulls, duplicate counts).\n",
    "\n",
    "### 4. Layered Architecture (Bronze & Silver)\n",
    "* **Bronze Layer**: Save the raw, unmerged DataFrames (air quality and weather) to separate Delta tables named `air_quality_bronze` and `weather_bronze`. Partition these tables by `ingestion_date` and use `append` mode.\n",
    "* **Silver Layer**: Save the final, clean, and merged DataFrame (after passing all quality checks) to a Delta table named `air_quality_and_weather_silver`. Use `append` mode for this save as well.\n",
    "\n",
    "### 5. Code Structure\n",
    "* The code should be a single, complete Databricks Python notebook script.\n",
    "* Include all necessary imports (`requests`, `pyspark.sql.functions`, etc.).\n",
    "* Add clear comments to explain each step of the ETL process.\n",
    "* The script should be runnable on Databricks using **serverless compute**.\n",
    "\n",
    "**Final Output**\n",
    "A comprehensive Python script that performs a full ETL pipeline with data merging, quality monitoring, and a Bronze/Silver layer architecture.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "992fce4d-2afe-4eed-b7b2-d83de746af85",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Databricks Python Notebook\n",
    "# End-to-end ETL for Open-Meteo Air Quality and Weather data with robust Delta schema handling.\n",
    "\n",
    "import requests\n",
    "from datetime import datetime\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as T\n",
    "\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"64\")\n",
    "\n",
    "AIR_QUALITY_URL = \"https://air-quality-api.open-meteo.com/v1/air-quality?latitude=40.3548&longitude=18.1724&hourly=pm10,pm2_5,carbon_monoxide,carbon_dioxide,nitrogen_dioxide,sulphur_dioxide,ozone&past_days=31&forecast_days=1\"\n",
    "WEATHER_URL = \"https://api.open-meteo.com/v1/forecast?latitude=40.3548&longitude=18.1724&hourly=temperature_2m,relative_humidity_2m,dew_point_2m,apparent_temperature,precipitation_probability,rain,wind_speed_10m&past_days=31&forecast_days=1\"\n",
    "\n",
    "air_cols = [\n",
    "    \"pm10\",\n",
    "    \"pm2_5\",\n",
    "    \"carbon_monoxide\",\n",
    "    \"carbon_dioxide\",\n",
    "    \"nitrogen_dioxide\",\n",
    "    \"sulphur_dioxide\",\n",
    "    \"ozone\",\n",
    "]\n",
    "wx_cols = [\n",
    "    \"temperature_2m\",\n",
    "    \"relative_humidity_2m\",\n",
    "    \"dew_point_2m\",\n",
    "    \"apparent_temperature\",\n",
    "    \"precipitation_probability\",\n",
    "    \"rain\",\n",
    "    \"wind_speed_10m\",\n",
    "]\n",
    "quality_check_cols = air_cols + wx_cols\n",
    "\n",
    "# Set these True ONCE to auto-repair schema by dropping/recreating tables if mismatches are found\n",
    "REPAIR_BRONZE_SCHEMA = False\n",
    "REPAIR_SILVER_SCHEMA = False\n",
    "\n",
    "def fetch_json(url: str) -> dict:\n",
    "    headers = {\"User-Agent\": \"databricks-etl/1.0 (+https://databricks.com/)\"}\n",
    "    resp = requests.get(url, headers=headers, timeout=60)\n",
    "    resp.raise_for_status()\n",
    "    return resp.json()\n",
    "\n",
    "def hourly_to_rows(hourly_obj: dict, keys: list[str]) -> list[dict]:\n",
    "    times = hourly_obj.get(\"time\", [])\n",
    "    n = len(times)\n",
    "    rows = []\n",
    "    for i in range(n):\n",
    "        row = {\"time\": times[i]}\n",
    "        for k in keys:\n",
    "            series = hourly_obj.get(k)\n",
    "            row[k] = series[i] if series is not None and i < len(series) else None\n",
    "        rows.append(row)\n",
    "    return rows\n",
    "\n",
    "# Explicit schemas for Bronze and Silver tables\n",
    "aq_bronze_schema = T.StructType([\n",
    "    T.StructField(\"time\", T.TimestampType(), True),\n",
    "    T.StructField(\"pm10\", T.DoubleType(), True),\n",
    "    T.StructField(\"pm2_5\", T.DoubleType(), True),\n",
    "    T.StructField(\"carbon_monoxide\", T.DoubleType(), True),\n",
    "    T.StructField(\"carbon_dioxide\", T.DoubleType(), True),\n",
    "    T.StructField(\"nitrogen_dioxide\", T.DoubleType(), True),\n",
    "    T.StructField(\"sulphur_dioxide\", T.DoubleType(), True),\n",
    "    T.StructField(\"ozone\", T.DoubleType(), True),\n",
    "    T.StructField(\"ingestion_date\", T.DateType(), True),\n",
    "])\n",
    "\n",
    "wx_bronze_schema = T.StructType([\n",
    "    T.StructField(\"time\", T.TimestampType(), True),\n",
    "    T.StructField(\"temperature_2m\", T.DoubleType(), True),\n",
    "    T.StructField(\"relative_humidity_2m\", T.DoubleType(), True),\n",
    "    T.StructField(\"dew_point_2m\", T.DoubleType(), True),\n",
    "    T.StructField(\"apparent_temperature\", T.DoubleType(), True),\n",
    "    T.StructField(\"precipitation_probability\", T.DoubleType(), True),\n",
    "    T.StructField(\"rain\", T.DoubleType(), True),\n",
    "    T.StructField(\"wind_speed_10m\", T.DoubleType(), True),\n",
    "    T.StructField(\"ingestion_date\", T.DateType(), True),\n",
    "])\n",
    "\n",
    "silver_schema = T.StructType([\n",
    "    T.StructField(\"time\", T.TimestampType(), True),\n",
    "    T.StructField(\"pm10\", T.DoubleType(), True),\n",
    "    T.StructField(\"pm2_5\", T.DoubleType(), True),\n",
    "    T.StructField(\"carbon_monoxide\", T.DoubleType(), True),\n",
    "    T.StructField(\"carbon_dioxide\", T.DoubleType(), True),\n",
    "    T.StructField(\"nitrogen_dioxide\", T.DoubleType(), True),\n",
    "    T.StructField(\"sulphur_dioxide\", T.DoubleType(), True),\n",
    "    T.StructField(\"ozone\", T.DoubleType(), True),\n",
    "    T.StructField(\"temperature_2m\", T.DoubleType(), True),\n",
    "    T.StructField(\"relative_humidity_2m\", T.DoubleType(), True),\n",
    "    T.StructField(\"dew_point_2m\", T.DoubleType(), True),\n",
    "    T.StructField(\"apparent_temperature\", T.DoubleType(), True),\n",
    "    T.StructField(\"precipitation_probability\", T.DoubleType(), True),\n",
    "    T.StructField(\"rain\", T.DoubleType(), True),\n",
    "    T.StructField(\"wind_speed_10m\", T.DoubleType(), True),\n",
    "    T.StructField(\"ingestion_date\", T.DateType(), True),\n",
    "])\n",
    "\n",
    "def table_exists(table_name: str) -> bool:\n",
    "    return spark.catalog.tableExists(table_name)\n",
    "\n",
    "def schemas_compatible(actual: T.StructType, expected: T.StructType) -> bool:\n",
    "    # Compare by column name and dataType; ignore nullability\n",
    "    actual_map = {f.name: f.dataType.simpleString() for f in actual}\n",
    "    expected_map = {f.name: f.dataType.simpleString() for f in expected}\n",
    "    # All expected columns must exist and have identical types\n",
    "    for k, v in expected_map.items():\n",
    "        if k not in actual_map or actual_map[k] != v:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def ensure_table_with_schema(table_name: str, expected_schema: T.StructType, partition_cols: list[str], repair: bool):\n",
    "    if table_exists(table_name):\n",
    "        actual_schema = spark.table(table_name).schema\n",
    "        if not schemas_compatible(actual_schema, expected_schema):\n",
    "            msg = f\"Schema mismatch detected for {table_name}. Actual: {actual_schema.json()}, Expected: {expected_schema.json()}\"\n",
    "            if repair:\n",
    "                print(msg)\n",
    "                print(f\"Dropping and recreating {table_name} due to schema mismatch (repair=True).\")\n",
    "                spark.sql(f\"DROP TABLE {table_name}\")\n",
    "            else:\n",
    "                raise ValueError(msg + \" Set REPAIR_*_SCHEMA=True to auto-recreate once.\")\n",
    "    # Create if not exists with explicit schema\n",
    "    if not table_exists(table_name):\n",
    "        # Build CREATE TABLE SQL from expected schema\n",
    "        cols_sql = \",\\n  \".join([f\"`{f.name}` {f.dataType.simpleString().upper()}\" for f in expected_schema])\n",
    "        part_sql = f\"PARTITIONED BY ({', '.join(partition_cols)})\" if partition_cols else \"\"\n",
    "        create_sql = f\"\"\"\n",
    "        CREATE TABLE {table_name} (\n",
    "          {cols_sql}\n",
    "        )\n",
    "        USING DELTA\n",
    "        {part_sql}\n",
    "        \"\"\"\n",
    "        spark.sql(create_sql)\n",
    "        print(f\"Created table {table_name} with expected schema.\")\n",
    "\n",
    "# 1) Extract\n",
    "air_payload = fetch_json(AIR_QUALITY_URL)\n",
    "weather_payload = fetch_json(WEATHER_URL)\n",
    "\n",
    "# 2) Transform to flattened rows\n",
    "aq_rows = hourly_to_rows(air_payload.get(\"hourly\", {}), air_cols)\n",
    "wx_rows = hourly_to_rows(weather_payload.get(\"hourly\", {}), wx_cols)\n",
    "\n",
    "aq_df = spark.createDataFrame(aq_rows)\n",
    "wx_df = spark.createDataFrame(wx_rows)\n",
    "\n",
    "# Cast to numeric and normalize time; add ingestion_date\n",
    "def cast_to_expected_aq(df):\n",
    "    return (\n",
    "        df\n",
    "        .withColumn(\"time\", F.to_timestamp(\"time\"))\n",
    "        .select(\n",
    "            F.col(\"time\"),\n",
    "            *[F.col(c).cast(\"double\").alias(c) for c in air_cols],\n",
    "        )\n",
    "        .withColumn(\"ingestion_date\", F.current_date().cast(\"date\"))\n",
    "    )\n",
    "\n",
    "def cast_to_expected_wx(df):\n",
    "    return (\n",
    "        df\n",
    "        .withColumn(\"time\", F.to_timestamp(\"time\"))\n",
    "        .select(\n",
    "            F.col(\"time\"),\n",
    "            *[F.col(c).cast(\"double\").alias(c) for c in wx_cols],\n",
    "        )\n",
    "        .withColumn(\"ingestion_date\", F.current_date().cast(\"date\"))\n",
    "    )\n",
    "\n",
    "aq_df = cast_to_expected_aq(aq_df)\n",
    "wx_df = cast_to_expected_wx(wx_df)\n",
    "\n",
    "# 3) Ensure Bronze tables exist with correct schema (and repair if needed)\n",
    "ensure_table_with_schema(\"air_quality_bronze\", aq_bronze_schema, [\"ingestion_date\"], REPAIR_BRONZE_SCHEMA)\n",
    "ensure_table_with_schema(\"weather_bronze\", wx_bronze_schema, [\"ingestion_date\"], REPAIR_BRONZE_SCHEMA)\n",
    "\n",
    "# 4) Bronze write (append)\n",
    "(\n",
    "    aq_df.select([f.name for f in aq_bronze_schema])  # enforce column order/schema\n",
    "    .write.format(\"delta\")\n",
    "    .mode(\"append\")\n",
    "    .partitionBy(\"ingestion_date\")\n",
    "    .saveAsTable(\"air_quality_bronze\")\n",
    ")\n",
    "\n",
    "(\n",
    "    wx_df.select([f.name for f in wx_bronze_schema])\n",
    "    .write.format(\"delta\")\n",
    "    .mode(\"append\")\n",
    "    .partitionBy(\"ingestion_date\")\n",
    "    .saveAsTable(\"weather_bronze\")\n",
    ")\n",
    "\n",
    "print(\"Bronze write completed.\")\n",
    "print(f\"air_quality_bronze rows this run: {aq_df.count()}\")\n",
    "print(f\"weather_bronze rows this run: {wx_df.count()}\")\n",
    "\n",
    "# 5) Merge (Inner Join on time)\n",
    "aq_df_renamed = aq_df.withColumnRenamed(\"ingestion_date\", \"ingestion_date_aq\")\n",
    "wx_df_renamed = wx_df.withColumnRenamed(\"ingestion_date\", \"ingestion_date_wx\")\n",
    "\n",
    "merged_df = aq_df_renamed.alias(\"aq\").join(wx_df_renamed.alias(\"wx\"), on=\"time\", how=\"inner\")\n",
    "merged_count = merged_df.count()\n",
    "print(f\"Merged rows (inner join on time): {merged_count}\")\n",
    "\n",
    "# 6) Data Quality Checks\n",
    "null_exprs = [F.sum(F.when(F.col(c).isNull(), 1).otherwise(0)).alias(c) for c in quality_check_cols]\n",
    "null_counts_row = merged_df.select(*null_exprs).collect()[0].asDict()\n",
    "total_nulls = int(sum(v for v in null_counts_row.values() if v is not None))\n",
    "\n",
    "print(\"Null Check - per column:\")\n",
    "for c in quality_check_cols:\n",
    "    print(f\"  {c}: {int(null_counts_row.get(c, 0) or 0)}\")\n",
    "print(f\"Total nulls across pollutant and weather columns: {total_nulls}\")\n",
    "\n",
    "duplicate_rows = merged_count - merged_df.dropDuplicates([\"time\"]).count()\n",
    "print(f\"Duplicate rows based on time: {duplicate_rows}\")\n",
    "\n",
    "merged_dedup = merged_df.dropDuplicates([\"time\"])\n",
    "\n",
    "# 7) Prepare Silver DataFrame\n",
    "silver_df = (\n",
    "    merged_dedup\n",
    "    .select(\n",
    "        \"time\",\n",
    "        *[F.col(c) for c in air_cols],\n",
    "        *[F.col(c) for c in wx_cols],\n",
    "    )\n",
    "    .withColumn(\"ingestion_date\", F.current_date().cast(\"date\"))\n",
    ")\n",
    "\n",
    "print(f\"Silver candidate rows (after dedup): {silver_df.count()}\")\n",
    "print(f\"Silver schema: {silver_df.dtypes}\")\n",
    "\n",
    "# 8) Ensure Silver table exists with correct schema (and repair if needed)\n",
    "ensure_table_with_schema(\"air_quality_and_weather_silver\", silver_schema, [], REPAIR_SILVER_SCHEMA)\n",
    "\n",
    "# 9) Silver write (append)\n",
    "(\n",
    "    silver_df.select([f.name for f in silver_schema])\n",
    "    .write.format(\"delta\")\n",
    "    .mode(\"append\")\n",
    "    .saveAsTable(\"air_quality_and_weather_silver\")\n",
    ")\n",
    "\n",
    "print(\"Silver write completed to table: air_quality_and_weather_silver\")\n",
    "\n",
    "# 10) Final Summary\n",
    "print(\"=== ETL Summary Report ===\")\n",
    "print(f\"Air Quality (bronze) rows this batch: {aq_df.count()}\")\n",
    "print(f\"Weather (bronze) rows this batch: {wx_df.count()}\")\n",
    "print(f\"Merged rows pre-dedup: {merged_count}\")\n",
    "print(f\"Duplicate rows dropped: {duplicate_rows}\")\n",
    "print(f\"Silver rows written this batch: {silver_df.count()}\")\n",
    "print(\"Null counts per column in merged (pre-dedup):\")\n",
    "for c in quality_check_cols:\n",
    "    print(f\"  {c}: {int(null_counts_row.get(c, 0) or 0)}\")\n",
    "print(f\"Total nulls across columns (pre-dedup): {total_nulls}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "172b5214-2a99-47b5-9c2e-5abfac2273ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "GPT 5 need 1 fix to work"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "GPT 5",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
