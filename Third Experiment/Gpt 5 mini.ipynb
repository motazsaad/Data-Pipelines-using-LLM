{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6cc8be2b-f73f-4b06-ba4b-966af4b17995",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Task**\n",
    "Create a Databricks Python code that extracts, processes, and loads data from two separate Open-Meteo APIs. The final output must be a clean, merged dataset saved to a Delta table using serverless compute.\n",
    "\n",
    "---\n",
    "\n",
    "**Requirements**\n",
    "\n",
    "### 1. Data Extraction\n",
    "* Fetch hourly air quality data from: `https://air-quality-api.open-meteo.com/v1/air-quality?latitude=40.3548&longitude=18.1724&hourly=pm10,pm2_5,carbon_monoxide,carbon_dioxide,nitrogen_dioxide,sulphur_dioxide,ozone&past_days=31&forecast_days=1`\n",
    "* Fetch hourly weather data from: `https://api.open-meteo.com/v1/forecast?latitude=40.3548&longitude=18.1724&hourly=temperature_2m,relative_humidity_2m,dew_point_2m,apparent_temperature,precipitation_probability,rain,wind_speed_10m&past_days=31&forecast_days=1`\n",
    "* Use the `requests` library to get the JSON responses.\n",
    "\n",
    "### 2. Data Processing and Merging\n",
    "* Parse the hourly JSON data from both APIs into two separate PySpark DataFrames.\n",
    "* The data within the `hourly` object (e.g., `time` and pollutant lists) must be flattened into structured rows.\n",
    "* Add an `ingestion_date` column to each DataFrame.\n",
    "* Merge the two DataFrames based on the `time` column to create a single, unified DataFrame. Use an `inner` join.\n",
    "\n",
    "### 3. Data Quality Checks\n",
    "* Implement checks to ensure data integrity and quality on the merged DataFrame.\n",
    "* **Null Check**: Identify and report the count of missing values in all pollutant and weather columns.\n",
    "* **Duplicate Check**: Identify duplicate entries based on the `time` column. Keep only the first occurrence for each timestamp.\n",
    "* Create a summary report using `print` statements to display the findings of the quality checks (e.g., total nulls, duplicate counts).\n",
    "\n",
    "### 4. Layered Architecture (Bronze & Silver)\n",
    "* **Bronze Layer**: Save the raw, unmerged DataFrames (air quality and weather) to separate Delta tables named `air_quality_bronze` and `weather_bronze`. Partition these tables by `ingestion_date` and use `append` mode.\n",
    "* **Silver Layer**: Save the final, clean, and merged DataFrame (after passing all quality checks) to a Delta table named `air_quality_and_weather_silver`. Use `append` mode for this save as well.\n",
    "\n",
    "### 5. Code Structure\n",
    "* The code should be a single, complete Databricks Python notebook script.\n",
    "* Include all necessary imports (`requests`, `pyspark.sql.functions`, etc.).\n",
    "* Add clear comments to explain each step of the ETL process.\n",
    "* The script should be runnable on Databricks using **serverless compute**.\n",
    "\n",
    "**Final Output**\n",
    "A comprehensive Python script that performs a full ETL pipeline with data merging, quality monitoring, and a Bronze/Silver layer architecture.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e77b709f-f90f-4754-a978-b6fc8779f59f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Databricks Python notebook script completo\n",
    "# ETL: Estrazione da Open-Meteo (air-quality & weather), trasformazione, controlli qualità,\n",
    "#       salvataggio Bronze (raw) e Silver (pulito) come tabelle Delta.\n",
    "# Progettato per essere eseguito su Databricks (serverless compute).\n",
    "\n",
    "# ---------------------------\n",
    "# Import necessari\n",
    "# ---------------------------\n",
    "import requests\n",
    "import datetime\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as T\n",
    "from pyspark.sql import Window\n",
    "\n",
    "# ---------------------------\n",
    "# 1) Configurazione e helper\n",
    "# ---------------------------\n",
    "\n",
    "# Endpoint API richiesti\n",
    "AIR_QUALITY_URL = (\n",
    "    \"https://air-quality-api.open-meteo.com/v1/air-quality?\"\n",
    "    \"latitude=40.3548&longitude=18.1724&\"\n",
    "    \"hourly=pm10,pm2_5,carbon_monoxide,carbon_dioxide,nitrogen_dioxide,sulphur_dioxide,ozone&\"\n",
    "    \"past_days=31&forecast_days=1\"\n",
    ")\n",
    "\n",
    "WEATHER_URL = (\n",
    "    \"https://api.open-meteo.com/v1/forecast?\"\n",
    "    \"latitude=40.3548&longitude=18.1724&\"\n",
    "    \"hourly=temperature_2m,relative_humidity_2m,dew_point_2m,apparent_temperature,precipitation_probability,rain,wind_speed_10m&\"\n",
    "    \"past_days=31&forecast_days=1\"\n",
    ")\n",
    "\n",
    "# Data di ingestione usata per il partitioning (YYYY-MM-DD)\n",
    "ingestion_date = datetime.datetime.utcnow().date().isoformat()\n",
    "\n",
    "def fetch_json(url):\n",
    "    \"\"\"Recupera JSON dall'URL usando requests con gestione errori semplice.\"\"\"\n",
    "    try:\n",
    "        resp = requests.get(url, timeout=30)\n",
    "        resp.raise_for_status()\n",
    "        return resp.json()\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Errore fetch da {url}: {e}\")\n",
    "\n",
    "# ---------------------------\n",
    "# 2) Estrazione dati\n",
    "# ---------------------------\n",
    "\n",
    "print(\"Fetching air quality JSON...\")\n",
    "aq_json = fetch_json(AIR_QUALITY_URL)\n",
    "print(\"Fetching weather JSON...\")\n",
    "weather_json = fetch_json(WEATHER_URL)\n",
    "\n",
    "# ---------------------------\n",
    "# 3) Processing: flatten hourly JSON -> PySpark DataFrames\n",
    "# ---------------------------\n",
    "\n",
    "def hourly_dict_to_rows(hourly_dict):\n",
    "    \"\"\"\n",
    "    Converte un oggetto 'hourly' (chiavi -> liste) in una lista di dict (righe),\n",
    "    usando 'time' come indice. Se una serie è più corta, inserisce None.\n",
    "    \"\"\"\n",
    "    if 'time' not in hourly_dict:\n",
    "        raise ValueError(\"L'oggetto 'hourly' non contiene la chiave 'time'\")\n",
    "    times = hourly_dict['time']\n",
    "    n = len(times)\n",
    "    rows = []\n",
    "    keys = [k for k in hourly_dict.keys() if k != 'time']\n",
    "    for i, t in enumerate(times):\n",
    "        row = {'time': t}\n",
    "        for k in keys:\n",
    "            arr = hourly_dict.get(k)\n",
    "            val = arr[i] if (arr is not None and i < len(arr)) else None\n",
    "            row[k] = val\n",
    "        rows.append(row)\n",
    "    return rows\n",
    "\n",
    "# Estrae l'oggetto 'hourly' dai JSON\n",
    "aq_hourly = aq_json.get('hourly', {})\n",
    "weather_hourly = weather_json.get('hourly', {})\n",
    "\n",
    "# Costruisce liste di righe\n",
    "aq_rows = hourly_dict_to_rows(aq_hourly)\n",
    "weather_rows = hourly_dict_to_rows(weather_hourly)\n",
    "\n",
    "# Crea DataFrame Spark dalle liste di dict\n",
    "print(\"Creazione DataFrame Spark dalle righe flatten...\")\n",
    "aq_df_raw = spark.createDataFrame(aq_rows)\n",
    "weather_df_raw = spark.createDataFrame(weather_rows)\n",
    "\n",
    "# Funzione per normalizzare tipi di colonna e aggiungere ingestion_date\n",
    "def normalize_df(df, numeric_cols):\n",
    "    \"\"\"\n",
    "    - Converte 'time' in timestamp.\n",
    "    - Cast delle colonne numeriche a Double.\n",
    "    - Aggiunge ingestion_date (string) per il partitioning.\n",
    "    - Se una colonna attesa manca, la aggiunge con valori null per stabilità di schema.\n",
    "    \"\"\"\n",
    "    df2 = df.withColumn(\"time\", F.to_timestamp(F.col(\"time\")))\n",
    "    for c in numeric_cols:\n",
    "        if c in df2.columns:\n",
    "            df2 = df2.withColumn(c, F.col(c).cast(T.DoubleType()))\n",
    "        else:\n",
    "            df2 = df2.withColumn(c, F.lit(None).cast(T.DoubleType()))\n",
    "    df2 = df2.withColumn(\"ingestion_date\", F.lit(ingestion_date).cast(T.StringType()))\n",
    "    return df2\n",
    "\n",
    "# Elenco colonne attese per ogni API\n",
    "aq_expected_cols = [\n",
    "    \"pm10\",\"pm2_5\",\"carbon_monoxide\",\"carbon_dioxide\",\n",
    "    \"nitrogen_dioxide\",\"sulphur_dioxide\",\"ozone\"\n",
    "]\n",
    "weather_expected_cols = [\n",
    "    \"temperature_2m\",\"relative_humidity_2m\",\"dew_point_2m\",\n",
    "    \"apparent_temperature\",\"precipitation_probability\",\"rain\",\"wind_speed_10m\"\n",
    "]\n",
    "\n",
    "# Normalizza DataFrame\n",
    "aq_df = normalize_df(aq_df_raw, aq_expected_cols)\n",
    "weather_df = normalize_df(weather_df_raw, weather_expected_cols)\n",
    "\n",
    "# Riordina colonne per leggibilità\n",
    "aq_df = aq_df.select([\"time\"] + aq_expected_cols + [\"ingestion_date\"])\n",
    "weather_df = weather_df.select([\"time\"] + weather_expected_cols + [\"ingestion_date\"])\n",
    "\n",
    "# ---------------------------\n",
    "# 4) Bronze Layer: salva raw DataFrames come tabelle Delta (partitioned by ingestion_date)\n",
    "# ---------------------------\n",
    "\n",
    "print(\"Scrittura air_quality_bronze (Delta, append, partitioned by ingestion_date)...\")\n",
    "aq_df.write.format(\"delta\").mode(\"append\").partitionBy(\"ingestion_date\").saveAsTable(\"air_quality_bronze\")\n",
    "\n",
    "print(\"Scrittura weather_bronze (Delta, append, partitioned by ingestion_date)...\")\n",
    "weather_df.write.format(\"delta\").mode(\"append\").partitionBy(\"ingestion_date\").saveAsTable(\"weather_bronze\")\n",
    "\n",
    "# ---------------------------\n",
    "# 5) Merge DataFrames (inner join su 'time')\n",
    "# ---------------------------\n",
    "\n",
    "# Rinominare ingestion_date per evitare ambiguità\n",
    "weather_df_renamed = weather_df.select(\n",
    "    [F.col(\"time\")] +\n",
    "    [F.col(c).alias(c) for c in weather_expected_cols] +\n",
    "    [F.col(\"ingestion_date\").alias(\"weather_ingestion_date\")]\n",
    ")\n",
    "\n",
    "aq_df_renamed = aq_df.select(\n",
    "    [F.col(\"time\")] +\n",
    "    [F.col(c).alias(c) for c in aq_expected_cols] +\n",
    "    [F.col(\"ingestion_date\").alias(\"aq_ingestion_date\")]\n",
    ")\n",
    "\n",
    "# Join inner su 'time'\n",
    "merged_df = aq_df_renamed.join(\n",
    "    weather_df_renamed,\n",
    "    on=\"time\",\n",
    "    how=\"inner\"\n",
    ").withColumn(\"ingestion_date\", F.lit(ingestion_date).cast(T.StringType()))\n",
    "\n",
    "# Seleziona colonne finali in ordine desiderato: time, pollutanti, weather, ingestion_date\n",
    "merged_columns = [\"time\"] + aq_expected_cols + weather_expected_cols + [\"ingestion_date\"]\n",
    "merged_df = merged_df.select(*merged_columns)\n",
    "\n",
    "print(f\"Righe nel DataFrame unito (post inner join): {merged_df.count()}\")\n",
    "\n",
    "# ---------------------------\n",
    "# 6) Controlli di qualità sui dati\n",
    "# ---------------------------\n",
    "\n",
    "print(\"Esecuzione controlli di qualità sul DataFrame unito...\")\n",
    "\n",
    "# Null Check: conta null per ogni colonna di interesse\n",
    "cols_to_check = aq_expected_cols + weather_expected_cols\n",
    "null_counts_exprs = [F.count(F.when(F.col(c).isNull(), c)).alias(c + \"_nulls\") for c in cols_to_check]\n",
    "null_counts_row = merged_df.agg(*null_counts_exprs).collect()[0].asDict()\n",
    "\n",
    "print(\"Conteggio valori null per colonna:\")\n",
    "for k, v in null_counts_row.items():\n",
    "    print(f\"  {k}: {v}\")\n",
    "\n",
    "# Duplicate Check: duplicati basati sulla colonna 'time'\n",
    "total_rows = merged_df.count()\n",
    "distinct_times = merged_df.select(\"time\").distinct().count()\n",
    "duplicate_count = total_rows - distinct_times\n",
    "\n",
    "print(f\"Controllo duplicati: total_rows={total_rows}, distinct_times={distinct_times}, duplicates={duplicate_count}\")\n",
    "\n",
    "# Se ci sono duplicati, rimuovere lasciando la prima occorrenza per timestamp\n",
    "if duplicate_count > 0:\n",
    "    # Aggiunge id per ordinamento deterministico e mantiene la prima riga per timestamp\n",
    "    merged_df = merged_df.withColumn(\"_order_id\", F.monotonically_increasing_id())\n",
    "    win = Window.partitionBy(\"time\").orderBy(F.col(\"_order_id\"))\n",
    "    merged_df = merged_df.withColumn(\"_rn\", F.row_number().over(win)).filter(F.col(\"_rn\") == 1).drop(\"_order_id\", \"_rn\")\n",
    "    print(f\"Duplicati rimossi. Nuovo numero di righe: {merged_df.count()}\")\n",
    "else:\n",
    "    print(\"Nessun duplicato trovato sulla colonna 'time'.\")\n",
    "\n",
    "# Segnala colonne completamente null (se presenti)\n",
    "fully_null_cols = [c for c in cols_to_check if null_counts_row.get(c + \"_nulls\", 0) >= merged_df.count()]\n",
    "if fully_null_cols:\n",
    "    print(\"Attenzione: le seguenti colonne sono completamente null nel DataFrame unito:\")\n",
    "    for c in fully_null_cols:\n",
    "        print(f\"  {c}\")\n",
    "\n",
    "# ---------------------------\n",
    "# 7) Silver Layer: salva DataFrame pulito come tabella Delta (append)\n",
    "# ---------------------------\n",
    "\n",
    "print(\"Scrittura air_quality_and_weather_silver (Delta, append, partitioned by ingestion_date)...\")\n",
    "merged_df.write.format(\"delta\").mode(\"append\").partitionBy(\"ingestion_date\").saveAsTable(\"air_quality_and_weather_silver\")\n",
    "\n",
    "print(\"ETL completato. Tabelle Bronze e Silver persistite come Delta.\")\n",
    "\n",
    "# ---------------------------\n",
    "# 8) Report finale (stampa)\n",
    "# ---------------------------\n",
    "\n",
    "print(\"\\nRIEPILOGO FINALE\")\n",
    "print(\"----------------\")\n",
    "print(f\"Ingestion date (partition): {ingestion_date}\")\n",
    "final_row_count = merged_df.count()\n",
    "print(f\"Righe nel silver scritte (stimate): {final_row_count}\")\n",
    "\n",
    "print(\"Null counts per column (final):\")\n",
    "final_null_counts = merged_df.agg(*null_counts_exprs).collect()[0].asDict()\n",
    "for k, v in final_null_counts.items():\n",
    "    print(f\"  {k}: {v}\")\n",
    "\n",
    "print(f\"Duplicati rimossi: {duplicate_count}\")\n",
    "print(\"Script terminato con successo.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8cc2d9b9-5f6c-48b2-b6bd-a0a74e016dcc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "GPT 5 mini work at the first attempt"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Gpt 5 mini",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
