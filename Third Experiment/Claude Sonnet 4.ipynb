{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "72d1f4a0-8ed8-4e7e-8e84-2bd0f4f78f5f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Task**\n",
    "Create a Databricks Python code that extracts, processes, and loads data from two separate Open-Meteo APIs. The final output must be a clean, merged dataset saved to a Delta table using serverless compute.\n",
    "\n",
    "---\n",
    "\n",
    "**Requirements**\n",
    "\n",
    "### 1. Data Extraction\n",
    "* Fetch hourly air quality data from: `https://air-quality-api.open-meteo.com/v1/air-quality?latitude=40.3548&longitude=18.1724&hourly=pm10,pm2_5,carbon_monoxide,carbon_dioxide,nitrogen_dioxide,sulphur_dioxide,ozone&past_days=31&forecast_days=1`\n",
    "* Fetch hourly weather data from: `https://api.open-meteo.com/v1/forecast?latitude=40.3548&longitude=18.1724&hourly=temperature_2m,relative_humidity_2m,dew_point_2m,apparent_temperature,precipitation_probability,rain,wind_speed_10m&past_days=31&forecast_days=1`\n",
    "* Use the `requests` library to get the JSON responses.\n",
    "\n",
    "### 2. Data Processing and Merging\n",
    "* Parse the hourly JSON data from both APIs into two separate PySpark DataFrames.\n",
    "* The data within the `hourly` object (e.g., `time` and pollutant lists) must be flattened into structured rows.\n",
    "* Add an `ingestion_date` column to each DataFrame.\n",
    "* Merge the two DataFrames based on the `time` column to create a single, unified DataFrame. Use an `inner` join.\n",
    "\n",
    "### 3. Data Quality Checks\n",
    "* Implement checks to ensure data integrity and quality on the merged DataFrame.\n",
    "* **Null Check**: Identify and report the count of missing values in all pollutant and weather columns.\n",
    "* **Duplicate Check**: Identify duplicate entries based on the `time` column. Keep only the first occurrence for each timestamp.\n",
    "* Create a summary report using `print` statements to display the findings of the quality checks (e.g., total nulls, duplicate counts).\n",
    "\n",
    "### 4. Layered Architecture (Bronze & Silver)\n",
    "* **Bronze Layer**: Save the raw, unmerged DataFrames (air quality and weather) to separate Delta tables named `air_quality_bronze` and `weather_bronze`. Partition these tables by `ingestion_date` and use `append` mode.\n",
    "* **Silver Layer**: Save the final, clean, and merged DataFrame (after passing all quality checks) to a Delta table named `air_quality_and_weather_silver`. Use `append` mode for this save as well.\n",
    "\n",
    "### 5. Code Structure\n",
    "* The code should be a single, complete Databricks Python notebook script.\n",
    "* Include all necessary imports (`requests`, `pyspark.sql.functions`, etc.).\n",
    "* Add clear comments to explain each step of the ETL process.\n",
    "* The script should be runnable on Databricks using **serverless compute**.\n",
    "\n",
    "**Final Output**\n",
    "A comprehensive Python script that performs a full ETL pipeline with data merging, quality monitoring, and a Bronze/Silver layer architecture.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f412e6c-9fb2-495e-b6c5-db40fd65faf6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Databricks notebook source\n",
    "# MAGIC %md\n",
    "# MAGIC # Air Quality and Weather Data ETL Pipeline\n",
    "# MAGIC This notebook extracts data from Open-Meteo APIs, processes it, and loads it into Delta tables using a Bronze/Silver architecture\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Import required libraries\n",
    "import requests\n",
    "import json\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    col, lit, current_timestamp, explode, arrays_zip, \n",
    "    count, when, isnan, isnull, row_number, to_timestamp\n",
    ")\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window\n",
    "from datetime import datetime\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## 1. Data Extraction\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Define API endpoints\n",
    "air_quality_url = \"https://air-quality-api.open-meteo.com/v1/air-quality?latitude=40.3548&longitude=18.1724&hourly=pm10,pm2_5,carbon_monoxide,carbon_dioxide,nitrogen_dioxide,sulphur_dioxide,ozone&past_days=31&forecast_days=1\"\n",
    "weather_url = \"https://api.open-meteo.com/v1/forecast?latitude=40.3548&longitude=18.1724&hourly=temperature_2m,relative_humidity_2m,dew_point_2m,apparent_temperature,precipitation_probability,rain,wind_speed_10m&past_days=31&forecast_days=1\"\n",
    "\n",
    "# Fetch data from APIs\n",
    "print(\"Fetching air quality data...\")\n",
    "air_quality_response = requests.get(air_quality_url)\n",
    "air_quality_data = air_quality_response.json()\n",
    "\n",
    "print(\"Fetching weather data...\")\n",
    "weather_response = requests.get(weather_url)\n",
    "weather_data = weather_response.json()\n",
    "\n",
    "print(\"Data extraction completed successfully!\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## 2. Data Processing and Transformation\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Function to flatten hourly data into structured rows\n",
    "def flatten_hourly_data(data, data_type):\n",
    "    \"\"\"\n",
    "    Flattens the nested hourly data structure into a list of dictionaries\n",
    "    \"\"\"\n",
    "    hourly_data = data['hourly']\n",
    "    time_array = hourly_data['time']\n",
    "    \n",
    "    # Create list to store flattened records\n",
    "    flattened_records = []\n",
    "    \n",
    "    # Get all keys except 'time'\n",
    "    metric_keys = [key for key in hourly_data.keys() if key != 'time']\n",
    "    \n",
    "    # Iterate through each timestamp\n",
    "    for i, timestamp in enumerate(time_array):\n",
    "        record = {'time': timestamp}\n",
    "        \n",
    "        # Add all metrics for this timestamp\n",
    "        for metric in metric_keys:\n",
    "            if metric in hourly_data and i < len(hourly_data[metric]):\n",
    "                record[metric] = hourly_data[metric][i]\n",
    "            else:\n",
    "                record[metric] = None\n",
    "                \n",
    "        flattened_records.append(record)\n",
    "    \n",
    "    return flattened_records\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Flatten air quality data\n",
    "air_quality_flattened = flatten_hourly_data(air_quality_data, 'air_quality')\n",
    "print(f\"Flattened {len(air_quality_flattened)} air quality records\")\n",
    "\n",
    "# Flatten weather data\n",
    "weather_flattened = flatten_hourly_data(weather_data, 'weather')\n",
    "print(f\"Flattened {len(weather_flattened)} weather records\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Create PySpark DataFrames from flattened data\n",
    "# Air quality DataFrame\n",
    "air_quality_df = spark.createDataFrame(air_quality_flattened)\n",
    "\n",
    "# Add ingestion_date column\n",
    "air_quality_df = air_quality_df.withColumn(\"ingestion_date\", current_timestamp().cast(\"date\"))\n",
    "\n",
    "# Convert time column to timestamp\n",
    "air_quality_df = air_quality_df.withColumn(\"time\", to_timestamp(col(\"time\"), \"yyyy-MM-dd'T'HH:mm\"))\n",
    "\n",
    "print(\"Air Quality DataFrame Schema:\")\n",
    "air_quality_df.printSchema()\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Weather DataFrame\n",
    "weather_df = spark.createDataFrame(weather_flattened)\n",
    "\n",
    "# Add ingestion_date column\n",
    "weather_df = weather_df.withColumn(\"ingestion_date\", current_timestamp().cast(\"date\"))\n",
    "\n",
    "# Convert time column to timestamp\n",
    "weather_df = weather_df.withColumn(\"time\", to_timestamp(col(\"time\"), \"yyyy-MM-dd'T'HH:mm\"))\n",
    "\n",
    "print(\"Weather DataFrame Schema:\")\n",
    "weather_df.printSchema()\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## 3. Save Bronze Layer (Raw Data)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Save air quality data to Bronze layer\n",
    "print(\"Saving air quality data to Bronze layer...\")\n",
    "air_quality_df.write \\\n",
    "    .mode(\"append\") \\\n",
    "    .partitionBy(\"ingestion_date\") \\\n",
    "    .format(\"delta\") \\\n",
    "    .saveAsTable(\"air_quality_bronze\")\n",
    "\n",
    "print(\"Air quality Bronze layer saved successfully!\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Save weather data to Bronze layer\n",
    "print(\"Saving weather data to Bronze layer...\")\n",
    "weather_df.write \\\n",
    "    .mode(\"append\") \\\n",
    "    .partitionBy(\"ingestion_date\") \\\n",
    "    .format(\"delta\") \\\n",
    "    .saveAsTable(\"weather_bronze\")\n",
    "\n",
    "print(\"Weather Bronze layer saved successfully!\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## 4. Data Merging\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Merge air quality and weather data on time column using inner join\n",
    "print(\"Merging air quality and weather data...\")\n",
    "merged_df = air_quality_df.join(\n",
    "    weather_df,\n",
    "    air_quality_df.time == weather_df.time,\n",
    "    \"inner\"\n",
    ").select(\n",
    "    air_quality_df.time,\n",
    "    air_quality_df.pm10,\n",
    "    air_quality_df.pm2_5,\n",
    "    air_quality_df.carbon_monoxide,\n",
    "    air_quality_df.carbon_dioxide,\n",
    "    air_quality_df.nitrogen_dioxide,\n",
    "    air_quality_df.sulphur_dioxide,\n",
    "    air_quality_df.ozone,\n",
    "    weather_df.temperature_2m,\n",
    "    weather_df.relative_humidity_2m,\n",
    "    weather_df.dew_point_2m,\n",
    "    weather_df.apparent_temperature,\n",
    "    weather_df.precipitation_probability,\n",
    "    weather_df.rain,\n",
    "    weather_df.wind_speed_10m,\n",
    "    air_quality_df.ingestion_date\n",
    ")\n",
    "\n",
    "print(f\"Merged DataFrame contains {merged_df.count()} records\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## 5. Data Quality Checks\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Define columns to check for nulls\n",
    "pollutant_columns = ['pm10', 'pm2_5', 'carbon_monoxide', 'carbon_dioxide', \n",
    "                     'nitrogen_dioxide', 'sulphur_dioxide', 'ozone']\n",
    "weather_columns = ['temperature_2m', 'relative_humidity_2m', 'dew_point_2m', \n",
    "                   'apparent_temperature', 'precipitation_probability', 'rain', 'wind_speed_10m']\n",
    "all_check_columns = pollutant_columns + weather_columns\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Null Check - Count missing values for each column\n",
    "print(\"=\" * 80)\n",
    "print(\"DATA QUALITY CHECK REPORT\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\n1. NULL VALUE CHECK:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "null_counts = {}\n",
    "for column in all_check_columns:\n",
    "    null_count = merged_df.filter(col(column).isNull() | isnan(col(column))).count()\n",
    "    null_counts[column] = null_count\n",
    "    if null_count > 0:\n",
    "        print(f\"   {column}: {null_count} null values\")\n",
    "\n",
    "total_nulls = sum(null_counts.values())\n",
    "print(f\"\\n   Total null values across all columns: {total_nulls}\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Duplicate Check - Check for duplicate timestamps\n",
    "print(\"\\n2. DUPLICATE CHECK:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Count total records before deduplication\n",
    "total_records_before = merged_df.count()\n",
    "\n",
    "# Check for duplicates based on time column\n",
    "duplicate_count = merged_df.groupBy(\"time\").count().filter(col(\"count\") > 1).count()\n",
    "print(f\"   Number of duplicate timestamps found: {duplicate_count}\")\n",
    "\n",
    "# Get actual number of duplicate records (total duplicates - unique timestamps)\n",
    "if duplicate_count > 0:\n",
    "    duplicate_records = merged_df.groupBy(\"time\").count().filter(col(\"count\") > 1)\n",
    "    total_duplicate_records = duplicate_records.agg({\"count\": \"sum\"}).collect()[0][0] - duplicate_count\n",
    "    print(f\"   Total duplicate records to be removed: {total_duplicate_records}\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Remove duplicates - keep only first occurrence for each timestamp\n",
    "print(\"\\n3. DATA CLEANING:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Create window specification for deduplication\n",
    "window_spec = Window.partitionBy(\"time\").orderBy(\"time\")\n",
    "\n",
    "# Add row number and keep only first occurrence\n",
    "cleaned_df = merged_df.withColumn(\"row_num\", row_number().over(window_spec)) \\\n",
    "    .filter(col(\"row_num\") == 1) \\\n",
    "    .drop(\"row_num\")\n",
    "\n",
    "total_records_after = cleaned_df.count()\n",
    "records_removed = total_records_before - total_records_after\n",
    "\n",
    "print(f\"   Records before cleaning: {total_records_before}\")\n",
    "print(f\"   Records after cleaning: {total_records_after}\")\n",
    "print(f\"   Records removed: {records_removed}\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Additional quality metrics\n",
    "print(\"\\n4. DATA COMPLETENESS METRICS:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "total_cells = total_records_after * len(all_check_columns)\n",
    "total_non_null_cells = total_cells - sum(null_counts.values())\n",
    "completeness_percentage = (total_non_null_cells / total_cells) * 100\n",
    "\n",
    "print(f\"   Total data cells: {total_cells}\")\n",
    "print(f\"   Non-null data cells: {total_non_null_cells}\")\n",
    "print(f\"   Data completeness: {completeness_percentage:.2f}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"DATA QUALITY CHECK COMPLETED\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## 6. Save Silver Layer (Clean, Merged Data)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Save cleaned and merged data to Silver layer\n",
    "print(\"Saving cleaned and merged data to Silver layer...\")\n",
    "\n",
    "cleaned_df.write \\\n",
    "    .mode(\"append\") \\\n",
    "    .format(\"delta\") \\\n",
    "    .saveAsTable(\"air_quality_and_weather_silver\")\n",
    "\n",
    "print(\"Silver layer saved successfully!\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## 7. Verify Final Results\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Display sample of final Silver layer data\n",
    "print(\"Sample of final Silver layer data:\")\n",
    "display(spark.table(\"air_quality_and_weather_silver\").limit(10))\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Show record counts for all tables\n",
    "print(\"Record counts for all tables:\")\n",
    "print(f\"Air Quality Bronze: {spark.table('air_quality_bronze').count()} records\")\n",
    "print(f\"Weather Bronze: {spark.table('weather_bronze').count()} records\")\n",
    "print(f\"Silver Layer: {spark.table('air_quality_and_weather_silver').count()} records\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Pipeline Execution Summary\n",
    "# MAGIC \n",
    "# MAGIC The ETL pipeline has been successfully executed with the following steps:\n",
    "# MAGIC \n",
    "# MAGIC 1. **Data Extraction**: Retrieved data from Open-Meteo Air Quality and Weather APIs\n",
    "# MAGIC 2. **Data Processing**: Flattened nested JSON structures into structured DataFrames\n",
    "# MAGIC 3. **Bronze Layer**: Saved raw data to partitioned Delta tables\n",
    "# MAGIC 4. **Data Merging**: Performed inner join on time column\n",
    "# MAGIC 5. **Quality Checks**: Identified nulls and duplicates, cleaned data\n",
    "# MAGIC 6. **Silver Layer**: Saved clean, merged dataset to Delta table\n",
    "# MAGIC \n",
    "# MAGIC The pipeline is designed to run on Databricks serverless compute and follows medallion architecture best practices.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "adf7d26c-2a02-4d3d-b9d9-0964937ea4d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Claude Sonnet 4 work at the first attempt"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Claude Sonnet 4",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
