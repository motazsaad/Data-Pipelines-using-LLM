{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "00d8612f-b231-469d-b7a9-cda5cca9c1a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Task**\n",
    "Create a Databricks Python code that extracts, processes, and loads data from two separate Open-Meteo APIs. The final output must be a clean, merged dataset saved to a Delta table using serverless compute.\n",
    "\n",
    "---\n",
    "\n",
    "**Requirements**\n",
    "\n",
    "### 1. Data Extraction\n",
    "* Fetch hourly air quality data from: `https://air-quality-api.open-meteo.com/v1/air-quality?latitude=40.3548&longitude=18.1724&hourly=pm10,pm2_5,carbon_monoxide,carbon_dioxide,nitrogen_dioxide,sulphur_dioxide,ozone&past_days=31&forecast_days=1`\n",
    "* Fetch hourly weather data from: `https://api.open-meteo.com/v1/forecast?latitude=40.3548&longitude=18.1724&hourly=temperature_2m,relative_humidity_2m,dew_point_2m,apparent_temperature,precipitation_probability,rain,wind_speed_10m&past_days=31&forecast_days=1`\n",
    "* Use the `requests` library to get the JSON responses.\n",
    "\n",
    "### 2. Data Processing and Merging\n",
    "* Parse the hourly JSON data from both APIs into two separate PySpark DataFrames.\n",
    "* The data within the `hourly` object (e.g., `time` and pollutant lists) must be flattened into structured rows.\n",
    "* Add an `ingestion_date` column to each DataFrame.\n",
    "* Merge the two DataFrames based on the `time` column to create a single, unified DataFrame. Use an `inner` join.\n",
    "\n",
    "### 3. Data Quality Checks\n",
    "* Implement checks to ensure data integrity and quality on the merged DataFrame.\n",
    "* **Null Check**: Identify and report the count of missing values in all pollutant and weather columns.\n",
    "* **Duplicate Check**: Identify duplicate entries based on the `time` column. Keep only the first occurrence for each timestamp.\n",
    "* Create a summary report using `print` statements to display the findings of the quality checks (e.g., total nulls, duplicate counts).\n",
    "\n",
    "### 4. Layered Architecture (Bronze & Silver)\n",
    "* **Bronze Layer**: Save the raw, unmerged DataFrames (air quality and weather) to separate Delta tables named `air_quality_bronze` and `weather_bronze`. Partition these tables by `ingestion_date` and use `append` mode.\n",
    "* **Silver Layer**: Save the final, clean, and merged DataFrame (after passing all quality checks) to a Delta table named `air_quality_and_weather_silver`. Use `append` mode for this save as well.\n",
    "\n",
    "### 5. Code Structure\n",
    "* The code should be a single, complete Databricks Python notebook script.\n",
    "* Include all necessary imports (`requests`, `pyspark.sql.functions`, etc.).\n",
    "* Add clear comments to explain each step of the ETL process.\n",
    "* The script should be runnable on Databricks using **serverless compute**.\n",
    "\n",
    "**Final Output**\n",
    "A comprehensive Python script that performs a full ETL pipeline with data merging, quality monitoring, and a Bronze/Silver layer architecture.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "382f35d0-38ba-42c7-947c-c1acc02c8240",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Databricks Notebook: Open-Meteo Data ETL Pipeline\n",
    "\n",
    "# Import required libraries\n",
    "import requests\n",
    "import json\n",
    "import traceback\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, current_timestamp, lit, expr\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, TimestampType\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder.appName(\"OpenMeteoDataETL\").getOrCreate()\n",
    "\n",
    "# Define API Endpoints\n",
    "AIR_QUALITY_API = \"https://air-quality-api.open-meteo.com/v1/air-quality\"\n",
    "WEATHER_API = \"https://api.open-meteo.com/v1/forecast\"\n",
    "\n",
    "# Separate parameters for each API\n",
    "AIR_QUALITY_PARAMS = {\n",
    "    \"latitude\": 40.3548,\n",
    "    \"longitude\": 18.1724,\n",
    "    \"hourly\": \"pm10,pm2_5,carbon_monoxide,carbon_dioxide,nitrogen_dioxide,sulphur_dioxide,ozone\",\n",
    "    \"past_days\": 31,\n",
    "    \"forecast_days\": 1\n",
    "}\n",
    "\n",
    "WEATHER_PARAMS = {\n",
    "    \"latitude\": 40.3548,\n",
    "    \"longitude\": 18.1724,\n",
    "    \"hourly\": \"temperature_2m,relative_humidity_2m,dew_point_2m,apparent_temperature,precipitation_probability,rain,wind_speed_10m\",\n",
    "    \"past_days\": 31,\n",
    "    \"forecast_days\": 1\n",
    "}\n",
    "\n",
    "# (Keep previous fetch_api_data and safe_fetch_data functions)\n",
    "\n",
    "# 2. Data Processing and Merging\n",
    "def create_dataframe_from_hourly_data(data, data_type):\n",
    "    \"\"\"\n",
    "    Convert hourly API data to Spark DataFrame with enhanced error handling\n",
    "    \n",
    "    Args:\n",
    "        data (dict): API response data\n",
    "        data_type (str): Type of data (air_quality or weather)\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame: Spark DataFrame with hourly data\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Validate hourly data\n",
    "        if 'hourly' not in data or 'time' not in data['hourly']:\n",
    "            raise ValueError(f\"Invalid hourly data structure for {data_type}\")\n",
    "        \n",
    "        # Prepare schema dynamically based on hourly keys\n",
    "        schema_fields = [\n",
    "            StructField(\"time\", TimestampType(), True),\n",
    "            *[StructField(key, DoubleType(), True) for key in data['hourly'].keys() if key != 'time']\n",
    "        ]\n",
    "        schema = StructType(schema_fields)\n",
    "        \n",
    "        # Safely convert timestamps\n",
    "        def safe_timestamp_convert(timestamp_str):\n",
    "            try:\n",
    "                return spark.sql(f\"select to_timestamp('{timestamp_str}') as time\").first().time\n",
    "            except Exception as e:\n",
    "                print(f\"Timestamp conversion error for {timestamp_str}: {e}\")\n",
    "                return None\n",
    "        \n",
    "        # Zip time with other columns, handling potential conversion errors\n",
    "        zipped_data = []\n",
    "        for i, timestamp in enumerate(data['hourly']['time']):\n",
    "            converted_time = safe_timestamp_convert(timestamp)\n",
    "            if converted_time is not None:\n",
    "                row_data = [converted_time]\n",
    "                row_data.extend([\n",
    "                    data['hourly'][col][i] if col != 'time' else None \n",
    "                    for col in data['hourly'].keys() if col != 'time'\n",
    "                ])\n",
    "                zipped_data.append(row_data)\n",
    "        \n",
    "        # Create DataFrame\n",
    "        df = spark.createDataFrame(zipped_data, schema=schema)\n",
    "        \n",
    "        # Add metadata columns\n",
    "        df = (df\n",
    "              .withColumn(\"data_source\", lit(data_type))\n",
    "              .withColumn(\"ingestion_timestamp\", current_timestamp())\n",
    "        )\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error creating DataFrame for {data_type}: {e}\")\n",
    "        traceback.print_exc()\n",
    "        raise\n",
    "\n",
    "# (Keep previous error handling and data fetching code)\n",
    "\n",
    "# 3. Data Quality Checks\n",
    "def perform_data_quality_checks(df):\n",
    "    \"\"\"\n",
    "    Perform data quality checks on DataFrame\n",
    "    \n",
    "    Args:\n",
    "        df (DataFrame): Input DataFrame\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame: Cleaned DataFrame\n",
    "    \"\"\"\n",
    "    # Null Check\n",
    "    null_counts = df.select([col(c).isNull().cast(\"int\").alias(c) for c in df.columns])\n",
    "    null_summary = null_counts.groupBy().sum().collect()[0]\n",
    "    \n",
    "    print(\"--- Data Quality Checks ---\")\n",
    "    print(\"Null Value Counts:\")\n",
    "    for col_name, count in zip(df.columns, null_summary):\n",
    "        print(f\"{col_name}: {count}\")\n",
    "    \n",
    "    # Duplicate Check\n",
    "    total_rows = df.count()\n",
    "    distinct_rows = df.dropDuplicates([\"time\"]).count()\n",
    "    duplicate_count = total_rows - distinct_rows\n",
    "    \n",
    "    print(f\"\\nDuplicate Entries (based on time): {duplicate_count}\")\n",
    "    \n",
    "    # Remove duplicates\n",
    "    df = df.dropDuplicates([\"time\"])\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply data quality checks\n",
    "air_quality_df = perform_data_quality_checks(air_quality_df)\n",
    "weather_df = perform_data_quality_checks(weather_df)\n",
    "\n",
    "# 4. Merge DataFrames\n",
    "# Rename columns to avoid conflicts\n",
    "def prepare_dataframe_for_merge(df, prefix):\n",
    "    \"\"\"\n",
    "    Prepare DataFrame for merging by renaming columns\n",
    "    \n",
    "    Args:\n",
    "        df (DataFrame): Input DataFrame\n",
    "        prefix (str): Prefix for columns\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame: Prepared DataFrame\n",
    "    \"\"\"\n",
    "    # Select and rename columns, excluding metadata columns\n",
    "    renamed_cols = [\n",
    "        col(\"time\")\n",
    "    ]\n",
    "    \n",
    "    # Rename data columns\n",
    "    for column in df.columns:\n",
    "        if column not in ['time', 'data_source', 'ingestion_timestamp']:\n",
    "            renamed_cols.append(col(column).alias(f\"{prefix}_{column}\"))\n",
    "    \n",
    "    return df.select(renamed_cols)\n",
    "\n",
    "# Prepare DataFrames for merging\n",
    "air_quality_df_merged = prepare_dataframe_for_merge(air_quality_df, \"aq\")\n",
    "weather_df_merged = prepare_dataframe_for_merge(weather_df, \"wx\")\n",
    "\n",
    "# Merge DataFrames\n",
    "merged_df = air_quality_df_merged.join(weather_df_merged, \"time\", \"inner\")\n",
    "\n",
    "# 5. Layered Architecture\n",
    "# Bronze Layer\n",
    "def save_bronze_table(df, table_name):\n",
    "    \"\"\"\n",
    "    Save DataFrame to Bronze layer Delta table\n",
    "    \n",
    "    Args:\n",
    "        df (DataFrame): Input DataFrame\n",
    "        table_name (str): Name of the table to save\n",
    "    \"\"\"\n",
    "    # Remove any existing columns that might cause conflicts\n",
    "    columns_to_keep = [col for col in df.columns if col not in ['ingestion_timestamp']]\n",
    "    \n",
    "    df.select(columns_to_keep) \\\n",
    "      .write.format(\"delta\") \\\n",
    "      .mode(\"overwrite\") \\\n",
    "      .saveAsTable(table_name)\n",
    "\n",
    "# Save Bronze tables\n",
    "save_bronze_table(air_quality_df, \"air_quality_bronze\")\n",
    "save_bronze_table(weather_df, \"weather_bronze\")\n",
    "\n",
    "# Silver Layer\n",
    "merged_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"air_quality_and_weather_silver\")\n",
    "\n",
    "print(\"ETL Process Completed Successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "255f4ee0-bf5d-4e6d-a13c-6c9fd023eba3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Claude Haiku 3.5 need 8 fixes to work\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Claude Haiku 3.5",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
