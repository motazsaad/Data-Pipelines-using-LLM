{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4784afef-bcdf-49fc-ad14-cacb13c6e7c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Task**\n",
    "Create a Databricks Python code that extracts, processes, and loads data from two separate Open-Meteo APIs. The final output must be a clean, merged dataset saved to a Delta table using serverless compute.\n",
    "\n",
    "---\n",
    "\n",
    "**Requirements**\n",
    "\n",
    "### 1. Data Extraction\n",
    "* Fetch hourly air quality data from: `https://air-quality-api.open-meteo.com/v1/air-quality?latitude=40.3548&longitude=18.1724&hourly=pm10,pm2_5,carbon_monoxide,carbon_dioxide,nitrogen_dioxide,sulphur_dioxide,ozone&past_days=31&forecast_days=1`\n",
    "* Fetch hourly weather data from: `https://api.open-meteo.com/v1/forecast?latitude=40.3548&longitude=18.1724&hourly=temperature_2m,relative_humidity_2m,dew_point_2m,apparent_temperature,precipitation_probability,rain,wind_speed_10m&past_days=31&forecast_days=1`\n",
    "* Use the `requests` library to get the JSON responses.\n",
    "\n",
    "### 2. Data Processing and Merging\n",
    "* Parse the hourly JSON data from both APIs into two separate PySpark DataFrames.\n",
    "* The data within the `hourly` object (e.g., `time` and pollutant lists) must be flattened into structured rows.\n",
    "* Add an `ingestion_date` column to each DataFrame.\n",
    "* Merge the two DataFrames based on the `time` column to create a single, unified DataFrame. Use an `inner` join.\n",
    "\n",
    "### 3. Data Quality Checks\n",
    "* Implement checks to ensure data integrity and quality on the merged DataFrame.\n",
    "* **Null Check**: Identify and report the count of missing values in all pollutant and weather columns.\n",
    "* **Duplicate Check**: Identify duplicate entries based on the `time` column. Keep only the first occurrence for each timestamp.\n",
    "* Create a summary report using `print` statements to display the findings of the quality checks (e.g., total nulls, duplicate counts).\n",
    "\n",
    "### 4. Layered Architecture (Bronze & Silver)\n",
    "* **Bronze Layer**: Save the raw, unmerged DataFrames (air quality and weather) to separate Delta tables named `air_quality_bronze` and `weather_bronze`. Partition these tables by `ingestion_date` and use `append` mode.\n",
    "* **Silver Layer**: Save the final, clean, and merged DataFrame (after passing all quality checks) to a Delta table named `air_quality_and_weather_silver`. Use `append` mode for this save as well.\n",
    "\n",
    "### 5. Code Structure\n",
    "* The code should be a single, complete Databricks Python notebook script.\n",
    "* Include all necessary imports (`requests`, `pyspark.sql.functions`, etc.).\n",
    "* Add clear comments to explain each step of the ETL process.\n",
    "* The script should be runnable on Databricks using **serverless compute**.\n",
    "\n",
    "**Final Output**\n",
    "A comprehensive Python script that performs a full ETL pipeline with data merging, quality monitoring, and a Bronze/Silver layer architecture.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "71989ea7-b785-48cd-871e-fcff7a5f027d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# =============================================\n",
    "# Databricks Notebook: Open-Meteo ETL Pipeline\n",
    "# Environment: Databricks Serverless Compute\n",
    "# Features: API Extraction, Data Quality Checks, Delta Lake (Bronze & Silver)\n",
    "# =============================================\n",
    "\n",
    "# ----------------------------\n",
    "# 1. IMPORTS (Avoid wildcard to prevent name conflicts)\n",
    "# ----------------------------\n",
    "import requests\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lit, to_timestamp\n",
    "from pyspark.sql.types import *\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import builtins  # To safely access Python's built-in functions\n",
    "\n",
    "# Safely preserve Python built-ins before any potential overrides\n",
    "py_sum = builtins.sum\n",
    "\n",
    "# Get or create Spark session\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Set ingestion date\n",
    "ingestion_date = datetime.now().date()\n",
    "print(f\"üöÄ Starting ETL pipeline | Ingestion Date: {ingestion_date}\")\n",
    "\n",
    "# ----------------------------\n",
    "# 2. DATA EXTRACTION FROM OPEN-METO APIs\n",
    "# ----------------------------\n",
    "\n",
    "air_quality_url = (\n",
    "    \"https://air-quality-api.open-meteo.com/v1/air-quality?\"\n",
    "    \"latitude=40.3548&longitude=18.1724\"\n",
    "    \"&hourly=pm10,pm2_5,carbon_monoxide,carbon_dioxide,nitrogen_dioxide,sulphur_dioxide,ozone\"\n",
    "    \"&past_days=31&forecast_days=1\"\n",
    ")\n",
    "\n",
    "weather_url = (\n",
    "    \"https://api.open-meteo.com/v1/forecast?\"\n",
    "    \"latitude=40.3548&longitude=18.1724\"\n",
    "    \"&hourly=temperature_2m,relative_humidity_2m,dew_point_2m,\"\n",
    "    \"apparent_temperature,precipitation_probability,rain,wind_speed_10m\"\n",
    "    \"&past_days=31&forecast_days=1\"\n",
    ")\n",
    "\n",
    "print(\"üì° Fetching data from Open-Meteo APIs...\")\n",
    "\n",
    "try:\n",
    "    air_quality_response = requests.get(air_quality_url)\n",
    "    air_quality_response.raise_for_status()\n",
    "    air_quality_json = air_quality_response.json()\n",
    "    print(\"‚úÖ Air quality data fetched successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Failed to fetch air quality data: {e}\")\n",
    "    raise\n",
    "\n",
    "try:\n",
    "    weather_response = requests.get(weather_url)\n",
    "    weather_response.raise_for_status()\n",
    "    weather_json = weather_response.json()\n",
    "    print(\"‚úÖ Weather data fetched successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Failed to fetch weather data: {e}\")\n",
    "    raise\n",
    "\n",
    "# ----------------------------\n",
    "# 3. FLATTEN JSON INTO PySpark DATAFRAMES\n",
    "# ----------------------------\n",
    "\n",
    "def flatten_hourly_data(api_response):\n",
    "    \"\"\"\n",
    "    Converts 'hourly' object into a Pandas DataFrame.\n",
    "    Each timestamp becomes a row.\n",
    "    \"\"\"\n",
    "    hourly = api_response.get(\"hourly\", {})\n",
    "    time_stamps = hourly.get(\"time\", [])\n",
    "    \n",
    "    rows = []\n",
    "    for idx, t in enumerate(time_stamps):\n",
    "        row = {\"time\": t}\n",
    "        for key, values in hourly.items():\n",
    "            if key != \"time\":\n",
    "                row[key] = values[idx] if idx < len(values) else None\n",
    "        rows.append(row)\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "print(\"üîÅ Flattening API responses into structured DataFrames...\")\n",
    "\n",
    "# Convert to Spark DataFrames\n",
    "air_quality_pdf = flatten_hourly_data(air_quality_json)\n",
    "weather_pdf = flatten_hourly_data(weather_json)\n",
    "\n",
    "air_quality_df = spark.createDataFrame(air_quality_pdf)\n",
    "weather_df = spark.createDataFrame(weather_pdf)\n",
    "\n",
    "# Add ingestion_date and fix time type\n",
    "air_quality_df = air_quality_df.withColumn(\"ingestion_date\", lit(ingestion_date).cast(DateType())) \\\n",
    "                               .withColumn(\"time\", to_timestamp(col(\"time\")))\n",
    "\n",
    "weather_df = weather_df.withColumn(\"ingestion_date\", lit(ingestion_date).cast(DateType())) \\\n",
    "                       .withColumn(\"time\", to_timestamp(col(\"time\")))\n",
    "\n",
    "# Validate counts without caching\n",
    "try:\n",
    "    air_count = air_quality_df.count()\n",
    "    weather_count = weather_df.count()\n",
    "    print(f\"üìä Air Quality Rows: {air_count}\")\n",
    "    print(f\"üìä Weather Rows: {weather_count}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error counting rows: {e}\")\n",
    "    raise\n",
    "\n",
    "# ----------------------------\n",
    "# 4. BRONZE LAYER: SAVE RAW DATA TO DELTA TABLES\n",
    "# ----------------------------\n",
    "\n",
    "bronze_air_table = \"air_quality_bronze\"\n",
    "bronze_weather_table = \"weather_bronze\"\n",
    "silver_table = \"air_quality_and_weather_silver\"\n",
    "\n",
    "print(\"üì¶ Writing raw data to Bronze Delta tables...\")\n",
    "\n",
    "# Write Air Quality Bronze\n",
    "air_quality_df.write \\\n",
    "    .mode(\"append\") \\\n",
    "    .format(\"delta\") \\\n",
    "    .partitionBy(\"ingestion_date\") \\\n",
    "    .option(\"mergeSchema\", \"true\") \\\n",
    "    .saveAsTable(bronze_air_table)\n",
    "\n",
    "# Write Weather Bronze\n",
    "weather_df.write \\\n",
    "    .mode(\"append\") \\\n",
    "    .format(\"delta\") \\\n",
    "    .partitionBy(\"ingestion_date\") \\\n",
    "    .option(\"mergeSchema\", \"true\") \\\n",
    "    .saveAsTable(bronze_weather_table)\n",
    "\n",
    "print(f\"‚úÖ Raw data saved to '{bronze_air_table}' and '{bronze_weather_table}'.\")\n",
    "\n",
    "# ----------------------------\n",
    "# 5. MERGE ON 'time' USING INNER JOIN\n",
    "# ----------------------------\n",
    "\n",
    "print(\"üîó Merging air quality and weather data on 'time' column...\")\n",
    "merged_df = air_quality_df.alias(\"aq\") \\\n",
    "    .join(\n",
    "        weather_df.alias(\"w\"),\n",
    "        col(\"aq.time\") == col(\"w.time\"),\n",
    "        \"inner\"\n",
    "    ) \\\n",
    "    .select(\n",
    "        col(\"aq.time\"),\n",
    "        # Air Quality Metrics\n",
    "        col(\"aq.pm10\"),\n",
    "        col(\"aq.pm2_5\"),\n",
    "        col(\"aq.carbon_monoxide\"),\n",
    "        col(\"aq.carbon_dioxide\"),\n",
    "        col(\"aq.nitrogen_dioxide\"),\n",
    "        col(\"aq.sulphur_dioxide\"),\n",
    "        col(\"aq.ozone\"),\n",
    "        # Weather Metrics\n",
    "        col(\"w.temperature_2m\"),\n",
    "        col(\"w.relative_humidity_2m\"),\n",
    "        col(\"w.dew_point_2m\"),\n",
    "        col(\"w.apparent_temperature\"),\n",
    "        col(\"w.precipitation_probability\"),\n",
    "        col(\"w.rain\"),\n",
    "        col(\"w.wind_speed_10m\"),\n",
    "        # Metadata\n",
    "        col(\"aq.ingestion_date\")\n",
    "    )\n",
    "\n",
    "merged_count = merged_df.count()\n",
    "print(f\"‚úÖ Merged DataFrame created with {merged_count} records.\")\n",
    "\n",
    "# ----------------------------\n",
    "# 6. DATA QUALITY CHECKS\n",
    "# ----------------------------\n",
    "\n",
    "print(\"\\nüîç RUNNING DATA QUALITY CHECKS...\")\n",
    "\n",
    "# Define columns\n",
    "pollutant_cols = [\"pm10\", \"pm2_5\", \"carbon_monoxide\", \"carbon_dioxide\",\n",
    "                  \"nitrogen_dioxide\", \"sulphur_dioxide\", \"ozone\"]\n",
    "weather_cols = [\"temperature_2m\", \"relative_humidity_2m\", \"dew_point_2m\",\n",
    "                \"apparent_temperature\", \"precipitation_probability\", \"rain\", \"wind_speed_10m\"]\n",
    "all_metric_cols = pollutant_cols + weather_cols\n",
    "\n",
    "# NULL CHECK\n",
    "print(\"\\nüßæ Null value count per metric column:\")\n",
    "null_counts = {}\n",
    "for col_name in all_metric_cols:\n",
    "    null_cnt = merged_df.filter(col(col_name).isNull()).count()\n",
    "    null_counts[col_name] = null_cnt\n",
    "    status = \"‚ö†Ô∏è\" if null_cnt > 0 else \"‚úÖ\"\n",
    "    print(f\"  {status} {col_name}: {null_cnt}\")\n",
    "\n",
    "# Use Python-native sum (safe from Spark override)\n",
    "total_nulls = py_sum(null_counts.values())\n",
    "print(f\"üìä Total null values across all metrics: {total_nulls}\")\n",
    "\n",
    "# DUPLICATE CHECK ON 'time'\n",
    "print(\"\\nüìã Duplicate check on 'time' column...\")\n",
    "total_rows = merged_df.count()\n",
    "distinct_times = merged_df.select(\"time\").distinct().count()\n",
    "duplicate_count = total_rows - distinct_times\n",
    "\n",
    "print(f\"Total rows          : {total_rows}\")\n",
    "print(f\"Distinct timestamps : {distinct_times}\")\n",
    "print(f\"Detected duplicates : {duplicate_count}\")\n",
    "\n",
    "# Deduplicate: Keep first occurrence\n",
    "cleaned_df = merged_df.dropDuplicates([\"time\"])\n",
    "final_row_count = cleaned_df.count()\n",
    "print(f\"‚úÖ After deduplication: {final_row_count} valid records retained.\")\n",
    "\n",
    "# ----------------------------\n",
    "# 7. SILVER LAYER: SAVE CLEANED DATA\n",
    "# ----------------------------\n",
    "\n",
    "print(f\"\\nüíæ Saving cleaned data to Silver Delta table: `{silver_table}`...\")\n",
    "\n",
    "# Finalize schema and order\n",
    "cleaned_df = cleaned_df.select(\n",
    "    \"time\",\n",
    "    *pollutant_cols,\n",
    "    *weather_cols,\n",
    "    \"ingestion_date\"\n",
    ").orderBy(\"time\")\n",
    "\n",
    "# Write to Silver\n",
    "cleaned_df.write \\\n",
    "    .mode(\"append\") \\\n",
    "    .format(\"delta\") \\\n",
    "    .option(\"mergeSchema\", \"true\") \\\n",
    "    .saveAsTable(silver_table)\n",
    "\n",
    "print(f\"‚úÖ Cleaned dataset saved to `{silver_table}`.\")\n",
    "\n",
    "# ----------------------------\n",
    "# 8. FINAL SUMMARY REPORT\n",
    "# ----------------------------\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"              üéâ ETL PIPELINE COMPLETED SUCCESSFULLY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Ingestion Date       : {ingestion_date}\")\n",
    "print(f\"Bronze Tables        : {bronze_air_table}, {bronze_weather_table}\")\n",
    "print(f\"Silver Table         : {silver_table}\")\n",
    "print(f\"Final Record Count   : {final_row_count}\")\n",
    "print(f\"Duplicate Timestamps : {duplicate_count}\")\n",
    "print(f\"Total Null Values    : {total_nulls}\")\n",
    "print(\"üí° Data is now ready for analysis in Delta Lake.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "234f7bfb-64eb-471f-99c0-90ee15cd94f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "QWEN need 2 fixes to work"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Qwen3-235B-A22B-2507",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
