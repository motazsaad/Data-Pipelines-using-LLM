{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b3bd6e9f-91dc-4f5f-a5bb-469e06bcf997",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Task**\n",
    "Create a Databricks Python code that extracts, processes, and loads data from two separate Open-Meteo APIs. The final output must be a clean, merged dataset saved to a Delta table using serverless compute.\n",
    "\n",
    "---\n",
    "\n",
    "**Requirements**\n",
    "\n",
    "### 1. Data Extraction\n",
    "* Fetch hourly air quality data from: `https://air-quality-api.open-meteo.com/v1/air-quality?latitude=40.3548&longitude=18.1724&hourly=pm10,pm2_5,carbon_monoxide,carbon_dioxide,nitrogen_dioxide,sulphur_dioxide,ozone&past_days=31&forecast_days=1`\n",
    "* Fetch hourly weather data from: `https://api.open-meteo.com/v1/forecast?latitude=40.3548&longitude=18.1724&hourly=temperature_2m,relative_humidity_2m,dew_point_2m,apparent_temperature,precipitation_probability,rain,wind_speed_10m&past_days=31&forecast_days=1`\n",
    "* Use the `requests` library to get the JSON responses.\n",
    "\n",
    "### 2. Data Processing and Merging\n",
    "* Parse the hourly JSON data from both APIs into two separate PySpark DataFrames.\n",
    "* The data within the `hourly` object (e.g., `time` and pollutant lists) must be flattened into structured rows.\n",
    "* Add an `ingestion_date` column to each DataFrame.\n",
    "* Merge the two DataFrames based on the `time` column to create a single, unified DataFrame. Use an `inner` join.\n",
    "\n",
    "### 3. Data Quality Checks\n",
    "* Implement checks to ensure data integrity and quality on the merged DataFrame.\n",
    "* **Null Check**: Identify and report the count of missing values in all pollutant and weather columns.\n",
    "* **Duplicate Check**: Identify duplicate entries based on the `time` column. Keep only the first occurrence for each timestamp.\n",
    "* Create a summary report using `print` statements to display the findings of the quality checks (e.g., total nulls, duplicate counts).\n",
    "\n",
    "### 4. Layered Architecture (Bronze & Silver)\n",
    "* **Bronze Layer**: Save the raw, unmerged DataFrames (air quality and weather) to separate Delta tables named `air_quality_bronze` and `weather_bronze`. Partition these tables by `ingestion_date` and use `append` mode.\n",
    "* **Silver Layer**: Save the final, clean, and merged DataFrame (after passing all quality checks) to a Delta table named `air_quality_and_weather_silver`. Use `append` mode for this save as well.\n",
    "\n",
    "### 5. Code Structure\n",
    "* The code should be a single, complete Databricks Python notebook script.\n",
    "* Include all necessary imports (`requests`, `pyspark.sql.functions`, etc.).\n",
    "* Add clear comments to explain each step of the ETL process.\n",
    "* The script should be runnable on Databricks using **serverless compute**.\n",
    "\n",
    "**Final Output**\n",
    "A comprehensive Python script that performs a full ETL pipeline with data merging, quality monitoring, and a Bronze/Silver layer architecture.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "856960bc-481f-4b5e-89cf-8374e13269c6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Databricks Python notebook script: Open-Meteo Air Quality + Weather ETL (Bronze/Silver, Serverless-ready)\n",
    "# Imports\n",
    "import requests\n",
    "from pyspark.sql import functions as F, types as T\n",
    "\n",
    "# 1) Configuration\n",
    "\n",
    "AIR_QUALITY_URL = (\n",
    "    \"https://air-quality-api.open-meteo.com/v1/air-quality\"\n",
    "    \"?latitude=40.3548&longitude=18.1724\"\n",
    "    \"&hourly=pm10,pm2_5,carbon_monoxide,carbon_dioxide,nitrogen_dioxide,sulphur_dioxide,ozone\"\n",
    "    \"&past_days=31&forecast_days=1\"\n",
    ")\n",
    "\n",
    "WEATHER_URL = (\n",
    "    \"https://api.open-meteo.com/v1/forecast\"\n",
    "    \"?latitude=40.3548&longitude=18.1724\"\n",
    "    \"&hourly=temperature_2m,relative_humidity_2m,dew_point_2m,apparent_temperature,precipitation_probability,rain,wind_speed_10m\"\n",
    "    \"&past_days=31&forecast_days=1\"\n",
    ")\n",
    "\n",
    "# Target table names (saved in the current catalog/schema context)\n",
    "AIR_QUALITY_BRONZE_TBL = \"air_quality_bronze\"\n",
    "WEATHER_BRONZE_TBL = \"weather_bronze\"\n",
    "SILVER_TBL = \"air_quality_and_weather_silver\"\n",
    "\n",
    "# 2) Helper functions\n",
    "def fetch_json(url: str, timeout: int = 60) -> dict:\n",
    "    \"\"\"Fetch JSON from an HTTP endpoint with basic error handling.\"\"\"\n",
    "    resp = requests.get(url, timeout=timeout)\n",
    "    resp.raise_for_status()\n",
    "    return resp.json()\n",
    "\n",
    "def hourly_dict_to_rows(hourly: dict) -> list:\n",
    "    \"\"\"\n",
    "    Convert an Open-Meteo 'hourly' object (arrays) into a list of row dicts.\n",
    "    Safely aligns all arrays by the shortest length if mismatched.\n",
    "    \"\"\"\n",
    "    if not hourly or \"time\" not in hourly:\n",
    "        raise ValueError(\"Invalid hourly payload: missing 'time'.\")\n",
    "\n",
    "    # Compute lengths for all list-like fields\n",
    "    lengths = {k: len(v) for k, v in hourly.items() if isinstance(v, list)}\n",
    "    if not lengths:\n",
    "        raise ValueError(\"Invalid hourly payload: no list-like fields found.\")\n",
    "\n",
    "    # Truncate to shortest length if any mismatch occurs\n",
    "    min_len = min(lengths.values())\n",
    "    if len(set(lengths.values())) != 1:\n",
    "        print(f\"Warning: hourly arrays have different lengths {lengths}; truncating to {min_len} rows.\")\n",
    "\n",
    "    time_list = hourly[\"time\"][:min_len]\n",
    "    variable_keys = [k for k in hourly.keys() if k != \"time\" and isinstance(hourly[k], list)]\n",
    "\n",
    "    rows = []\n",
    "    for i in range(min_len):\n",
    "        row = {\"time\": time_list[i]}\n",
    "        for k in variable_keys:\n",
    "            row[k] = hourly[k][i] if i < len(hourly[k]) else None\n",
    "        rows.append(row)\n",
    "    return rows\n",
    "\n",
    "def hourly_to_spark_df(hourly: dict):\n",
    "    \"\"\"\n",
    "    Convert an Open-Meteo 'hourly' dict to a Spark DataFrame:\n",
    "    - Flattens arrays into rows\n",
    "    - Casts 'time' to timestamp\n",
    "    - Casts numeric fields to DoubleType\n",
    "    - Adds ingestion_date (DateType)\n",
    "    \"\"\"\n",
    "    rows = hourly_dict_to_rows(hourly)\n",
    "    df = spark.createDataFrame(rows)\n",
    "\n",
    "    # Cast 'time' to timestamp\n",
    "    df = df.withColumn(\"time\", F.to_timestamp(\"time\"))\n",
    "\n",
    "    # Cast all non-time fields to Double (safe for ints/floats)\n",
    "    numeric_cols = [c for c in df.columns if c != \"time\"]\n",
    "    for c in numeric_cols:\n",
    "        df = df.withColumn(c, F.col(c).cast(T.DoubleType()))\n",
    "\n",
    "    # Add ingestion_date (serverless-ready; evaluated per row)\n",
    "    df = df.withColumn(\"ingestion_date\", F.current_date())\n",
    "\n",
    "    return df\n",
    "\n",
    "def print_null_summary(df, exclude_cols=None, title=\"Null Check Summary\"):\n",
    "    \"\"\"Print null counts for all columns except those excluded.\"\"\"\n",
    "    exclude_cols = set(exclude_cols or [])\n",
    "    cols = [c for c in df.columns if c not in exclude_cols]\n",
    "    if not cols:\n",
    "        print(f\"{title}: No columns to evaluate.\")\n",
    "        return\n",
    "    agg_exprs = [F.sum(F.col(c).isNull().cast(\"int\")).alias(c) for c in cols]\n",
    "    row = df.select(agg_exprs).collect()[0]\n",
    "    total_nulls = 0\n",
    "    print(title)\n",
    "    for c in cols:\n",
    "        count_c = row[c] if row[c] is not None else 0\n",
    "        print(f\" {c}: {count_c}\")\n",
    "        total_nulls += count_c\n",
    "    print(f\"Total nulls across evaluated columns: {total_nulls}\")\n",
    "\n",
    "# 3) Data Extraction\n",
    "print(\"Starting extraction from Open-Meteo APIs...\")\n",
    "air_quality_json = fetch_json(AIR_QUALITY_URL)\n",
    "weather_json = fetch_json(WEATHER_URL)\n",
    "print(\"Extraction completed.\")\n",
    "\n",
    "# 4) Data Processing (Flatten hourly arrays into structured rows)\n",
    "print(\"Parsing and flattening hourly payloads into Spark DataFrames...\")\n",
    "air_quality_df = hourly_to_spark_df(air_quality_json.get(\"hourly\", {}))\n",
    "weather_df = hourly_to_spark_df(weather_json.get(\"hourly\", {}))\n",
    "\n",
    "# Basic sanity checks\n",
    "print(f\"Air Quality rows: {air_quality_df.count()}, columns: {len(air_quality_df.columns)}\")\n",
    "print(f\"Weather rows: {weather_df.count()}, columns: {len(weather_df.columns)}\")\n",
    "\n",
    "# 5) Bronze Layer: Save raw, unmerged DataFrames to Delta (partitioned by ingestion_date)\n",
    "print(\"Writing Bronze tables (Delta, partitioned by ingestion_date)...\")\n",
    "(\n",
    "    air_quality_df.write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"append\")\n",
    "    .partitionBy(\"ingestion_date\")\n",
    "    .saveAsTable(AIR_QUALITY_BRONZE_TBL)\n",
    ")\n",
    "(\n",
    "    weather_df.write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"append\")\n",
    "    .partitionBy(\"ingestion_date\")\n",
    "    .saveAsTable(WEATHER_BRONZE_TBL)\n",
    ")\n",
    "print(f\"Bronze writes complete: {AIR_QUALITY_BRONZE_TBL}, {WEATHER_BRONZE_TBL}\")\n",
    "\n",
    "# 6) Merge DataFrames on 'time' (inner join)\n",
    "print(\"Merging Air Quality and Weather data on 'time' (inner join)...\")\n",
    "weather_df_renamed = weather_df.withColumnRenamed(\"ingestion_date\", \"ingestion_date_weather\")\n",
    "\n",
    "merged_df = (\n",
    "    air_quality_df\n",
    "    .join(weather_df_renamed, on=\"time\", how=\"inner\")\n",
    "    # Keep the primary ingestion_date from air quality and drop the secondary\n",
    "    .drop(\"ingestion_date_weather\")\n",
    ")\n",
    "\n",
    "print(f\"Merged rows: {merged_df.count()}, columns: {len(merged_df.columns)}\")\n",
    "\n",
    "# 7) Data Quality Checks on merged DataFrame\n",
    "print(\"Running Data Quality Checks...\")\n",
    "\n",
    "# Null Check (for pollutants and weather columns; exclude non-measurement fields)\n",
    "exclude_for_nulls = {\"time\", \"ingestion_date\"}\n",
    "print_null_summary(merged_df, exclude_cols=exclude_for_nulls, title=\"Null Check: Merged DataFrame\")\n",
    "\n",
    "# Duplicate Check on 'time'\n",
    "merged_count_before = merged_df.count()\n",
    "deduped_df = merged_df.dropDuplicates([\"time\"])\n",
    "merged_count_after = deduped_df.count()\n",
    "duplicate_count = merged_count_before - merged_count_after\n",
    "\n",
    "print(\"Duplicate Check: Merged DataFrame\")\n",
    "print(f\" Rows before dedup: {merged_count_before}\")\n",
    "print(f\" Rows after dedup: {merged_count_after}\")\n",
    "print(f\" Duplicate rows removed (based on time): {duplicate_count}\")\n",
    "\n",
    "# 8) Silver Layer: Save the clean, merged DataFrame to Delta (append)\n",
    "print(\"Writing Silver table (clean, deduplicated merged data) to Delta...\")\n",
    "(\n",
    "    deduped_df.write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"append\")\n",
    "    .saveAsTable(SILVER_TBL)\n",
    ")\n",
    "print(f\"Silver write complete: {SILVER_TBL}\")\n",
    "\n",
    "print(\"ETL pipeline finished successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3a926613-90cc-4e5e-b5c2-36bd626c5e38",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "GPT 5 nano need 1 fix to work"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "GPT 5 nano",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
