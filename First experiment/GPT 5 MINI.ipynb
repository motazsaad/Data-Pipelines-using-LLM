{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1095aa95-d6e3-44b5-a0fd-478327be8794",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Task**\n",
    "--------\n",
    "\n",
    "Create a Databricks python code that extracts air quality data from an API, processes it with quality checks, and saves clean data to Delta tables using **serverless compute** (student version compatible).\n",
    "\n",
    "**Requirements**\n",
    "----------------\n",
    "\n",
    "### **1\\. Extract Data**\n",
    "\n",
    "*   Fetch air quality data from: https://air-quality-api.open-meteo.com/v1/air-quality?latitude=40.3548&longitude=18.1724&hourly=pm10,pm2\\_5,carbon\\_monoxide,carbon\\_dioxide,nitrogen\\_dioxide,sulphur\\_dioxide,ozone&past\\_days=31&forecast\\_days=1\n",
    "    \n",
    "*   Use Python requests to get the JSON response\n",
    "    \n",
    "\n",
    "### **2\\. Transform and Load**\n",
    "\n",
    "*   Parse the JSON hourly data into a PySpark DataFrame\n",
    "    \n",
    "*   Convert time and pollutant lists into structured rows\n",
    "    \n",
    "*   Add an ingestion\\_date column\n",
    "    \n",
    "*   Save to Delta table air\\_quality\\_bronze (partitioned by ingestion\\_date, append mode)\n",
    "    \n",
    "\n",
    "### **3\\. Data Quality Checks**\n",
    "\n",
    "*   **Null Check**: Find missing values in pollutant columns, report counts and affected rows\n",
    "    \n",
    "*   **Duplicate Check**: Identify duplicate timestamps, keep only first occurrence\n",
    "    \n",
    "*   Generate quality report for any issues found\n",
    "    \n",
    "\n",
    "### **4\\. Final Save**\n",
    "\n",
    "*   Create summary of data quality issues using print statements\n",
    "    \n",
    "*   Save only clean rows (passed all checks) to Delta table air\\_quality\\_silver (append mode)\n",
    "    \n",
    "\n",
    "**Output**\n",
    "----------\n",
    "\n",
    "A complete Databricks python code that implements the full ETL pipeline with data quality monitoring and Bronze/Silver layer architecture.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "203d2f51-6f07-4851-93a8-aaee729e8364",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Databricks Python notebook / script\n",
    "# ETL: Open-Meteo Air Quality API -> Bronze/Silver Delta tables with data quality checks\n",
    "# - Bronze: air_quality_bronze partitioned by ingestion_date (append)\n",
    "# - Silver: air_quality_silver partitioned by ingestion_date (append)\n",
    "# - Serverless compute friendly (Databricks student/serverless environment)\n",
    "\n",
    "import requests\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# 1) Extract Data\n",
    "def fetch_air_quality_json(api_url, timeout=60):\n",
    "    \"\"\"Fetch JSON payload from API using requests.\"\"\"\n",
    "    resp = requests.get(api_url, timeout=timeout)\n",
    "    resp.raise_for_status()\n",
    "    return resp.json()\n",
    "\n",
    "def api_to_rows(api_json):\n",
    "    \"\"\"\n",
    "    Transform the API's hourly section into a list of dict rows.\n",
    "    API structure:\n",
    "      hourly: {\n",
    "        time: [...],\n",
    "        pm10: [...],\n",
    "        pm2_5: [...],\n",
    "        carbon_monoxide: [...],\n",
    "        carbon_dioxide: [...],\n",
    "        nitrogen_dioxide: [...],\n",
    "        sulphur_dioxide: [...],\n",
    "        ozone: [...]\n",
    "      }\n",
    "    \"\"\"\n",
    "    hourly = api_json.get(\"hourly\", {})\n",
    "    times = hourly.get(\"time\", [])\n",
    "    pollutant_keys = [\n",
    "        \"pm10\",\n",
    "        \"pm2_5\",\n",
    "        \"carbon_monoxide\",\n",
    "        \"carbon_dioxide\",\n",
    "        \"nitrogen_dioxide\",\n",
    "        \"sulphur_dioxide\",\n",
    "        \"ozone\"\n",
    "    ]\n",
    "\n",
    "    pollutant_lists = {k: hourly.get(k, []) for k in pollutant_keys}\n",
    "    n = len(times)\n",
    "\n",
    "    # Normalize all pollutant lists to length n\n",
    "    for k in pollutant_keys:\n",
    "        lst = pollutant_lists.get(k, [])\n",
    "        if len(lst) < n:\n",
    "            lst = lst + [None] * (n - len(lst))\n",
    "        else:\n",
    "            lst = lst[:n]\n",
    "        pollutant_lists[k] = lst\n",
    "\n",
    "    # Build rows\n",
    "    rows = []\n",
    "    for i in range(n):\n",
    "        row = {\"time\": times[i]}\n",
    "        for k in pollutant_keys:\n",
    "            row[k] = pollutant_lists[k][i]\n",
    "        rows.append(row)\n",
    "    return rows\n",
    "\n",
    "# 2) Transform and Load\n",
    "def main_etl():\n",
    "    # API URL (as provided)\n",
    "    api_url = (\n",
    "        \"https://air-quality-api.open-meteo.com/v1/air-quality\"\n",
    "        \"?latitude=40.3548&longitude=18.1724\"\n",
    "        \"&hourly=pm10,pm2_5,carbon_monoxide,carbon_dioxide,nitrogen_dioxide,sulphur_dioxide,ozone\"\n",
    "        \"&past_days=31&forecast_days=1\"\n",
    "    )\n",
    "\n",
    "    # 1) Extract\n",
    "    try:\n",
    "        api_json = fetch_air_quality_json(api_url, timeout=60)\n",
    "        rows = api_to_rows(api_json)\n",
    "        if not rows:\n",
    "            print(\"No data retrieved from API or empty payload.\")\n",
    "            return\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR during API fetch/parse: {e}\")\n",
    "        raise\n",
    "\n",
    "    # 2) Create DataFrame and basic normalization\n",
    "    df = spark.createDataFrame(rows)\n",
    "    df = df.withColumn(\"time\", F.to_timestamp(F.col(\"time\")))\n",
    "    df = df.withColumn(\"ingestion_date\", F.current_date())  # partitioning key\n",
    "\n",
    "    pollutant_cols = [\n",
    "        \"pm10\",\n",
    "        \"pm2_5\",\n",
    "        \"carbon_monoxide\",\n",
    "        \"carbon_dioxide\",\n",
    "        \"nitrogen_dioxide\",\n",
    "        \"sulphur_dioxide\",\n",
    "        \"ozone\"\n",
    "    ]\n",
    "\n",
    "    # Bronze: save all data (append), partitioned by ingestion_date\n",
    "    try:\n",
    "        df.write \\\n",
    "          .mode(\"append\") \\\n",
    "          .partitionBy(\"ingestion_date\") \\\n",
    "          .format(\"delta\") \\\n",
    "          .saveAsTable(\"air_quality_bronze\")\n",
    "        print(\"Bronze load complete: air_quality_bronze\")\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR writing Bronze table: {e}\")\n",
    "        raise\n",
    "\n",
    "    # 3) Data Quality Checks (on Bronze data)\n",
    "    total_rows = df.count()\n",
    "\n",
    "    # Null counts per pollutant\n",
    "    null_counts = {c: df.filter(F.col(c).isNull()).count() for c in pollutant_cols}\n",
    "\n",
    "    # Rows affected by any null in pollutant columns\n",
    "    any_null_condition = None\n",
    "    for c in pollutant_cols:\n",
    "        cond = F.col(c).isNull()\n",
    "        any_null_condition = cond if any_null_condition is None else (any_null_condition | cond)\n",
    "    affected_rows_with_nulls = df.filter(any_null_condition).count() if any_null_condition is not None else 0\n",
    "\n",
    "    # Duplicate timestamps (time)\n",
    "    duplicates_per_time = df.groupBy(\"time\").count().filter(F.col(\"count\") > 1).count()\n",
    "\n",
    "    # Quality report (print statements)\n",
    "    print(\"DATA QUALITY REPORT (Bronze)\")\n",
    "    print(f\"Total rows loaded: {total_rows}\")\n",
    "    print(\"Null value counts per pollutant:\")\n",
    "    for c in pollutant_cols:\n",
    "        print(f\" - {c}: {null_counts[c]}\")\n",
    "    print(f\"Rows affected by any null in pollutant columns: {affected_rows_with_nulls}\")\n",
    "    print(f\"Duplicate timestamps detected: {duplicates_per_time}\")\n",
    "\n",
    "    # 4) Final Save (Silver) - only clean rows\n",
    "    # Build non-null condition for all pollutant columns\n",
    "    non_null_expr = None\n",
    "    for c in pollutant_cols:\n",
    "        expr = F.col(c).isNotNull()\n",
    "        non_null_expr = expr if non_null_expr is None else (non_null_expr & expr)\n",
    "\n",
    "    clean_df = df.filter(non_null_expr)\n",
    "\n",
    "    # Remove duplicates by time (keep first occurrence)\n",
    "    w = Window.partitionBy(\"time\").orderBy(F.col(\"time\").asc())\n",
    "    df_with_rn = clean_df.withColumn(\"rn\", F.row_number().over(w))\n",
    "    silver_df = df_with_rn.filter(F.col(\"rn\") == 1).drop(\"rn\")\n",
    "\n",
    "    # Silver: append, partitioned by ingestion_date\n",
    "    try:\n",
    "        silver_df.write \\\n",
    "            .mode(\"append\") \\\n",
    "            .partitionBy(\"ingestion_date\") \\\n",
    "            .format(\"delta\") \\\n",
    "            .saveAsTable(\"air_quality_silver\")\n",
    "        print(\"Silver load complete: air_quality_silver\")\n",
    "        print(f\"Silver rows written: {silver_df.count()}\")\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR writing Silver table: {e}\")\n",
    "        raise\n",
    "\n",
    "# Execute ETL (works well in Databricks notebooks)\n",
    "main_etl()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4619eb83-ea70-4561-ba80-ef9ed2991e45",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "GPT 5 mini work at the first attempt"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "GPT 5 MINI",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
