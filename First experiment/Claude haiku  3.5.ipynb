{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ec92ab6e-8a2b-42aa-b7d2-f4072c7e6b64",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Task**\n",
    "--------\n",
    "\n",
    "Create a Databricks python code that extracts air quality data from an API, processes it with quality checks, and saves clean data to Delta tables using **serverless compute** (student version compatible).\n",
    "\n",
    "**Requirements**\n",
    "----------------\n",
    "\n",
    "### **1\\. Extract Data**\n",
    "\n",
    "*   Fetch air quality data from: https://air-quality-api.open-meteo.com/v1/air-quality?latitude=40.3548&longitude=18.1724&hourly=pm10,pm2\\_5,carbon\\_monoxide,carbon\\_dioxide,nitrogen\\_dioxide,sulphur\\_dioxide,ozone&past\\_days=31&forecast\\_days=1\n",
    "    \n",
    "*   Use Python requests to get the JSON response\n",
    "    \n",
    "\n",
    "### **2\\. Transform and Load**\n",
    "\n",
    "*   Parse the JSON hourly data into a PySpark DataFrame\n",
    "    \n",
    "*   Convert time and pollutant lists into structured rows\n",
    "    \n",
    "*   Add an ingestion\\_date column\n",
    "    \n",
    "*   Save to Delta table air\\_quality\\_bronze (partitioned by ingestion\\_date, append mode)\n",
    "    \n",
    "\n",
    "### **3\\. Data Quality Checks**\n",
    "\n",
    "*   **Null Check**: Find missing values in pollutant columns, report counts and affected rows\n",
    "    \n",
    "*   **Duplicate Check**: Identify duplicate timestamps, keep only first occurrence\n",
    "    \n",
    "*   Generate quality report for any issues found\n",
    "    \n",
    "\n",
    "### **4\\. Final Save**\n",
    "\n",
    "*   Create summary of data quality issues using print statements\n",
    "    \n",
    "*   Save only clean rows (passed all checks) to Delta table air\\_quality\\_silver (append mode)\n",
    "    \n",
    "\n",
    "**Output**\n",
    "----------\n",
    "\n",
    "A complete Databricks python code that implements the full ETL pipeline with data quality monitoring and Bronze/Silver layer architecture.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d4e1d01-12ac-4354-b96b-ac154bebe97f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Databricks Notebook: Air Quality ETL Pipeline\n",
    "\n",
    "# Import required libraries\n",
    "import requests\n",
    "import json\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, TimestampType\n",
    "from datetime import datetime, timezone, timedelta\n",
    "import pytz\n",
    "\n",
    "# 1. Extract Data Function\n",
    "def fetch_air_quality_data():\n",
    "    \"\"\"\n",
    "    Fetch air quality data from Open Meteo API\n",
    "    \"\"\"\n",
    "    url = \"https://air-quality-api.open-meteo.com/v1/air-quality\"\n",
    "    params = {\n",
    "        \"latitude\": 40.3548,\n",
    "        \"longitude\": 18.1724,\n",
    "        \"hourly\": \"pm10,pm2_5,carbon_monoxide,carbon_dioxide,nitrogen_dioxide,sulphur_dioxide,ozone\",\n",
    "        \"past_days\": 31,\n",
    "        \"forecast_days\": 1\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, params=params)\n",
    "        response.raise_for_status()  # Raise an exception for bad responses\n",
    "        return response.json()\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Error fetching data: {e}\")\n",
    "        return None\n",
    "\n",
    "# 2. Transform Data Function\n",
    "def transform_air_quality_data(air_quality_json):\n",
    "    \"\"\"\n",
    "    Transform JSON data into a PySpark DataFrame\n",
    "    \"\"\"\n",
    "    # Define schema for the DataFrame\n",
    "    schema = StructType([\n",
    "        StructField(\"timestamp\", TimestampType(), True),\n",
    "        StructField(\"pm10\", DoubleType(), True),\n",
    "        StructField(\"pm2_5\", DoubleType(), True),\n",
    "        StructField(\"carbon_monoxide\", DoubleType(), True),\n",
    "        StructField(\"carbon_dioxide\", DoubleType(), True),\n",
    "        StructField(\"nitrogen_dioxide\", DoubleType(), True),\n",
    "        StructField(\"sulphur_dioxide\", DoubleType(), True),\n",
    "        StructField(\"ozone\", DoubleType(), True),\n",
    "        StructField(\"ingestion_date\", TimestampType(), True)\n",
    "    ])\n",
    "    \n",
    "    # Prepare data for DataFrame\n",
    "    ingestion_date = datetime.now(timezone.utc)\n",
    "    rows = []\n",
    "    \n",
    "    # Extract hourly data\n",
    "    timestamps = air_quality_json['hourly']['time']\n",
    "    pollutants = {\n",
    "        'pm10': air_quality_json['hourly']['pm10'],\n",
    "        'pm2_5': air_quality_json['hourly']['pm2_5'],\n",
    "        'carbon_monoxide': air_quality_json['hourly']['carbon_monoxide'],\n",
    "        'carbon_dioxide': air_quality_json['hourly']['carbon_dioxide'],\n",
    "        'nitrogen_dioxide': air_quality_json['hourly']['nitrogen_dioxide'],\n",
    "        'sulphur_dioxide': air_quality_json['hourly']['sulphur_dioxide'],\n",
    "        'ozone': air_quality_json['hourly']['ozone']\n",
    "    }\n",
    "    \n",
    "    # Create rows with proper timestamp conversion\n",
    "    for i in range(len(timestamps)):\n",
    "        try:\n",
    "            # Convert timestamp to UTC datetime\n",
    "            ts = datetime.fromisoformat(timestamps[i]).replace(tzinfo=timezone.utc)\n",
    "            \n",
    "            row = {\n",
    "                'timestamp': ts,\n",
    "                'pm10': float(pollutants['pm10'][i]) if pollutants['pm10'][i] is not None else None,\n",
    "                'pm2_5': float(pollutants['pm2_5'][i]) if pollutants['pm2_5'][i] is not None else None,\n",
    "                'carbon_monoxide': float(pollutants['carbon_monoxide'][i]) if pollutants['carbon_monoxide'][i] is not None else None,\n",
    "                'carbon_dioxide': float(pollutants['carbon_dioxide'][i]) if pollutants['carbon_dioxide'][i] is not None else None,\n",
    "                'nitrogen_dioxide': float(pollutants['nitrogen_dioxide'][i]) if pollutants['nitrogen_dioxide'][i] is not None else None,\n",
    "                'sulphur_dioxide': float(pollutants['sulphur_dioxide'][i]) if pollutants['sulphur_dioxide'][i] is not None else None,\n",
    "                'ozone': float(pollutants['ozone'][i]) if pollutants['ozone'][i] is not None else None,\n",
    "                'ingestion_date': ingestion_date\n",
    "            }\n",
    "            rows.append(row)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing timestamp {timestamps[i]}: {e}\")\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = spark.createDataFrame(rows, schema=schema)\n",
    "    return df\n",
    "\n",
    "# 3. Data Quality Checks Function\n",
    "def perform_data_quality_checks(df):\n",
    "    \"\"\"\n",
    "    Perform data quality checks and generate report\n",
    "    \"\"\"\n",
    "    # Null Check\n",
    "    null_counts = {}\n",
    "    pollutant_columns = [\n",
    "        'pm10', 'pm2_5', 'carbon_monoxide', 'carbon_dioxide', \n",
    "        'nitrogen_dioxide', 'sulphur_dioxide', 'ozone'\n",
    "    ]\n",
    "    \n",
    "    for col in pollutant_columns:\n",
    "        null_count = df.filter(F.col(col).isNull()).count()\n",
    "        if null_count > 0:\n",
    "            null_counts[col] = null_count\n",
    "    \n",
    "    # Duplicate Check\n",
    "    total_rows = df.count()\n",
    "    distinct_rows = df.dropDuplicates(['timestamp']).count()\n",
    "    duplicate_count = total_rows - distinct_rows\n",
    "    \n",
    "    # Generate Quality Report\n",
    "    print(\"Data Quality Report:\")\n",
    "    print(\"-\" * 20)\n",
    "    \n",
    "    if null_counts:\n",
    "        print(\"Null Value Counts:\")\n",
    "        for col, count in null_counts.items():\n",
    "            print(f\"{col}: {count} null values\")\n",
    "    else:\n",
    "        print(\"No null values found in pollutant columns.\")\n",
    "    \n",
    "    print(f\"\\nDuplicate Timestamps: {duplicate_count}\")\n",
    "    \n",
    "    # Remove duplicates and null rows\n",
    "    clean_df = df.dropDuplicates(['timestamp']).na.drop(subset=pollutant_columns)\n",
    "    \n",
    "    return clean_df, null_counts, duplicate_count\n",
    "\n",
    "# Main ETL Pipeline\n",
    "def air_quality_etl():\n",
    "    # 1. Extract Data\n",
    "    air_quality_json = fetch_air_quality_data()\n",
    "    \n",
    "    if not air_quality_json:\n",
    "        print(\"Failed to fetch air quality data.\")\n",
    "        return\n",
    "    \n",
    "    # 2. Transform Data\n",
    "    try:\n",
    "        bronze_df = transform_air_quality_data(air_quality_json)\n",
    "        \n",
    "        # Save Bronze Layer\n",
    "        bronze_df.write \\\n",
    "            .format(\"delta\") \\\n",
    "            .mode(\"append\") \\\n",
    "            .partitionBy(\"ingestion_date\") \\\n",
    "            .saveAsTable(\"air_quality_bronze\")\n",
    "        \n",
    "        # 3. Data Quality Checks\n",
    "        silver_df, null_issues, duplicate_count = perform_data_quality_checks(bronze_df)\n",
    "        \n",
    "        # 4. Save Silver Layer\n",
    "        silver_df.write \\\n",
    "            .format(\"delta\") \\\n",
    "            .mode(\"append\") \\\n",
    "            .saveAsTable(\"air_quality_silver\")\n",
    "        \n",
    "        print(\"\\nETL Process Completed Successfully!\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error in ETL process: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "# Execute the ETL Pipeline\n",
    "try:\n",
    "    air_quality_etl()\n",
    "except Exception as e:\n",
    "    print(f\"Unexpected error: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0eaa8df0-3391-4fb2-a93c-39339cb939f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Claude Haiku 3.5 work at first attempt\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Claude haiku  3.5",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
