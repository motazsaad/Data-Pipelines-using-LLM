{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bb100097-24dd-40d2-bfe4-7bb679a1cf3d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Task**\n",
    "--------\n",
    "\n",
    "Create a Databricks python code that extracts air quality data from an API, processes it with quality checks, and saves clean data to Delta tables using **serverless compute** (student version compatible).\n",
    "\n",
    "**Requirements**\n",
    "----------------\n",
    "\n",
    "### **1\\. Extract Data**\n",
    "\n",
    "*   Fetch air quality data from: https://air-quality-api.open-meteo.com/v1/air-quality?latitude=40.3548&longitude=18.1724&hourly=pm10,pm2\\_5,carbon\\_monoxide,carbon\\_dioxide,nitrogen\\_dioxide,sulphur\\_dioxide,ozone&past\\_days=31&forecast\\_days=1\n",
    "    \n",
    "*   Use Python requests to get the JSON response\n",
    "    \n",
    "\n",
    "### **2\\. Transform and Load**\n",
    "\n",
    "*   Parse the JSON hourly data into a PySpark DataFrame\n",
    "    \n",
    "*   Convert time and pollutant lists into structured rows\n",
    "    \n",
    "*   Add an ingestion\\_date column\n",
    "    \n",
    "*   Save to Delta table air\\_quality\\_bronze (partitioned by ingestion\\_date, append mode)\n",
    "    \n",
    "\n",
    "### **3\\. Data Quality Checks**\n",
    "\n",
    "*   **Null Check**: Find missing values in pollutant columns, report counts and affected rows\n",
    "    \n",
    "*   **Duplicate Check**: Identify duplicate timestamps, keep only first occurrence\n",
    "    \n",
    "*   Generate quality report for any issues found\n",
    "    \n",
    "\n",
    "### **4\\. Final Save**\n",
    "\n",
    "*   Create summary of data quality issues using print statements\n",
    "    \n",
    "*   Save only clean rows (passed all checks) to Delta table air\\_quality\\_silver (append mode)\n",
    "    \n",
    "\n",
    "**Output**\n",
    "----------\n",
    "\n",
    "A complete Databricks python code that implements the full ETL pipeline with data quality monitoring and Bronze/Silver layer architecture.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "735800f6-796a-4c55-b26b-fcefdff9d9db",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Databricks notebook source\n",
    "# MAGIC %md\n",
    "# MAGIC # Air Quality Data ETL Pipeline\n",
    "# MAGIC This notebook extracts air quality data from Open-Meteo API, performs quality checks, and saves to Delta tables\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Import required libraries\n",
    "import requests\n",
    "import json\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## 1. Extract Data from API\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Define API endpoint and parameters\n",
    "api_url = \"https://air-quality-api.open-meteo.com/v1/air-quality\"\n",
    "params = {\n",
    "    \"latitude\": 40.3548,\n",
    "    \"longitude\": 18.1724,\n",
    "    \"hourly\": \"pm10,pm2_5,carbon_monoxide,carbon_dioxide,nitrogen_dioxide,sulphur_dioxide,ozone\",\n",
    "    \"past_days\": 31,\n",
    "    \"forecast_days\": 1\n",
    "}\n",
    "\n",
    "# Fetch data from API\n",
    "try:\n",
    "    response = requests.get(api_url, params=params)\n",
    "    response.raise_for_status()\n",
    "    data = response.json()\n",
    "    print(\"‚úÖ Successfully fetched data from API\")\n",
    "    print(f\"API Response Status: {response.status_code}\")\n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(f\"‚ùå Error fetching data: {e}\")\n",
    "    raise\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## 2. Transform and Load to Bronze Layer\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Parse JSON data and create structured DataFrame\n",
    "def parse_air_quality_data(json_data):\n",
    "    \"\"\"Parse JSON response and create structured rows\"\"\"\n",
    "    hourly_data = json_data.get('hourly', {})\n",
    "    \n",
    "    # Extract time and pollutant data\n",
    "    times = hourly_data.get('time', [])\n",
    "    pm10 = hourly_data.get('pm10', [])\n",
    "    pm2_5 = hourly_data.get('pm2_5', [])\n",
    "    carbon_monoxide = hourly_data.get('carbon_monoxide', [])\n",
    "    carbon_dioxide = hourly_data.get('carbon_dioxide', [])\n",
    "    nitrogen_dioxide = hourly_data.get('nitrogen_dioxide', [])\n",
    "    sulphur_dioxide = hourly_data.get('sulphur_dioxide', [])\n",
    "    ozone = hourly_data.get('ozone', [])\n",
    "    \n",
    "    # Create list of rows\n",
    "    rows = []\n",
    "    for i in range(len(times)):\n",
    "        row = {\n",
    "            'timestamp': times[i] if i < len(times) else None,\n",
    "            'pm10': pm10[i] if i < len(pm10) else None,\n",
    "            'pm2_5': pm2_5[i] if i < len(pm2_5) else None,\n",
    "            'carbon_monoxide': carbon_monoxide[i] if i < len(carbon_monoxide) else None,\n",
    "            'carbon_dioxide': carbon_dioxide[i] if i < len(carbon_dioxide) else None,\n",
    "            'nitrogen_dioxide': nitrogen_dioxide[i] if i < len(nitrogen_dioxide) else None,\n",
    "            'sulphur_dioxide': sulphur_dioxide[i] if i < len(sulphur_dioxide) else None,\n",
    "            'ozone': ozone[i] if i < len(ozone) else None,\n",
    "            'latitude': json_data.get('latitude'),\n",
    "            'longitude': json_data.get('longitude')\n",
    "        }\n",
    "        rows.append(row)\n",
    "    \n",
    "    return rows\n",
    "\n",
    "# Parse the data\n",
    "parsed_rows = parse_air_quality_data(data)\n",
    "print(f\"‚úÖ Parsed {len(parsed_rows)} rows of air quality data\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Create PySpark DataFrame\n",
    "schema = StructType([\n",
    "    StructField(\"timestamp\", StringType(), True),\n",
    "    StructField(\"pm10\", DoubleType(), True),\n",
    "    StructField(\"pm2_5\", DoubleType(), True),\n",
    "    StructField(\"carbon_monoxide\", DoubleType(), True),\n",
    "    StructField(\"carbon_dioxide\", DoubleType(), True),\n",
    "    StructField(\"nitrogen_dioxide\", DoubleType(), True),\n",
    "    StructField(\"sulphur_dioxide\", DoubleType(), True),\n",
    "    StructField(\"ozone\", DoubleType(), True),\n",
    "    StructField(\"latitude\", DoubleType(), True),\n",
    "    StructField(\"longitude\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "# Create DataFrame\n",
    "df_bronze = spark.createDataFrame(parsed_rows, schema)\n",
    "\n",
    "# Add ingestion_date column\n",
    "df_bronze = df_bronze.withColumn(\"ingestion_date\", current_date())\n",
    "\n",
    "# Convert timestamp to proper datetime format\n",
    "df_bronze = df_bronze.withColumn(\"timestamp\", to_timestamp(col(\"timestamp\")))\n",
    "\n",
    "# Show sample data\n",
    "print(\"Sample of Bronze data:\")\n",
    "df_bronze.show(5, truncate=False)\n",
    "print(f\"Total rows in Bronze: {df_bronze.count()}\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Save to Bronze Delta table using DBFS path\n",
    "# For Databricks Community Edition, we'll use the default database\n",
    "bronze_table_name = \"default.air_quality_bronze\"\n",
    "\n",
    "try:\n",
    "    # Write to Delta table with partitioning\n",
    "    df_bronze.write \\\n",
    "        .mode(\"append\") \\\n",
    "        .partitionBy(\"ingestion_date\") \\\n",
    "        .format(\"delta\") \\\n",
    "        .saveAsTable(bronze_table_name)\n",
    "    \n",
    "    print(f\"‚úÖ Successfully saved {df_bronze.count()} rows to Bronze layer\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error saving to Bronze layer: {e}\")\n",
    "    # If table already exists, try to append\n",
    "    try:\n",
    "        df_bronze.write \\\n",
    "            .mode(\"append\") \\\n",
    "            .format(\"delta\") \\\n",
    "            .insertInto(bronze_table_name)\n",
    "        print(f\"‚úÖ Successfully appended {df_bronze.count()} rows to existing Bronze table\")\n",
    "    except:\n",
    "        raise\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## 3. Data Quality Checks\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Define pollutant columns for quality checks\n",
    "pollutant_columns = [\n",
    "    \"pm10\", \"pm2_5\", \"carbon_monoxide\", \"carbon_dioxide\", \n",
    "    \"nitrogen_dioxide\", \"sulphur_dioxide\", \"ozone\"\n",
    "]\n",
    "\n",
    "# Initialize quality report\n",
    "quality_report = {\n",
    "    \"total_rows\": df_bronze.count(),\n",
    "    \"null_checks\": {},\n",
    "    \"duplicate_checks\": {},\n",
    "    \"issues_found\": False\n",
    "}\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ### 3.1 Null Check\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Perform null checks\n",
    "print(\"=\" * 50)\n",
    "print(\"NULL VALUE CHECK\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "null_summary = []\n",
    "for col_name in pollutant_columns:\n",
    "    null_count = df_bronze.filter(col(col_name).isNull()).count()\n",
    "    null_percentage = (null_count / quality_report[\"total_rows\"]) * 100\n",
    "    \n",
    "    quality_report[\"null_checks\"][col_name] = {\n",
    "        \"null_count\": null_count,\n",
    "        \"null_percentage\": null_percentage\n",
    "    }\n",
    "    \n",
    "    if null_count > 0:\n",
    "        quality_report[\"issues_found\"] = True\n",
    "        print(f\"‚ö†Ô∏è  {col_name}: {null_count} null values ({null_percentage:.2f}%)\")\n",
    "        null_summary.append(f\"{col_name}: {null_count} nulls\")\n",
    "    else:\n",
    "        print(f\"‚úÖ {col_name}: No null values\")\n",
    "\n",
    "# Show sample rows with nulls\n",
    "if quality_report[\"issues_found\"]:\n",
    "    print(\"\\nSample rows with null values:\")\n",
    "    null_condition = \" OR \".join([f\"{col} IS NULL\" for col in pollutant_columns])\n",
    "    df_bronze.filter(null_condition).select(\"timestamp\", *pollutant_columns).show(5, truncate=False)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ### 3.2 Duplicate Check\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Check for duplicate timestamps\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"DUPLICATE TIMESTAMP CHECK\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Count duplicates\n",
    "duplicate_count = df_bronze.groupBy(\"timestamp\") \\\n",
    "    .count() \\\n",
    "    .filter(col(\"count\") > 1) \\\n",
    "    .count()\n",
    "\n",
    "quality_report[\"duplicate_checks\"][\"timestamp_duplicates\"] = duplicate_count\n",
    "\n",
    "if duplicate_count > 0:\n",
    "    quality_report[\"issues_found\"] = True\n",
    "    print(f\"‚ö†Ô∏è  Found {duplicate_count} duplicate timestamps\")\n",
    "    \n",
    "    # Show duplicate timestamps\n",
    "    print(\"\\nDuplicate timestamps:\")\n",
    "    df_bronze.groupBy(\"timestamp\") \\\n",
    "        .count() \\\n",
    "        .filter(col(\"count\") > 1) \\\n",
    "        .orderBy(col(\"count\").desc()) \\\n",
    "        .show(10, truncate=False)\n",
    "else:\n",
    "    print(\"‚úÖ No duplicate timestamps found\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ### 3.3 Remove Duplicates and Create Clean Dataset\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Remove duplicates - keep first occurrence\n",
    "df_clean = df_bronze.dropDuplicates([\"timestamp\"])\n",
    "\n",
    "# Create a flag for rows with any null values in pollutant columns\n",
    "null_condition = \" OR \".join([f\"{col} IS NULL\" for col in pollutant_columns])\n",
    "df_with_quality_flag = df_clean.withColumn(\n",
    "    \"has_null_values\",\n",
    "    when(expr(null_condition), True).otherwise(False)\n",
    ")\n",
    "\n",
    "# Filter to get only clean rows (no nulls in any pollutant column)\n",
    "df_silver = df_with_quality_flag.filter(col(\"has_null_values\") == False).drop(\"has_null_values\")\n",
    "\n",
    "print(f\"‚úÖ Clean data prepared:\")\n",
    "print(f\"   - Original rows: {quality_report['total_rows']}\")\n",
    "print(f\"   - After removing duplicates: {df_clean.count()}\")\n",
    "print(f\"   - After removing nulls: {df_silver.count()}\")\n",
    "print(f\"   - Rows removed: {quality_report['total_rows'] - df_silver.count()}\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## 4. Generate Quality Report and Save to Silver Layer\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Generate comprehensive quality report\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"DATA QUALITY REPORT SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Report Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"Data Source: Open-Meteo Air Quality API\")\n",
    "print(f\"Location: Latitude {params['latitude']}, Longitude {params['longitude']}\")\n",
    "print(f\"Date Range: Past {params['past_days']} days + {params['forecast_days']} forecast days\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "print(f\"\\nüìä DATA VOLUME:\")\n",
    "print(f\"   Total rows fetched: {quality_report['total_rows']}\")\n",
    "print(f\"   Clean rows for Silver: {df_silver.count()}\")\n",
    "print(f\"   Data quality score: {(df_silver.count() / quality_report['total_rows'] * 100):.2f}%\")\n",
    "\n",
    "print(f\"\\nüîç QUALITY ISSUES FOUND:\")\n",
    "if not quality_report[\"issues_found\"]:\n",
    "    print(\"   ‚úÖ No quality issues detected!\")\n",
    "else:\n",
    "    print(\"   ‚ö†Ô∏è  Issues detected:\")\n",
    "    \n",
    "    # Null value summary\n",
    "    null_issues = [f\"{k}: {v['null_count']} nulls ({v['null_percentage']:.2f}%)\" \n",
    "                   for k, v in quality_report[\"null_checks\"].items() \n",
    "                   if v['null_count'] > 0]\n",
    "    if null_issues:\n",
    "        print(f\"\\n   NULL VALUES:\")\n",
    "        for issue in null_issues:\n",
    "            print(f\"      - {issue}\")\n",
    "    \n",
    "    # Duplicate summary\n",
    "    if quality_report[\"duplicate_checks\"][\"timestamp_duplicates\"] > 0:\n",
    "        print(f\"\\n   DUPLICATES:\")\n",
    "        print(f\"      - {quality_report['duplicate_checks']['timestamp_duplicates']} duplicate timestamps found\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Save clean data to Silver Delta table\n",
    "silver_table_name = \"default.air_quality_silver\"\n",
    "\n",
    "try:\n",
    "    # Write to Silver Delta table\n",
    "    df_silver.write \\\n",
    "        .mode(\"append\") \\\n",
    "        .format(\"delta\") \\\n",
    "        .saveAsTable(silver_table_name)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Successfully saved {df_silver.count()} clean rows to Silver layer\")\n",
    "    \n",
    "    # Show sample of silver data\n",
    "    print(\"\\nSample of Silver data:\")\n",
    "    df_silver.select(\"timestamp\", \"pm10\", \"pm2_5\", \"ozone\", \"ingestion_date\") \\\n",
    "        .orderBy(col(\"timestamp\").desc()) \\\n",
    "        .show(5, truncate=False)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error saving to Silver layer: {e}\")\n",
    "    # If table already exists, try to append\n",
    "    try:\n",
    "        df_silver.write \\\n",
    "            .mode(\"append\") \\\n",
    "            .format(\"delta\") \\\n",
    "            .insertInto(silver_table_name)\n",
    "        print(f\"‚úÖ Successfully appended {df_silver.count()} rows to existing Silver table\")\n",
    "    except:\n",
    "        raise\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## 5. Verify Delta Tables\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Verify Bronze table\n",
    "print(\"BRONZE TABLE VERIFICATION:\")\n",
    "bronze_count = spark.table(bronze_table_name).count()\n",
    "print(f\"‚úÖ Bronze table rows: {bronze_count}\")\n",
    "\n",
    "# Verify Silver table\n",
    "print(\"\\nSILVER TABLE VERIFICATION:\")\n",
    "silver_count = spark.table(silver_table_name).count()\n",
    "print(f\"‚úÖ Silver table rows: {silver_count}\")\n",
    "\n",
    "# Show table history\n",
    "print(\"\\nDELTA TABLE HISTORY:\")\n",
    "spark.sql(f\"DESCRIBE HISTORY {silver_table_name} LIMIT 5\").show(truncate=False)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Optional: Query the Tables\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Query Bronze table\n",
    "print(\"Recent Bronze data:\")\n",
    "spark.sql(f\"\"\"\n",
    "    SELECT timestamp, pm10, pm2_5, ozone, ingestion_date\n",
    "    FROM {bronze_table_name}\n",
    "    ORDER BY timestamp DESC\n",
    "    LIMIT 10\n",
    "\"\"\").show(truncate=False)\n",
    "\n",
    "# Query Silver table\n",
    "print(\"\\nRecent Silver data:\")\n",
    "spark.sql(f\"\"\"\n",
    "    SELECT timestamp, pm10, pm2_5, ozone, ingestion_date\n",
    "    FROM {silver_table_name}\n",
    "    ORDER BY timestamp DESC\n",
    "    LIMIT 10\n",
    "\"\"\").show(truncate=False)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Pipeline Complete! üéâ\n",
    "# MAGIC \n",
    "# MAGIC The ETL pipeline has successfully:\n",
    "# MAGIC 1. ‚úÖ Extracted air quality data from the API\n",
    "# MAGIC 2. ‚úÖ Transformed and loaded data to Bronze layer (with partitioning)\n",
    "# MAGIC 3. ‚úÖ Performed comprehensive data quality checks\n",
    "# MAGIC 4. ‚úÖ Generated detailed quality report\n",
    "# MAGIC 5. ‚úÖ Saved clean data to Silver layer\n",
    "# MAGIC \n",
    "# MAGIC **Tables Created:**\n",
    "# MAGIC - `default.air_quality_bronze` - Raw data with all records\n",
    "# MAGIC - `default.air_quality_silver` - Clean data with quality checks passed\n",
    "# MAGIC \n",
    "# MAGIC **Next Steps:**\n",
    "# MAGIC - Schedule this notebook to run periodically\n",
    "# MAGIC - Add additional quality checks as needed\n",
    "# MAGIC - Create Gold layer aggregations for analytics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e49a7b17-9f55-4b50-a69f-86bf52d8c0c6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Claude Opus 4 need 1 fix the code work"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Claude Opus 4",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
