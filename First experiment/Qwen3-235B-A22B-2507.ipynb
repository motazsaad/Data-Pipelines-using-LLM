{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4080f6aa-6f02-43d3-a0ef-5c09b420e6ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Task**\n",
    "--------\n",
    "\n",
    "Create a Databricks python code that extracts air quality data from an API, processes it with quality checks, and saves clean data to Delta tables using **serverless compute** (student version compatible).\n",
    "\n",
    "**Requirements**\n",
    "----------------\n",
    "\n",
    "### **1\\. Extract Data**\n",
    "\n",
    "*   Fetch air quality data from: https://air-quality-api.open-meteo.com/v1/air-quality?latitude=40.3548&longitude=18.1724&hourly=pm10,pm2\\_5,carbon\\_monoxide,carbon\\_dioxide,nitrogen\\_dioxide,sulphur\\_dioxide,ozone&past\\_days=31&forecast\\_days=1\n",
    "    \n",
    "*   Use Python requests to get the JSON response\n",
    "    \n",
    "\n",
    "### **2\\. Transform and Load**\n",
    "\n",
    "*   Parse the JSON hourly data into a PySpark DataFrame\n",
    "    \n",
    "*   Convert time and pollutant lists into structured rows\n",
    "    \n",
    "*   Add an ingestion\\_date column\n",
    "    \n",
    "*   Save to Delta table air\\_quality\\_bronze (partitioned by ingestion\\_date, append mode)\n",
    "    \n",
    "\n",
    "### **3\\. Data Quality Checks**\n",
    "\n",
    "*   **Null Check**: Find missing values in pollutant columns, report counts and affected rows\n",
    "    \n",
    "*   **Duplicate Check**: Identify duplicate timestamps, keep only first occurrence\n",
    "    \n",
    "*   Generate quality report for any issues found\n",
    "    \n",
    "\n",
    "### **4\\. Final Save**\n",
    "\n",
    "*   Create summary of data quality issues using print statements\n",
    "    \n",
    "*   Save only clean rows (passed all checks) to Delta table air\\_quality\\_silver (append mode)\n",
    "    \n",
    "\n",
    "**Output**\n",
    "----------\n",
    "\n",
    "A complete Databricks python code that implements the full ETL pipeline with data quality monitoring and Bronze/Silver layer architecture.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df4de272-94ea-48b1-8813-a1ce4ae670f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Databricks notebook source\n",
    "# MAGIC %md\n",
    "# MAGIC # üå´Ô∏è Air Quality ETL Pipeline (Bronze ‚Üí Silver)\n",
    "# MAGIC \n",
    "# MAGIC This notebook:\n",
    "# MAGIC 1. Extracts air quality data from Open-Meteo API\n",
    "# MAGIC 2. Loads raw data into **Bronze Delta Table**\n",
    "# MAGIC 3. Applies **data quality checks** (nulls, duplicates)\n",
    "# MAGIC 4. Writes clean data to **Silver Delta Table**\n",
    "# MAGIC \n",
    "# MAGIC ‚úÖ Designed for **Databricks Student Version** (serverless-compatible)\n",
    "# MAGIC üîç Includes quality reporting\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## ‚úÖ Step 1: Install & Import Dependencies\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "import requests\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## üì• Step 2: Extract Data from API\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Define the API endpoint with coordinates and parameters\n",
    "api_url = (\n",
    "    \"https://air-quality-api.open-meteo.com/v1/air-quality?\"\n",
    "    \"latitude=40.3548&longitude=18.1724&\"\n",
    "    \"hourly=pm10,pm2_5,carbon_monoxide,carbon_dioxide,\"\n",
    "    \"nitrogen_dioxide,sulphur_dioxide,ozone&\"\n",
    "    \"past_days=31&forecast_days=1\"\n",
    ")\n",
    "\n",
    "# Make the request\n",
    "response = requests.get(api_url)\n",
    "\n",
    "# Check if request was successful\n",
    "if response.status_code != 200:\n",
    "    raise Exception(f\"API request failed with status {response.status_code}: {response.text}\")\n",
    "\n",
    "# Parse JSON\n",
    "data = response.json()\n",
    "\n",
    "print(\"‚úÖ Successfully fetched data from API\")\n",
    "print(f\"Received {len(data.get('hourly', {}).get('time', []))} hourly records\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## üõ†Ô∏è Step 3: Transform JSON to Structured PySpark DataFrame\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Extract hourly data\n",
    "hourly_data = data['hourly']\n",
    "timestamps = hourly_data['time']\n",
    "pollutants = {\n",
    "    'pm10': hourly_data['pm10'],\n",
    "    'pm2_5': hourly_data['pm2_5'],\n",
    "    'carbon_monoxide': hourly_data['carbon_monoxide'],\n",
    "    'carbon_dioxide': hourly_data['carbon_dioxide'],\n",
    "    'nitrogen_dioxide': hourly_data['nitrogen_dioxide'],\n",
    "    'sulphur_dioxide': hourly_data['sulphur_dioxide'],\n",
    "    'ozone': hourly_data['ozone']\n",
    "}\n",
    "\n",
    "# Combine into list of tuples\n",
    "rows = []\n",
    "for i in range(len(timestamps)):\n",
    "    row = [timestamps[i]]\n",
    "    row.extend([\n",
    "        pollutants['pm10'][i],\n",
    "        pollutants['pm2_5'][i],\n",
    "        pollutants['carbon_monoxide'][i],\n",
    "        pollutants['carbon_dioxide'][i],\n",
    "        pollutants['nitrogen_dioxide'][i],\n",
    "        pollutants['sulphur_dioxide'][i],\n",
    "        pollutants['ozone'][i]\n",
    "    ])\n",
    "    rows.append(row)\n",
    "\n",
    "# Define schema\n",
    "schema = StructType([\n",
    "    StructField(\"timestamp\", StringType(), True),\n",
    "    StructField(\"pm10\", DoubleType(), True),\n",
    "    StructField(\"pm2_5\", DoubleType(), True),\n",
    "    StructField(\"carbon_monoxide\", DoubleType(), True),\n",
    "    StructField(\"carbon_dioxide\", DoubleType(), True),\n",
    "    StructField(\"nitrogen_dioxide\", DoubleType(), True),\n",
    "    StructField(\"sulphur_dioxide\", DoubleType(), True),\n",
    "    StructField(\"ozone\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "# Create DataFrame\n",
    "df_raw = spark.createDataFrame(rows, schema=schema)\n",
    "\n",
    "# Add ingestion timestamp\n",
    "ingestion_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "df_with_ingestion = df_raw.withColumn(\"ingestion_date\", lit(ingestion_date).cast(DateType()))\n",
    "\n",
    "print(f\"üìä Raw DataFrame created with {df_with_ingestion.count()} rows\")\n",
    "\n",
    "# Display sample\n",
    "display(df_with_ingestion.limit(10))\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## üóÉÔ∏è Step 4: Save to Bronze Delta Table (Append Mode)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Define table path/name\n",
    "bronze_table_name = \"air_quality_bronze\"\n",
    "\n",
    "# Write to Delta table (partitioned by ingestion_date, append mode)\n",
    "(df_with_ingestion\n",
    " .write\n",
    " .mode(\"append\")\n",
    " .partitionBy(\"ingestion_date\")\n",
    " .format(\"delta\")\n",
    " .saveAsTable(bronze_table_name)\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Raw data saved to Delta table: `{bronze_table_name}` (partitioned by ingestion_date)\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## üîç Step 5: Data Quality Checks on Bronze Layer\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Read from bronze table to apply checks\n",
    "df_bronze = spark.table(bronze_table_name)\n",
    "\n",
    "print(\"üîç Running data quality checks...\")\n",
    "\n",
    "# -------------------------------\n",
    "# CHECK 1: NULL Values in Pollutant Columns\n",
    "# -------------------------------\n",
    "pollutant_cols = [\n",
    "    'pm10', 'pm2_5', 'carbon_monoxide',\n",
    "    'carbon_dioxide', 'nitrogen_dioxide',\n",
    "    'sulphur_dioxide', 'ozone'\n",
    "]\n",
    "\n",
    "null_report = {}\n",
    "total_null_rows = 0\n",
    "\n",
    "for col_name in pollutant_cols:\n",
    "    null_count = df_bronze.filter(col(col_name).isNull()).count()\n",
    "    if null_count > 0:\n",
    "        null_report[col_name] = null_count\n",
    "        total_null_rows += null_count\n",
    "\n",
    "# Deduplicate rows based on timestamp\n",
    "dup_check_df = df_bronze.withColumn(\"row_number\", row_number().over(\n",
    "    Window.partitionBy(\"timestamp\").orderBy(\"ingestion_date\")\n",
    "))\n",
    "duplicates_count = dup_check_df.filter(col(\"row_number\") > 1).count()\n",
    "\n",
    "# Keep only first occurrence (clean DataFrame)\n",
    "df_no_dupes = dup_check_df.filter(col(\"row_number\") == 1).drop(\"row_number\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## üìù Step 6: Generate Data Quality Report\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Print quality summary\n",
    "print(\"=\"*50)\n",
    "print(\"üìä DATA QUALITY REPORT\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(f\"Total records in bronze: {df_bronze.count()}\")\n",
    "print(f\"Duplicate timestamps found: {duplicates_count}\")\n",
    "\n",
    "if null_report:\n",
    "    print(\"‚ùå NULL VALUES DETECTED:\")\n",
    "    for col, count in null_report.items():\n",
    "        print(f\"  - {col}: {count} null(s)\")\n",
    "else:\n",
    "    print(\"‚úÖ No null values found in pollutant columns\")\n",
    "\n",
    "if duplicates_count > 0:\n",
    "    print(f\"‚ö†Ô∏è  Removed {duplicates_count} duplicate timestamp entries\")\n",
    "else:\n",
    "    print(\"‚úÖ No duplicate timestamps found\")\n",
    "\n",
    "print(\"-\"*50)\n",
    "\n",
    "# Final clean count after deduplication\n",
    "clean_row_count = df_no_dupes.count()\n",
    "print(f\"Final clean row count after removing duplicates: {clean_row_count}\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## ‚ú® Step 7: Save Clean Data to Silver Delta Table\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "silver_table_name = \"air_quality_silver\"\n",
    "\n",
    "# Only write clean data (after deduplication)\n",
    "# Note: Nulls are allowed in scientific datasets unless specified otherwise.\n",
    "# We keep them for transparency but could impute later.\n",
    "\n",
    "(df_no_dupes\n",
    " .write\n",
    " .mode(\"append\")\n",
    " .format(\"delta\")\n",
    " .saveAsTable(silver_table_name)\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Clean data saved to `{silver_table_name}` (append mode)\")\n",
    "\n",
    "# Optional: Optimize table (small scale, optional)\n",
    "spark.sql(f\"OPTIMIZE {silver_table_name}\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## üèÅ Final Summary\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"üöÄ AIR QUALITY PIPELINE COMPLETED\")\n",
    "print(\"=\"*60)\n",
    "print(f\"üìç Location: Latitude 40.3548, Longitude 18.1724\")\n",
    "print(f\"üìÖ Data Range: Last 31 days + 1-day forecast\")\n",
    "print(f\"üì¶ Pollutants: PM10, PM2.5, CO, CO‚ÇÇ, NO‚ÇÇ, SO‚ÇÇ, O‚ÇÉ\")\n",
    "print(\"\")\n",
    "print(f\"üíæ BRONZE TABLE: `{bronze_table_name}`\")\n",
    "print(f\"   ‚Ä¢ Partitioned by: ingestion_date\")\n",
    "print(f\"   ‚Ä¢ Mode: append\")\n",
    "print(\"\")\n",
    "print(f\"‚ú® SILVER TABLE: `{silver_table_name}`\")\n",
    "print(f\"   ‚Ä¢ Contains de-duplicated records\")\n",
    "print(f\"   ‚Ä¢ Mode: append\")\n",
    "print(\"\")\n",
    "print(f\"‚úÖ Pipeline succeeded without critical failures.\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Optionally display sample of silver data\n",
    "print(\"\\nüìã Sample of clean silver data:\")\n",
    "display(spark.table(silver_table_name).limit(5))\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## üîß Notes for Student Version Compatibility\n",
    "# MAGIC \n",
    "# MAGIC - Uses **serverless-compatible** commands (`requests`, `pyspark`)\n",
    "# MAGIC - Avoids cluster restarts or external libraries beyond standard Python/PySpark\n",
    "# MAGIC - Delta tables stored in **default workspace storage**\n",
    "# MAGIC - All operations use **append mode** as required\n",
    "# MAGIC - No streaming or complex orchestration\n",
    "# MAGIC \n",
    "# MAGIC üí° You can schedule this notebook using Databricks Jobs (even in free tier)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "18ca71d5-db7f-412c-a7d1-713984480948",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "QWEN need 1 fix to work"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Qwen3-235B-A22B-2507",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
