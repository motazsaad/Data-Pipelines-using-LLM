{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "df539072-ffed-4263-a8ce-db4e2766b201",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Task**\n",
    "--------\n",
    "\n",
    "Create a Databricks python code that extracts air quality data from an API, processes it with quality checks, and saves clean data to Delta tables using **serverless compute** (student version compatible).\n",
    "\n",
    "**Requirements**\n",
    "----------------\n",
    "\n",
    "### **1\\. Extract Data**\n",
    "\n",
    "*   Fetch air quality data from: https://air-quality-api.open-meteo.com/v1/air-quality?latitude=40.3548&longitude=18.1724&hourly=pm10,pm2\\_5,carbon\\_monoxide,carbon\\_dioxide,nitrogen\\_dioxide,sulphur\\_dioxide,ozone&past\\_days=31&forecast\\_days=1\n",
    "    \n",
    "*   Use Python requests to get the JSON response\n",
    "    \n",
    "\n",
    "### **2\\. Transform and Load**\n",
    "\n",
    "*   Parse the JSON hourly data into a PySpark DataFrame\n",
    "    \n",
    "*   Convert time and pollutant lists into structured rows\n",
    "    \n",
    "*   Add an ingestion\\_date column\n",
    "    \n",
    "*   Save to Delta table air\\_quality\\_bronze (partitioned by ingestion\\_date, append mode)\n",
    "    \n",
    "\n",
    "### **3\\. Data Quality Checks**\n",
    "\n",
    "*   **Null Check**: Find missing values in pollutant columns, report counts and affected rows\n",
    "    \n",
    "*   **Duplicate Check**: Identify duplicate timestamps, keep only first occurrence\n",
    "    \n",
    "*   Generate quality report for any issues found\n",
    "    \n",
    "\n",
    "### **4\\. Final Save**\n",
    "\n",
    "*   Create summary of data quality issues using print statements\n",
    "    \n",
    "*   Save only clean rows (passed all checks) to Delta table air\\_quality\\_silver (append mode)\n",
    "    \n",
    "\n",
    "**Output**\n",
    "----------\n",
    "\n",
    "A complete Databricks python code that implements the full ETL pipeline with data quality monitoring and Bronze/Silver layer architecture.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2365ffa7-74d8-4cc8-9f1c-857db7cf9600",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Databricks Python ETL: Open-Meteo Air Quality -> Bronze/Silver Delta with quality checks\n",
    "# Notes:\n",
    "# - Uses serverless-compatible PySpark + Delta (append, partitioned by ingestion_date)\n",
    "# - Extracts via requests, transforms into structured Spark rows, loads Bronze, runs quality checks,\n",
    "#   and writes clean rows to Silver\n",
    "\n",
    "# 1) Extract Data (API call)\n",
    "import requests\n",
    "import datetime\n",
    "from functools import reduce\n",
    "\n",
    "# Open-Meteo API (past 31 days + 1 forecast day)\n",
    "LAT = 40.3548\n",
    "LON = 18.1724\n",
    "URL = (\n",
    "    f\"https://air-quality-api.open-meteo.com/v1/air-quality\"\n",
    "    f\"?latitude={LAT}&longitude={LON}\"\n",
    "    f\"&hourly=pm10,pm2_5,carbon_monoxide,carbon_dioxide,nitrogen_dioxide,sulphur_dioxide,ozone\"\n",
    "    f\"&past_days=31&forecast_days=1\"\n",
    ")\n",
    "\n",
    "print(f\"Fetching data from API: {URL}\")\n",
    "resp = requests.get(URL, timeout=60)\n",
    "resp.raise_for_status()  # fail fast if request fails\n",
    "data = resp.json()\n",
    "\n",
    "hourly = data.get(\"hourly\", {})\n",
    "times = hourly.get(\"time\", [])\n",
    "pollutant_keys = [\"pm10\", \"pm2_5\", \"carbon_monoxide\", \"carbon_dioxide\",\n",
    "                  \"nitrogen_dioxide\", \"sulphur_dioxide\", \"ozone\"]\n",
    "\n",
    "# Build structured rows (one row per hourly timestamp)\n",
    "rows = []\n",
    "n_times = len(times)\n",
    "for i in range(n_times):\n",
    "    row = {\"timestamp\": times[i]}\n",
    "    for key in pollutant_keys:\n",
    "        vals = hourly.get(key, [])\n",
    "        row[key] = vals[i] if i < len(vals) else None\n",
    "    # Ingestion date (date only)\n",
    "    row[\"ingestion_date\"] = datetime.date.today()\n",
    "    rows.append(row)\n",
    "\n",
    "# 2) Transform and Load (Bronze)\n",
    "# Create DataFrame from Python rows\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, TimestampType, DateType\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import to_timestamp, col\n",
    "\n",
    "# If you are in a Databricks notebook, `spark` is already available\n",
    "# Create DataFrame with implicit schema (will infer types)\n",
    "bronze_df = spark.createDataFrame(rows)\n",
    "\n",
    "# Cast types to desired types\n",
    "bronze_df = bronze_df \\\n",
    "    .withColumn(\"timestamp\", to_timestamp(col(\"timestamp\"))) \\\n",
    "    .withColumn(\"ingestion_date\", col(\"ingestion_date\").cast(\"date\"))\n",
    "\n",
    "for k in pollutant_keys:\n",
    "    bronze_df = bronze_df.withColumn(k, col(k).cast(\"double\"))\n",
    "\n",
    "# Persist to Delta bronze table (partitioned by ingestion_date)\n",
    "bronze_table = \"air_quality_bronze\"\n",
    "bronze_df.write.format(\"delta\").mode(\"append\").partitionBy(\"ingestion_date\").saveAsTable(bronze_table)\n",
    "\n",
    "print(f\"Bronze write complete: {bronze_df.count()} rows partitioned by ingestion_date into {bronze_table}.\")\n",
    "\n",
    "# 3) Data Quality Checks (on Bronze data just loaded)\n",
    "# Define pollutant columns\n",
    "pollutant_cols = pollutant_keys\n",
    "\n",
    "# Null checks: per-column nulls and rows with any nulls\n",
    "nulls_by_column = {}\n",
    "for c in pollutant_cols:\n",
    "    nulls_by_column[c] = bronze_df.filter(col(c).isNull()).count()\n",
    "\n",
    "any_null_condition = reduce(lambda a, b: a | b, [F.col(c).isNull() for c in pollutant_cols])\n",
    "rows_with_any_nulls = bronze_df.filter(any_null_condition).count()\n",
    "\n",
    "# Duplicate checks: duplicates by timestamp (keep first occurrence)\n",
    "from pyspark.sql.window import Window\n",
    "dup_window = Window.partitionBy(\"timestamp\").orderBy(\"ingestion_date\")\n",
    "bronze_with_rn = bronze_df.withColumn(\"rn\", F.row_number().over(dup_window))\n",
    "duplicate_count = bronze_with_rn.filter(F.col(\"rn\") > 1).count()\n",
    "bronze_dedup = bronze_with_rn.filter(F.col(\"rn\") == 1).drop(\"rn\")\n",
    "\n",
    "# Build a quality report\n",
    "quality_report = {\n",
    "    \"nulls_by_column\": nulls_by_column,\n",
    "    \"rows_with_any_nulls\": rows_with_any_nulls,\n",
    "    \"duplicate_timestamp_count\": duplicate_count,\n",
    "    \"bronze_total_rows\": bronze_df.count(),\n",
    "    \"bronze_dedup_rows\": bronze_dedup.count()\n",
    "}\n",
    "\n",
    "# Print quality report\n",
    "print(\"=== Quality Report (Bronze) ===\")\n",
    "print(f\"Total Bronze rows loaded: {quality_report['bronze_total_rows']}\")\n",
    "print(\"Nulls by column:\")\n",
    "for c, v in nulls_by_column.items():\n",
    "    print(f\" - {c}: {v} nulls\")\n",
    "print(f\"Rows with any pollutant nulls: {rows_with_any_nulls}\")\n",
    "print(f\"Duplicate timestamps detected: {duplicate_count} (kept first occurrence)\")\n",
    "print(f\"Bronze rows after dedup: {quality_report['bronze_dedup_rows']}\")\n",
    "\n",
    "# 4) Final Save (Silver) - keep only clean rows (no nulls in pollutants, after dedup)\n",
    "# Define non-null condition for all pollutant columns\n",
    "non_null_condition = None\n",
    "for c in pollutant_cols:\n",
    "    if non_null_condition is None:\n",
    "        non_null_condition = F.col(c).isNotNull()\n",
    "    else:\n",
    "        non_null_condition = non_null_condition & F.col(c).isNotNull()\n",
    "\n",
    "clean_rows = bronze_dedup.filter(non_null_condition)\n",
    "\n",
    "# Persist clean data to Silver (Delta), append mode\n",
    "silver_table = \"air_quality_silver\"\n",
    "clean_rows.write.format(\"delta\").mode(\"append\").partitionBy(\"ingestion_date\").saveAsTable(silver_table)\n",
    "\n",
    "print(f\"Silver write complete: {clean_rows.count()} clean rows saved to {silver_table} (partitioned by ingestion_date).\")\n",
    "\n",
    "# Optional: final summary\n",
    "print(\"ETL pipeline finished.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "130e851b-a39c-437f-a9fc-9c1413d58758",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "GPT 5 nano work at the first attempt"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "GPT 5 NANO",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
