{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "40202508-3550-46b9-916a-ec36b989c962",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Task**\n",
    "--------\n",
    "\n",
    "Create a Databricks python code that extracts air quality data from an API, processes it with quality checks, and saves clean data to Delta tables using **serverless compute** (student version compatible).\n",
    "\n",
    "**Requirements**\n",
    "----------------\n",
    "\n",
    "### **1\\. Extract Data**\n",
    "\n",
    "*   Fetch air quality data from: https://air-quality-api.open-meteo.com/v1/air-quality?latitude=40.3548&longitude=18.1724&hourly=pm10,pm2\\_5,carbon\\_monoxide,carbon\\_dioxide,nitrogen\\_dioxide,sulphur\\_dioxide,ozone&past\\_days=31&forecast\\_days=1\n",
    "    \n",
    "*   Use Python requests to get the JSON response\n",
    "    \n",
    "\n",
    "### **2\\. Transform and Load**\n",
    "\n",
    "*   Parse the JSON hourly data into a PySpark DataFrame\n",
    "    \n",
    "*   Convert time and pollutant lists into structured rows\n",
    "    \n",
    "*   Add an ingestion\\_date column\n",
    "    \n",
    "*   Save to Delta table air\\_quality\\_bronze (partitioned by ingestion\\_date, append mode)\n",
    "    \n",
    "\n",
    "### **3\\. Data Quality Checks**\n",
    "\n",
    "*   **Null Check**: Find missing values in pollutant columns, report counts and affected rows\n",
    "    \n",
    "*   **Duplicate Check**: Identify duplicate timestamps, keep only first occurrence\n",
    "    \n",
    "*   Generate quality report for any issues found\n",
    "    \n",
    "\n",
    "### **4\\. Final Save**\n",
    "\n",
    "*   Create summary of data quality issues using print statements\n",
    "    \n",
    "*   Save only clean rows (passed all checks) to Delta table air\\_quality\\_silver (append mode)\n",
    "    \n",
    "\n",
    "**Output**\n",
    "----------\n",
    "\n",
    "A complete Databricks python code that implements the full ETL pipeline with data quality monitoring and Bronze/Silver layer architecture.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "957d7b25-a560-4376-bd11-ba41cb482d6c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Databricks Python ETL: Open-Meteo Air Quality -> Bronze/Silver Delta tables\n",
    "# Serverless compute (Databricks â€” student/workspace)\n",
    "# End-to-end: Extract from API -> Bronze (partitioned) -> Quality checks -> Silver (cleaned)\n",
    "\n",
    "# Prerequisites:\n",
    "# - requests library available in the cluster (Databricks runtime). If not, install via Libraries.\n",
    "# - Delta Lake enabled. This script uses Delta tables via saveAsTable(..., format=\"delta\").\n",
    "\n",
    "import requests\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import col, to_timestamp, current_date\n",
    "\n",
    "# 1) Extract: Fetch data from API using requests\n",
    "def fetch_air_quality_json(url: str, timeout: int = 60) -> dict:\n",
    "    \"\"\"\n",
    "    Fetch JSON payload from the given API URL.\n",
    "    \"\"\"\n",
    "    resp = requests.get(url, timeout=timeout)\n",
    "    resp.raise_for_status()\n",
    "    return resp.json()\n",
    "\n",
    "# 2) Transform: Convert API JSON to a Spark DataFrame\n",
    "def json_to_spark_df(spark: SparkSession, data: dict):\n",
    "    \"\"\"\n",
    "    Translate the hourly air quality JSON into a Spark DataFrame with:\n",
    "    - time (timestamp)\n",
    "    - ingestion_date (to be populated later)\n",
    "    - pollutant columns: pm10, pm2_5, carbon_monoxide, carbon_dioxide,\n",
    "      nitrogen_dioxide, sulphur_dioxide, ozone\n",
    "    \"\"\"\n",
    "    hourly = data.get(\"hourly\", {})\n",
    "    times = hourly.get(\"time\", [])\n",
    "\n",
    "    pollutant_keys = [\n",
    "        \"pm10\",\n",
    "        \"pm2_5\",\n",
    "        \"carbon_monoxide\",\n",
    "        \"carbon_dioxide\",\n",
    "        \"nitrogen_dioxide\",\n",
    "        \"sulphur_dioxide\",\n",
    "        \"ozone\",\n",
    "    ]\n",
    "\n",
    "    pollutant_arrays = {k: hourly.get(k, []) for k in pollutant_keys}\n",
    "\n",
    "    rows = []\n",
    "    for i, t in enumerate(times):\n",
    "        row = {\"time\": t}\n",
    "        for k in pollutant_keys:\n",
    "            arr = pollutant_arrays.get(k, [])\n",
    "            value = arr[i] if i < len(arr) else None\n",
    "            row[k] = value\n",
    "        rows.append(row)\n",
    "\n",
    "    if not rows:\n",
    "        # Return an empty DataFrame if payload is empty\n",
    "        return spark.createDataFrame([], schema=None)\n",
    "\n",
    "    df = spark.createDataFrame(rows)\n",
    "\n",
    "    # Normalize time to Spark Timestamp\n",
    "    df = df.withColumn(\"time\", to_timestamp(col(\"time\")))\n",
    "\n",
    "    # Placeholder for ingestion_date; will be filled in main()\n",
    "    df = df.withColumn(\"ingestion_date\", F.lit(None).cast(\"date\"))\n",
    "\n",
    "    return df\n",
    "\n",
    "def main():\n",
    "    # Create Spark session (Databricks runtime provides this)\n",
    "    spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "    # API URL (provided)\n",
    "    api_url = (\n",
    "        \"https://air-quality-api.open-meteo.com/v1/air-quality?\"\n",
    "        \"latitude=40.3548&longitude=18.1724&hourly=pm10,pm2_5,carbon_monoxide,\"\n",
    "        \"carbon_dioxide,nitrogen_dioxide,sulphur_dioxide,ozone&past_days=31&forecast_days=1\"\n",
    "    )\n",
    "\n",
    "    # 1) Extract\n",
    "    try:\n",
    "        api_data = fetch_air_quality_json(api_url, timeout=60)\n",
    "        print(\"API data retrieved successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching API data: {e}\")\n",
    "        return\n",
    "\n",
    "    # 2) Transform: to Spark DataFrame\n",
    "    df_raw = json_to_spark_df(spark, api_data)\n",
    "    # Emptiness check without RDDs: use DataFrame.count()\n",
    "    if df_raw.count() == 0:\n",
    "        print(\"No data transformed from API payload.\")\n",
    "        return\n",
    "\n",
    "    pollutant_cols = [\n",
    "        \"pm10\",\n",
    "        \"pm2_5\",\n",
    "        \"carbon_monoxide\",\n",
    "        \"carbon_dioxide\",\n",
    "        \"nitrogen_dioxide\",\n",
    "        \"sulphur_dioxide\",\n",
    "        \"ozone\",\n",
    "    ]\n",
    "\n",
    "    # Populate ingestion_date (current date) for Bronze partitioning\n",
    "    df = df_raw.withColumn(\"ingestion_date\", current_date())\n",
    "\n",
    "    # Reorder columns for readability\n",
    "    df = df.select([\"time\", \"ingestion_date\"] + pollutant_cols)\n",
    "\n",
    "    # 2a) Bronze: Write raw data to Delta Bronze table (partitioned by ingestion_date)\n",
    "    try:\n",
    "        df.write.format(\"delta\").mode(\"append\").partitionBy(\"ingestion_date\").saveAsTable(\"air_quality_bronze\")\n",
    "        print(\"Bronze table updated: air_quality_bronze (partitioned by ingestion_date).\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error writing Bronze table: {e}\")\n",
    "        return\n",
    "\n",
    "    # 3) Data Quality Checks\n",
    "    total_rows = df.count()\n",
    "\n",
    "    # Null checks per pollutant column\n",
    "    null_counts = {}\n",
    "    for c in pollutant_cols:\n",
    "        null_counts[c] = df.filter(col(c).isNull()).count()\n",
    "\n",
    "    # Rows with any null among pollutant columns\n",
    "    any_null_expr = None\n",
    "    for c in pollutant_cols:\n",
    "        if any_null_expr is None:\n",
    "            any_null_expr = col(c).isNull()\n",
    "        else:\n",
    "            any_null_expr = any_null_expr | col(c).isNull()\n",
    "    rows_with_any_null = df.filter(any_null_expr).count()\n",
    "\n",
    "    # Duplicate check on \"time\"\n",
    "    dup_times = df.groupBy(\"time\").count().filter(col(\"count\") > 1).collect()\n",
    "    duplicate_times = [row[\"time\"] for row in dup_times]\n",
    "\n",
    "    # Deduplicate by time (keep first occurrence)\n",
    "    df_dedup = df.dropDuplicates([\"time\"])\n",
    "    dedup_rows = df_dedup.count()\n",
    "\n",
    "    # Filter to clean rows: non-null for all pollutants\n",
    "    clean_df = df_dedup\n",
    "    for c in pollutant_cols:\n",
    "        clean_df = clean_df.filter(col(c).isNotNull())\n",
    "    clean_rows = clean_df.count()\n",
    "\n",
    "    # Quality report (printed)\n",
    "    print(\"DATA QUALITY REPORT\")\n",
    "    print(\"===================\")\n",
    "    print(f\"Total Bronze rows before quality filtering: {total_rows}\")\n",
    "    print(\"Null counts per pollutant column:\")\n",
    "    for c in pollutant_cols:\n",
    "        print(f\" - {c}: {null_counts[c]}\")\n",
    "    print(f\"Rows with any null pollutant value: {rows_with_any_null}\")\n",
    "    print(f\"Duplicate timestamps found: {duplicate_times if duplicate_times else []}\")\n",
    "    print(f\"Rows after deduplication: {dedup_rows}\")\n",
    "    print(f\"Rows passing all quality checks (non-null for all pollutants, deduplicated): {clean_rows}\")\n",
    "\n",
    "    # 4) Final Save: write only clean rows to Silver table (append)\n",
    "    if clean_rows > 0:\n",
    "        try:\n",
    "            clean_df.write.format(\"delta\").mode(\"append\").saveAsTable(\"air_quality_silver\")\n",
    "            print(\"Silver table updated: air_quality_silver (append).\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error writing Silver table: {e}\")\n",
    "            return\n",
    "    else:\n",
    "        print(\"No clean data to save to Silver.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5b9febdf-3996-4e79-90a6-d0926aa08dee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "GPT 5 need 4 fix to work\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "GPT 5",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
